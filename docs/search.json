[{"path":"https://aoiferyan-sc.github.io/BertopicR/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2023 BertopicR authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/articles/bertopic.html","id":"adjusting-model-parameters","dir":"Articles","previous_headings":"","what":"Adjusting Model Parameters","title":"Topic Modelling with Bertopic","text":"creating model, can alter min_topic_size arguments control number posts required per topic. can also specify ngram_range diversity parameters control size diversity ngrams used represent topics set stopwords TRUE prevent stopwords appearing representative ngrams (Note remove text analysed, ngrams used describe topics). also possible use nr_topics argument specify exact number topics like model output however done without careful examination data. run bt_fit_transform_model() function might noticed can get different results time run , due stochastic nature umap, avoid can specify random state prevents stochastic behaviour gives reproducible results. Reproducing results can also achieved saving model reusing , completely fine run model number times see topics output save model feel best fits research. already run bt_fit_transform_model() data, can simply feed already saved embedding model, time bt_fit_transform_model function output bertopic model, embeddings. Note: information hyperparameter tuning can found BERTopic website Lets look output topics time: time get 6 topics output much manageable analysis.","code":"model <- bt_fit_transform_model(cleaned_text = data$text_clean,                              calculated_embeddings = embeddings,                              min_topic_size = 30,                              ngram_range = c(1, 3),                              diversity = 0.5,                              stopwords = TRUE,                              random_state = 42) model$get_topic_info() #>   Topic Count #> 1    -1   170 #> 2     0  5235 #> 3     1   220 #> 4     2   123 #> 5     3    41 #>                                                                                      Name #> 1                                   -1_hispanic heritage_prepare_ellen ochoa_receive turn #> 2                                   0_heritage month_hispanic heritage month_day_students #> 3                                                              1_beto_heritage_fake_lacks #> 4 2_beto rourke_rourke membership lacks_membership lacks hispanic_hispanic caucus refuses #> 5                                         3_54th annual hispanic_5th ave____tradition 5th #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ]"},{"path":"https://aoiferyan-sc.github.io/BertopicR/articles/bertopic.html","id":"exploring-topics","dir":"Articles","previous_headings":"","what":"Exploring Topics","title":"Topic Modelling with Bertopic","text":"Now might need take closer look topic understand better . use bt_make_df() function merge bert results Sprinklr export. can look topics using ParseR create bigrams, BertopicR function bt_viz_top_terms() create top terms plot look exemplars filtering df topic ’re interested . Make bigrams:  Look top terms:   SegmentR, all_terms max_only plots returned can choose display charts bar lollipop viz pluck either all_terms max_only wish.  can also look diff_terms done using SegmentR:  want look exemplars can simply filter dataframe: can also look umap shows us topic distribution. can helpful showing us closely different topics relate one another continue check diagram merge topics reduce outliers. can use LandscapeR bertopic umap viz achieve . LandscapeR: BertopicR: key difference Bert LDA use hierarchical clustering, means can visualise cluster formed clusters might similar . can also look topic similarity heatmap see similar topics .","code":"merged_df <- data %>% bt_make_df(model = model,                              embeddings = embeddings,                              text_var = text_clean)  merged_df #> # A tibble: 5,829 × 61 #>    document universal_message_id                   social_network sender_user_id #>       <int> <chr>                                  <chr>          <chr>          #>  1        1 TWITTER_4_1057818026265980928          TWITTER        328184177      #>  2        2 TWITTER_2_1057782423700815873          TWITTER        2544249232     #>  3        3 TWITTER_2_1057758721747701762          TWITTER        227137623      #>  4        4 TWITTER_2_1057744985527910401          TWITTER        21702500       #>  5        5 TWITTER_2_1057730393053519873          TWITTER        1708113440     #>  6        6 INSTAGRAM_36_1902499764302394154_5416… INSTAGRAM      agriosmo       #>  7        7 TWITTER_2_1057716292281622528          TWITTER        7395452317968… #>  8        8 TWITTER_2_1057700738732908546          TWITTER        4258372295     #>  9        9 INSTAGRAM_36_1902419377766942925_7958… INSTAGRAM      tasteofstylec… #> 10       10 INSTAGRAM_36_1902389293970072552_1934… INSTAGRAM      salviqtpie84   #> # ℹ 5,819 more rows #> # ℹ 57 more variables: sender_screen_name <chr>, sender_listed_name <chr>, #> #   sender_profile_img_url <chr>, sender_profile_link <lgl>, #> #   sender_followers_count <dbl>, sender_influencer_score <lgl>, #> #   sender_age <lgl>, sender_gender <chr>, title <chr>, text_clean <chr>, #> #   message <chr>, message_type <chr>, created_time <date>, language <chr>, #> #   language_code <chr>, country_code <chr>, media_type_list <chr>, … merged_df %>% filter(topic == 4) %>%   ParseR::count_ngram(text_var = text_clean,                       remove_stops = TRUE,                       min_freq = 5,                       top_n = 25) %>%   purrr::pluck(\"viz\") %>%   ParseR::viz_ngram(emphasis = TRUE) #> Warning in max(node_freq): no non-missing arguments to max; returning -Inf #> Warning in min(node_freq): no non-missing arguments to min; returning Inf #> Warning in max(edge_freq): no non-missing arguments to max; returning -Inf #> Warning in min(edge_freq): no non-missing arguments to min; returning Inf merged_df %>%    bt_viz_top_terms(min_freq = 5,                 type = \"lollipop\")  #> $all_terms #>  #> $max_only merged_df %>%    bt_viz_top_terms(min_freq = 5,                 type = \"bars\") %>%   purrr::pluck(\"max_only\") merged_df %>%    bt_viz_diff_terms(min_freq = 5,                  type = \"lollipop\") %>%   purrr::pluck(\"topic0_vs_topic1\") merged_df %>%    filter(topic == 0) merged_df %>%    mutate(created_time = as.Date(created_time)) %>%   LandscapeR::conversation_landscape(id = document,                                      text_var = message,                                      colour_var = topic,                                      cleaned_text_var = text_clean,                                      date_var = created_time,                                      url_var = permalink,                                      sentiment_var = sentiment,                                      x_var = V1,                                      y_var = V2) model$visualize_documents(docs = data$text_clean,                            embeddings = embeddings)$show() model$visualize_hierarchy()$show() model$visualize_heatmap()$show()"},{"path":"https://aoiferyan-sc.github.io/BertopicR/articles/bertopic.html","id":"refine-topics","dir":"Articles","previous_headings":"","what":"Refine Topics","title":"Topic Modelling with Bertopic","text":"Now better idea topic , maybe now like manually merge topics think might similar reduce size outlier category (-1). Merging topics performed outlier reduction. can see tree diagram visualise topic hierarchy topic 4 topic 0 branches cluster, looked individual documents bigrams apparent discuss hispanic heritage, might make sense merge . Make sure save model merging topics case unhappy result want revert back. Note: topics merge must given merge_topics function python list integers. convert r vector python list using r_to_py() function “L” suffix denote integer. unhappy new topics can always go back original model. need use “py$” suffix access libraries python environment. Now happy merged topics can look reducing “outliers”. important project, perhaps important get really high quality topics expense data maybe important include much data possible expense resolution. example going use “embeddings” strategy, lots different strategies can used can find : https://maartengr.github.io/BERTopic/getting_started/outlier_reduction/outlier_reduction.html#exploration order correctly reassign topics outliers need tell function document belongs topic, can use df output bt_make_df function, remember merged topics since generating df, need run update topics table. can play around threshold parameter adjust number reassigned documents, refers minimum similarity document topic reassigned topic correct value entirely dependent dataset specifications project. new topics, can update model reflect , , make sure save model updating case want return . TODO: flesh fully","code":"model$save(path = \"BertopicR_example.bt\") topics_to_merge <- r_to_py(c(0L,4L))  model$merge_topics(docs = data$text_clean,                    topics_to_merge = topics_to_merge)  model$get_topic_info() #>   Topic Count #> 1    -1   170 #> 2     0  5275 #> 3     1   220 #> 4     2   123 #> 5     3    41 #>                                                                                      Name #> 1                             -1_hispanic heritage_prepare_ellen ochoa_familys cyber cafe #> 2                                   0_heritage month_hispanic heritage month_day_students #> 3                                  1_trump_hispanic caucus_lacks hispanic heritage_warren #> 4 2_beto rourke_rourke membership lacks_membership lacks hispanic_hispanic caucus refuses #> 5                                            3_day parade_54th annual hispanic_5th ave___ model_og <- py$bertopic$BERTopic$load(path = \"BertopicR_example.bt\") merged_df <- data %>% bt_make_df(model = model,                               embeddings = embeddings,                              text_var = text_clean)  # redistribute topics new_topics <- model$reduce_outliers(documents = data$text_clean,                                      topics = merged_df$topic,                                     strategy=\"embeddings\",                                     threshold = 0.3) model$save(path = \"BERTopic_example.bt\") model$update_topics(docs = data$text_clean,                      topics = new_topics)"},{"path":"https://aoiferyan-sc.github.io/BertopicR/articles/modular_approach.html","id":"modules","dir":"Articles","previous_headings":"","what":"Modules","title":"Interacting with individaul modules","text":"Bertopic Python Library Maarten Grootendorst 6 sub-modules: Embeddings - transforming documents numerical representation Dimensionality Reduction - reducing number features embeddings output Clustering - finding groups similar documents represent topics Vectorisers - find n-grams describe topic c-TF-IDF - create topic-level (rather document) bag words matrices representing topics Fine-tuning topic representations - tools topic representations (includes generative AI/LLMs)  vignette show use {BertopicR} tune module creating topic models.","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/articles/modular_approach.html","id":"data","dir":"Articles","previous_headings":"Modules","what":"Data","title":"Interacting with individaul modules","text":"demonstrative purposes, ’ll use {stringr}‘s ’sentences’ data set, comes fairly clean. help cleaning text data visit ParseR/LimpiaR documentation. Let’s take look first five posts brevity. ’ll turn sentences data frame:","code":"sentences <- stringr::sentences sentences[1:5] #> [1] \"The birch canoe slid on the smooth planks.\"  #> [2] \"Glue the sheet to the dark blue background.\" #> [3] \"It's easy to tell the depth of a well.\"      #> [4] \"These days a chicken leg is a rare dish.\"    #> [5] \"Rice is often served in round bowls.\" df <- dplyr::tibble(sentences = tolower(sentences))"},{"path":"https://aoiferyan-sc.github.io/BertopicR/articles/modular_approach.html","id":"embeddings","dir":"Articles","previous_headings":"Modules","what":"Embeddings","title":"Interacting with individaul modules","text":"order work efficiently text data, need turn words numbers. current state---art approach turning text numbers, contextualised word embeddings. ’ll use MpNet model, ‘-mpnet-base-v2’ take sentences turn numbers (embeddings). allow us find similarities differences sentences, using standard mathematical techniques (don’t worry isn’t making sense right now, often best way learn ). BertopicR usually either want making modules, , compiling fitting models. make components, actions data components. compile components models, fit models data.","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/articles/modular_approach.html","id":"make-the-embedder","dir":"Articles","previous_headings":"Modules > Embeddings","what":"Make the embedder","title":"Interacting with individaul modules","text":"First ’ll make embedder (embedding_model) using bt_make_embedder function. ’ll embed sentences using embedder. TIP: ’s good idea save embeddings, working many documents process time consuming.","code":"embedder <- bt_make_embedder(   model_name = \"all-mpnet-base-v2\"   )"},{"path":"https://aoiferyan-sc.github.io/BertopicR/articles/modular_approach.html","id":"do-the-embedding","dir":"Articles","previous_headings":"Modules > Embeddings","what":"Do the embedding","title":"Interacting with individaul modules","text":"row embeddings output represents one original sentences, column represents different embedding dimension; 768 dimensions outputted ‘-mpnet-base-v2’ mode take peek first 10 columns (dimensions), first row embeddings see 10 floating point numbers.","code":"embeddings <- bt_do_embedding(   embedder,    df$sentences   ) #>  #> Embedding proccess finished #> all-mpnet-base-v2 added to embeddings attributes  embeddings[1, 1:10] #>  [1]  0.0031732221 -0.0454455651 -0.0003524857  0.0046989759  0.0179691557 #>  [6] -0.0300240256 -0.0278541520 -0.0199615341 -0.0032402743  0.0257827863"},{"path":"https://aoiferyan-sc.github.io/BertopicR/articles/modular_approach.html","id":"reducing-dimensions","dir":"Articles","previous_headings":"Modules","what":"Reducing Dimensions","title":"Interacting with individaul modules","text":"next step pipeline reduce dimensions embeddings, two reasons: allow clustering algorithm run smoothly visualise clusters plane (eventually reduce 2 dimensions) machine learning generally, dimensionality reduction often important step avoid overfitting curse dimensionality. example use UMAP algorithm (however dimensionality reduction using PCA truncatedSVD also currently available) information UMAP algorithm - uniform manifold approximation projection dimension reduction - catchy, see UMAP Docs. TIP: Like embeddings, ’s good idea save reduced embeddings, reducing dimensions can costly process.","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/articles/modular_approach.html","id":"make-the-reducer","dir":"Articles","previous_headings":"Modules > Reducing Dimensions","what":"Make the reducer","title":"Interacting with individaul modules","text":"’ll use low-ish value n_neighbours (small dataset) output 5 dimensions (n_components = 5L). ’ll set min_distance 0, dimensionality reduction model can place similar documents close together. ’ll set metric “Euclidean” see Embedding non-Euclidean Spaces alternatives.","code":"reducer <- bt_make_reducer_umap(   n_neighbors = 10L,    n_components = 10L,   min_dist = 0L,   metric = \"euclidean\"   )"},{"path":"https://aoiferyan-sc.github.io/BertopicR/articles/modular_approach.html","id":"do-the-reducing","dir":"Articles","previous_headings":"Modules > Reducing Dimensions","what":"Do the reducing","title":"Interacting with individaul modules","text":"’ll take peek two rows now represent reduced dimension embeddings document. Notice numbers floating points, also bounded -1 1. next step cluster data. first pass, bertopic considers discovered cluster topic. Choice clustering model selected parameters therefore important. ’ll use hdbscan cluster, ’s bertopic initially built .","code":"reduced_embeddings <- bt_do_reducing(   reducer, embeddings = embeddings ) #> UMAP(low_memory=False, min_dist=0, n_components=10, n_neighbors=10, random_state=42, verbose=True) #> Tue Aug 22 17:35:37 2023 Construct fuzzy simplicial set #> Tue Aug 22 17:35:37 2023 Finding Nearest Neighbors #> Tue Aug 22 17:35:38 2023 Finished Nearest Neighbor Search #> Tue Aug 22 17:35:40 2023 Construct embedding #> Tue Aug 22 17:35:41 2023 Finished embedding  reduced_embeddings[1:2, ] #>          [,1]     [,2]      [,3]     [,4]     [,5]     [,6]     [,7]     [,8] #> [1,] 5.739700 6.123053 10.222593 5.357183 2.770175 6.529313 7.878425 2.544012 #> [2,] 4.772967 7.098315  8.398382 6.239881 3.431901 7.322098 9.159200 3.022838 #>          [,9]    [,10] #> [1,] 6.631907 6.297930 #> [2,] 7.621526 6.564954"},{"path":"https://aoiferyan-sc.github.io/BertopicR/articles/modular_approach.html","id":"clustering","dir":"Articles","previous_headings":"Modules","what":"Clustering","title":"Interacting with individaul modules","text":"lot learn comes clustering, selecting correct parameters notoriously difficult - especially clustering without pre-assigned labels, clustering tends . run ’ll use hdbscan clustering algorithm, don’t know many clusters look advance (using kMeans clustering exampel).hdbscan documentation ’s important know get level updating topic representations, bertopic pipeline 1 topic = 1 cluster. ’s therefore crucial gather information can data inform clustering process.","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/articles/modular_approach.html","id":"make-the-clusterer","dir":"Articles","previous_headings":"Modules > Clustering","what":"Make the clusterer","title":"Interacting with individaul modules","text":"’ll stick Euclidean distance metric, ’ll reduce min_cluster_size 10, giving us theoretical maximum number clusters : length(sentences) / 10 min_samples equal 5. relationship min_cluster_size min_samples important, default min_samples = min_cluster_size specified, likely adverse effects clustering outputs dealing larger datasets (’ll likely want raise min_cluster_size parameter significantly). hand, hdbscan documentation claims min_samples, parameter inherited dbscan, importance hdbscan algorithm - though also say remains algorithm’s biggest weakness. ’ll also set cluster_selection_method = “leaf”, means ’ll tend find many small clusters, rather large clusters. another parameter fraught danger, get right first time unlikely, likely require trial error, least beginning.","code":"clusterer <- bt_make_clusterer_hdbscan(min_cluster_size = 5L,                           metric = \"euclidean\",                           cluster_selection_method = \"leaf\",                           min_samples = 3L,                           prediction_data = TRUE)"},{"path":"https://aoiferyan-sc.github.io/BertopicR/articles/modular_approach.html","id":"do-the-clustering","dir":"Articles","previous_headings":"Modules > Clustering","what":"Do the clustering","title":"Interacting with individaul modules","text":"clusters now list cluster labels, 720 labels total - one sentence {Stringr}’s sentences data set. cluster labels output integers, ’s important assume work like regular integers . ’s necessarily case cluster 1 closer cluster 4 cluster 15, ordering labels can effectively considered random. likely labels training/test/validation data set, rely inspecting clusters, remembering bertopic 1 cluster = 1 topic. check whether clusters make sense, draw upon data analysis & visualisation tool kit, inspect cluster every . soon become intractable. Instead, ’ll take quick look distribution ’ll use bt_compile_model() bt_fit_model() functions get topic models .","code":"clusters <- bt_do_clustering(clustering_model = clusterer, embeddings = reduced_embeddings)"},{"path":"https://aoiferyan-sc.github.io/BertopicR/articles/modular_approach.html","id":"create-a-data-frame","dir":"Articles","previous_headings":"Modules > Clustering","what":"Create a data frame","title":"Interacting with individaul modules","text":"first, ’re beginning acquire bunch objects may become hard maintain. can store data frame: want save data frame, ’ll need save .Rdata/.rds object, .csv .xlsx contains list columns.","code":"data <- dplyr::tibble(sentence = tolower(sentences)) %>%   mutate(embeddings = list(embeddings),          reduced_embeddings = list(reduced_embeddings),          cluster = as.integer(clusters))"},{"path":"https://aoiferyan-sc.github.io/BertopicR/articles/modular_approach.html","id":"count-the-clusters","dir":"Articles","previous_headings":"Modules > Clustering","what":"Count the clusters","title":"Interacting with individaul modules","text":"can see distribution via histogram:  exception outlier group, clusters labelled order size: 213/720 (29.6%) data points labelled noise (cluster == -1), upon first inspection appear quite eclectic. clusters look? clusters look , difficult become figure ’s happening… Inspecting clusters trying figure cluster means inter-relate soon become intractable humans. Thankfully, within BERTopic quantitative methods already place aid procedure. Instead looking individual posts cluster, ’ll attempt summarise contents keywords phrases. order , ’ll make vectoriser ctfidf model. creating models, can use everything ’ve looked far compile model, fit model data, finally explore topics representations.","code":"library(ggplot2)  data %>%   filter(cluster != -1) %>%   count(cluster, sort = TRUE) %>%   ggplot(aes(x= n)) +   geom_histogram(fill = \"midnightblue\", bins = 20) +   theme_minimal() +   xlab(\"Cluster Size\") +   ylab(\"Number of Clusters\") data %>%   count(cluster, sort = TRUE) #> # A tibble: 58 × 2 #>    cluster     n #>      <int> <int> #>  1      -1   213 #>  2       0    34 #>  3       1    19 #>  4       2    18 #>  5       3    18 #>  6       4    17 #>  7       5    13 #>  8       6    13 #>  9       7    13 #> 10       8    13 #> # ℹ 48 more rows data %>%   filter(cluster == -1) %>%    slice(30:40) %>%   pull(sentence) #>  [1] \"the heart beat strongly and with firm strokes.\" #>  [2] \"the hat brim was wide and too droopy.\"          #>  [3] \"cut the pie into large parts.\"                  #>  [4] \"he lay prone and hardly moved a limb.\"          #>  [5] \"the fin was sharp and cut the clear water.\"     #>  [6] \"oak is strong and also gives shade.\"            #>  [7] \"the pipe began to rust while new.\"              #>  [8] \"thieves who rob friends deserve jail.\"          #>  [9] \"the ripe taste of cheese improves with age.\"    #> [10] \"the hog crawled under the high fence.\"          #> [11] \"split the log with a quick, sharp blow.\" data %>%   filter(cluster == 0) %>%   sample_n(5) %>%    pull(sentence) #> [1] \"slash the gold cloth into fine ribbons.\"   #> [2] \"a stiff cord will do to fasten your shoe.\" #> [3] \"hang tinsel from both branches.\"           #> [4] \"cut the cord that binds the box tightly.\"  #> [5] \"the key you designed will fit the lock.\" data %>%   filter(cluster == 1) %>%   sample_n(5) %>%   pull(sentence) #> [1] \"a pencil with black lead writes best.\"          #> [2] \"open your book to the first page.\"              #> [3] \"he offered proof in the form of a large chart.\" #> [4] \"he wrote down a long list of items.\"            #> [5] \"draw the chart with heavy black lines.\" data %>%   filter(cluster == 2) %>%   sample_n(5) %>%   pull(sentence) #> [1] \"quench your thirst, then eat the crackers.\" #> [2] \"a pot of tea helps to pass the evening.\"    #> [3] \"rice is often served in round bowls.\"       #> [4] \"better hash is made of rare beef.\"          #> [5] \"a bowl of rice is free with chicken stew.\""},{"path":"https://aoiferyan-sc.github.io/BertopicR/articles/modular_approach.html","id":"make-the-vectoriser","dir":"Articles","previous_headings":"Modules > Clustering","what":"Make the vectoriser","title":"Interacting with individaul modules","text":"vectoriser ’ll set ngram range c(1, 2) means topics can represented single words bigrams. ’ll set stop_words ‘english’ English stop words removed ’ll tell vectoriser consider words frequency 3 higher, rare words chance occurrences don’t clog representations much. practice, want set higher value min_frequency ’ll working significantly data.","code":"vectoriser <- bt_make_vectoriser(ngram_range = c(1, 2), stop_words = \"english\", min_frequency = 3L)"},{"path":"https://aoiferyan-sc.github.io/BertopicR/articles/modular_approach.html","id":"make-the-ctfidf-model","dir":"Articles","previous_headings":"Modules > Clustering","what":"Make the ctfidf model","title":"Interacting with individaul modules","text":"’ll create ctfidf model allow us represent topic according words important topic (high frequency) distinct topic (relatively low frequency topics):","code":"ctfidf <- bt_make_ctfidf(reduce_frequent_words = TRUE, bm25_weighting = FALSE)"},{"path":"https://aoiferyan-sc.github.io/BertopicR/articles/modular_approach.html","id":"compile-the-model","dir":"Articles","previous_headings":"Modules","what":"Compile the model","title":"Interacting with individaul modules","text":"’ve already made individual components, modules, selected parameters. ’ve already performed embeddings dimensionality reduction, bertopic allows us skip steps easily feeding empty models bt_compile_model function embedding reducing. can also skip clustering, won’t task adds extra complexity, clustering performed bt_do_clustering just explore clusterer work practice. N.B. practice need pause explore parameters depth.","code":"topic_model <- bt_compile_model(   embedding_model = bt_base_embedder(),   reduction_model = bt_base_reducer(),   clustering_model = clusterer,   vectoriser_model = vectoriser,   ctfidf_model = ctfidf ) #>  #> Model built"},{"path":"https://aoiferyan-sc.github.io/BertopicR/articles/modular_approach.html","id":"fit-the-model","dir":"Articles","previous_headings":"Modules","what":"Fit the model","title":"Interacting with individaul modules","text":"feed reduced embeddings rather original embeddings, allows us skip steps workflow; can save us lot time, particularly many documents. can create look table join sentences topic labels topic descriptions join information original dataframe: df compiled common place everything generated far, practice don’t really need cluster column now topic column, provide information. Topics clusters largely labels, discrepancy multiple clusters/topics size.","code":"bt_fit_model(topic_model, data$sentence, embeddings = reduced_embeddings) topic_representations <- topic_model$get_topic_info() topic_rep_lookup <- topic_representations %>%   select(topic = Topic, description = Name, topic_size = Count)  data <- data %>%   mutate(topic = topic_model$topics_) %>%   left_join(topic_rep_lookup) #> Joining with `by = join_by(topic)`  (data <- data %>%    relocate(sentence, topic, topic_size, description) ) #> # A tibble: 720 × 7 #>    sentence   topic topic_size description embeddings reduced_embeddings cluster #>    <chr>      <dbl>      <dbl> <chr>       <list>     <list>               <int> #>  1 the birch…    -1        213 -1_night_l… <dbl[…]>   <dbl [720 × 10]>        -1 #>  2 glue the …    -1        213 -1_night_l… <dbl[…]>   <dbl [720 × 10]>        -1 #>  3 it's easy…    43          6 43_pipe_te… <dbl[…]>   <dbl [720 × 10]>        39 #>  4 these day…     2         18 2_sweet_ra… <dbl[…]>   <dbl [720 × 10]>         2 #>  5 rice is o…     2         18 2_sweet_ra… <dbl[…]>   <dbl [720 × 10]>         2 #>  6 the juice…    14         10 14_tree_se… <dbl[…]>   <dbl [720 × 10]>        14 #>  7 the box w…    45          5 45_box_squ… <dbl[…]>   <dbl [720 × 10]>        50 #>  8 the hogs …    -1        213 -1_night_l… <dbl[…]>   <dbl [720 × 10]>        -1 #>  9 four hour…    -1        213 -1_night_l… <dbl[…]>   <dbl [720 × 10]>        -1 #> 10 a large s…    -1        213 -1_night_l… <dbl[…]>   <dbl [720 × 10]>        -1 #> # ℹ 710 more rows"},{"path":"https://aoiferyan-sc.github.io/BertopicR/articles/modular_approach.html","id":"changing-the-model-representation","dir":"Articles","previous_headings":"","what":"Changing the Model Representation","title":"Interacting with individaul modules","text":"happy topics/clusters formed, methods can use improve topic representations get better understanding topic . representation methods currently available : KeyBERT keyword extraction technique uses BERT embeddings represent topics appropriate keywords phrases. MaximalMarginalRelevance concept used select relevant keywords phrases promoting diversity keywords. balances relevance topic distinctiveness previously chosen keywords phrases using trade-parameter called lambda. OpenAI allows us use available models generate topic summaries. OpenAI API key required access api models. HuggingFace allows us use available models generate topic summaries. Unlike OpenAI, need API key completely free. However, models sophisticated OpenAI’s. Now trialed representation methods, can look compare default representations able get good idea topic . notice gpt-3.5 model gives coherent topic representation easy just take gospel chose topic title based . important remember, like representation methods, number input nr_repr_docs bt_representation_openai sent model large topic, documents may represent topic whole.","code":"representation_keybert <- bt_representation_keybert(fitted_model = topic_model,                                                     documents = sentences,                                                     document_embeddings = embeddings,                                                     embedding_model = embedder,                                                     top_n_words = 10,                                                     nr_repr_docs = 50,                                                     nr_samples = 500,                                                     nr_candidate_words = 100)  representation_mmr <- bt_representation_mmr(fitted_model = topic_model,                                             embedding_model = embedder,                                             diversity = 0.5)   representation_openai <- bt_representation_openai(fitted_model = topic_model,                                                   documents = sentences,                                                   openai_model = \"gpt-3.5-turbo\",                                                   nr_repr_docs = 10,                                                   chat = TRUE,                                                   api_key = \"sk-\")  representation_hf <- bt_representation_hf(fitted_model = topic_model,                                           documents = sentences,                                           task = \"text2text-generation\",                                           hf_model = \"google/flan-t5-base\",                                           default_prompt = \"keywords\") topic_representations <- topic_model$get_topic_info() %>%   mutate(keybert = representation_keybert,          mmr = representation_mmr,          # openai = representation_openai,          flanT5 = representation_hf) %>%   select(-Representative_Docs)  topic_representations %>% DT::datatable(options = list(scrollX = TRUE))"},{"path":[]},{"path":"https://aoiferyan-sc.github.io/BertopicR/articles/modular_approach.html","id":"merging-topics","dir":"Articles","previous_headings":"Modifying Topics","what":"Merging Topics","title":"Interacting with individaul modules","text":"Particularly using hdbscan can end large number topics can useful merge topics think suitably similar. can get certain idea topic descriptions already generated, can also useful look data closely merging.","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/articles/modular_approach.html","id":"hierarchical-clustering","dir":"Articles","previous_headings":"Modifying Topics > Merging Topics","what":"Hierarchical Clustering","title":"Interacting with individaul modules","text":"Hdbscan clustering forms clusters hierarchical processes can visualise dendrogram. can useful merging topics can see clusters split become topics emerged topic modelling process. x-axis measure distance topic embeddings, clusters split higher x-value larger distance embeddings. can see particular dataset, clusters split final topics quite early hierarchy might appropriate merge topics based emerged hierarchy. hierarchical structure based topics emerge based similarity embeddings, however, can often find topics think merged based knowledge. example, despite embeddings relatively large distance , topic 2 14 appear food. larger topics use ParseR analyse topic bigrams , since topics relatively small, can just examine exemplars. pretty happy two topics merged larger “food” topic, use bt_merge_topics function: maintaining dataframe along tracking step ’ve completed, good now update dataframe new topics.","code":"hierarchical_topics <- topic_model$hierarchical_topics(sentences) topic_model$visualize_hierarchy(hierarchical_topics = hierarchical_topics)$show() #>    0%|          | 0/56 [00:00<?, ?it/s]  61%|######    | 34/56 [00:00<00:00, 331.22it/s] 100%|##########| 56/56 [00:00<00:00, 343.23it/s] topic_representations %>%    filter(Topic %in% c(2,14)) #>   Topic Count                     Name #> 1     2    18 2_sweet_rare_covered_far #> 2    14    10   14_tree_sent_knife_ice #>                                                    Representation #> 1 sweet, rare, covered, far, cup, comes, china, bowl, food, drink #> 2      tree, sent, knife, ice, lack, cut, kept, makes, used, fine #>                                                 keybert #> 1 brown_food_bowl_taste_round_drink_covered_cup_hot_old #> 2     ice_knife_kept_tree_used_cut_makes_lack_fine_sent #>                                                      mmr #> 1 cup_china_bowl_drink_food_covered_rare_far_comes_sweet #> 2      knife_ice_cut_sent_used_kept_makes_fine_lack_tree #>                             flanT5 #> 1 china tea bowl with a cup of tea #> 2                        ice knife data %>%   filter(topic %in% c(2,14)) %>%    select(sentence, topic) #> # A tibble: 28 × 2 #>    sentence                                 topic #>    <chr>                                    <dbl> #>  1 these days a chicken leg is a rare dish.     2 #>  2 rice is often served in round bowls.         2 #>  3 the juice of lemons makes fine punch.       14 #>  4 a pot of tea helps to pass the evening.      2 #>  5 a cup of sugar makes sweet fudge.            2 #>  6 the fruit peel was cut in thick slices.     14 #>  7 a pound of sugar costs more than eggs.       2 #>  8 he ordered peach pie with ice cream.        14 #>  9 the fruit of a fig tree is apple shaped.    14 #> 10 fruit flavors are used in fizz drinks.      14 #> # ℹ 18 more rows bt_merge_topics(fitted_model = topic_model,                 documents = sentences,                 topics_to_merge = list(2, 14)) data <- data %>%   mutate(merged_topics = topic_model$topics_)"},{"path":"https://aoiferyan-sc.github.io/BertopicR/articles/modular_approach.html","id":"reducing-outliers","dir":"Articles","previous_headings":"Modifying Topics","what":"Reducing Outliers","title":"Interacting with individaul modules","text":"One feature hdbscan outlier category, can quite large. Sometimes might want redistribute outlier documents fall within one existing topics. number methods achieve good practice look different parameters different methods reducing outliers can quite difficult redistribute outlier documents maintaining clarity within topics. end, consider project goal implementing methods, important concise coherent topics force /documents topics, balance two? methods currently available us : Tokenset Similarity: Divides documents tokensets calculates c-TF-IDF cosine similarity tokenset topic. summation cosine similarity score topic across outlier document gives similar topic outlier document. Embeddings: Measures cosine similarty embeddings outlier document topic. passed empty embedding model bt_compile_model (), must specify embedding model used function. c-TF-IDF: Calculates c-TF-IDF cosine similarity outlier document topic redistributes outliers based topic highest similarity. can play outlier strategies , unlike merge topics fit model, bt_outlier_* functions update model, output df document, current topic classification potential new topics. must update model using bt_update_topics actually change topics within model. useful now look method redistributed outlier topics. graph shows outliers redistributed topics topic 12. can see strategy redistribute topics way, embedding strategy example, found 6 outlier documents best represented topic 1, strategy found outlier documents best represented topic 1. embedding method also redistributed outlier documents, c-TF-IDF tokenset similarity methods left certain documents outliers. playing around threshold parameter, find good fit data chosen strategy, important.  take look documents redistributed topic redistributed deciding best strategy data. can quite laborious large amounts data many topics. dataset relatively small may appropriate, can use things like bigrams top terms outlier documents redistributed investigate line topics moved . settled new list topics happy , can update dataframe keeping. example, looking data decided Tokenset Similarity method appropriate: using model future, can also update model new topics. ’s ! ’ve completed basic topic modelling pipeline using bertopic! like deeper look else can using bertopic, refer BertopicR function documentation BERTopic python library, https://maartengr.github.io/BERTopic/index.html.","code":"outliers_ts_sim <- bt_outliers_tokenset_similarity(fitted_model = topic_model,                                                    documents = sentences,                                                    topics = topic_model$topics_,                                                    threshold = 0.1)  outliers_embed <- bt_outliers_embeddings(fitted_model = topic_model,                                          documents = sentences,                                          topics = topic_model$topics_,                                          embeddings = reduced_embeddings,                                          embedding_model = embedder,                                          threshold = 0.1)  outliers_ctfidf <- bt_outliers_ctfidf(fitted_model = topic_model,                                       documents = sentences,                                       topics = topic_model$topics_,                                       threshold = 0.1) data %>%   mutate(outliers_ts_sim = outliers_ts_sim$new_topics,          outliers_embed = outliers_embed$new_topics,          outliers_ctfidf = outliers_ctfidf$new_topics) %>%   filter(merged_topics == -1,          outliers_ctfidf < 12,          outliers_embed < 12,          outliers_ts_sim < 12) %>%   select(outliers_ts_sim, outliers_embed, outliers_ctfidf) %>%   pivot_longer(everything(), names_to = \"outlier_distribution_strategy\", values_to = \"topic\") %>%   ggplot(aes(x = as.factor(topic), fill = outlier_distribution_strategy)) +   geom_bar(position = position_dodge2(preserve = \"single\")) +   theme_minimal() +    labs(x = \"Numbers\",         y = \"Count\",         title = \"Number of outliers in each topic after redistribution\",        fill = \"Outlier redistribution strategy\") +   scale_fill_discrete(labels = c(outliers_ctfidf = \"c-TF-IDF\",                                outliers_embed = \"Embeddings\",                                outliers_ts_sim = \"Tokenset similarity\")) data <- data %>%   mutate(new_topics = outliers_ts_sim$new_topics)  data %>%   filter(merged_topics == -1) %>%   select(merged_topics, new_topics) #> # A tibble: 213 × 2 #>    merged_topics new_topics #>            <int>      <dbl> #>  1            -1         -1 #>  2            -1         32 #>  3            -1         -1 #>  4            -1         50 #>  5            -1         31 #>  6            -1          8 #>  7            -1         27 #>  8            -1         -1 #>  9            -1         53 #> 10            -1         37 #> # ℹ 203 more rows bt_update_topics(fitted_model = topic_model,                  documents = sentences,                  new_topics = outliers_ts_sim$new_topics)"},{"path":"https://aoiferyan-sc.github.io/BertopicR/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Jack Penzer. Maintainer. Aoife Ryan. Author.","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Ryan (2023). BertopicR: Wraps bertopic reticulate. https://jpcompartir.github.io/BertopicR/, https://aoiferyan-sc.github.io/BertopicR/.","code":"@Manual{,   title = {BertopicR: Wraps bertopic through reticulate},   author = {Aoife Ryan},   year = {2023},   note = {https://jpcompartir.github.io/BertopicR/, https://aoiferyan-sc.github.io/BertopicR/}, }"},{"path":"https://aoiferyan-sc.github.io/BertopicR/index.html","id":"bertopicr","dir":"","previous_headings":"","what":"Wraps bertopic through reticulate","title":"Wraps bertopic through reticulate","text":"goal BertopicR allow R users access bertopic’s topic modelling suite R via Reticulate. Bertopic written developed Maarten Grootendorst (contributors!). package aim implement every feature bertopic, designed specific end users mind may experienced programmers developers. may submit issues feature requests; however, may faster go direct original, Python library excellent documentation. [BERTopic documentation] tried stay true Python library, whilst developing package ‘R feel’ .e. uses functions OOP. places changed names arguments, example Python library BERTopic() takes hdbscan_model = x, opted clustering_model = x. reason hdbscan_model = artifact early days bertopic author wants ensure code backwards compatible, package developed now ’s likely author opt clustering_model =. changes aware . package currently installs exact version bertopic - 0.15.0, features introduced version take time , may never, reach package.","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Wraps bertopic through reticulate","text":"installing bertopic make sure miniconda installed, don’t: reticulate miniconda installed, can install development version BertopicR GitHub : receive message: “bertopic installed packages current environment…” run:","code":"library(reticulate) #install.packages(\"reticulate\") if you don't have this already or aren't sure how to install.  reticulate::install_miniconda() # install.packages(\"devtools\") devtools::install_github(\"jpcompartir/BertopicR\")  library(BertopicR)  #Check your environment has been loaded correctly and bertopic has been installed: BertopicR::check_python_dependencies() BertopicR::install_python_dependencies()"},{"path":"https://aoiferyan-sc.github.io/BertopicR/index.html","id":"quickstart","dir":"","previous_headings":"","what":"Quickstart","title":"Wraps bertopic through reticulate","text":"BertopicR ships dataset unstructured text data bert_example_data","code":"data <- BertopicR::bert_example_data  embedder <- bt_make_embedder(\"all-minilm-l6-v2\") embeddings <- bt_do_embedding(embedder, documents = data$message,  batch_size = 16L) #>  #> Embedding proccess finished #> all-minilm-l6-v2 added to embeddings attributes   reducer <- bt_make_reducer(n_neighbors = 10L, n_components = 10L, metric = \"cosine\") clusterer <- bt_make_clusterer_hdbscan(min_cluster_size = 20L, metric = \"euclidean\", cluster_selection_method = \"eom\", min_samples = 10L)  topic_model <- bt_compile_model(embedding_model = embedder,                                 reduction_model = reducer,                                 clustering_model = clusterer) #>  #> No vectorising model provided, creating model with default parameters #>  #> No ctfidf model provided, creating model with default parameters #>  #> Model built  #Fit the model fitted_model <- bt_fit_model(model = topic_model,                               documents = data$message,                               embeddings = embeddings)  fitted_model$get_topic_info() %>%   dplyr::tibble() #> # A tibble: 34 × 3 #>    Topic Count Name                                                  #>    <dbl> <dbl> <chr>                                                 #>  1    -1  1760 -1_music_honor_amazing_spanish                        #>  2     0   197 0_celebrates_month celebration_october_heritage month #>  3     1   185 1_heritage night_festival_night_heritage celebration  #>  4     2   177 2_teachers_grade_students_parents                     #>  5     3   170 3_white_black_like_really                             #>  6     4   130 4_world_going_latina_got                              #>  7     5   125 5_awesome_yes_love_cool                               #>  8     6   119 6_utsw_hispanics_hispanicheritage_news                #>  9     7   108 7_latinx_latina_ll_chance                             #> 10     8    96 8_honored_community_honor_hosting                     #> # ℹ 24 more rows"},{"path":"https://aoiferyan-sc.github.io/BertopicR/index.html","id":"error-messages-and-causes","dir":"","previous_headings":"","what":"Error Messages and causes","title":"Wraps bertopic through reticulate","text":"““” Error py_call_impl(callable, dotsargs, dotskeywords) : ValueError: max_df corresponds < documents min_df\"\"\" usually caused using relatively small data. Lower min_frequency argument inbt_make_vectoriser()` use data fix .","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bert_example_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Example bert data — bert_example_data","title":"Example bert data — bert_example_data","text":"Example text data frame old social media API","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bert_example_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Example bert data — bert_example_data","text":"","code":"bert_example_data"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bert_example_data.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Example bert data — bert_example_data","text":"data frame 4,035 rows 1 column message Text variable, common variable SegmentR functions","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bertopic_detach.html","id":null,"dir":"Reference","previous_headings":"","what":"Detach bertopic from the python session — bertopic_detach","title":"Detach bertopic from the python session — bertopic_detach","text":"Call finished topic modelling process. Although, safer may simply save work restart R session, Python session still running (far know, way safely close)","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bertopic_detach.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Detach bertopic from the python session — bertopic_detach","text":"","code":"bertopic_detach()"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bertopic_detach.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Detach bertopic from the python session — bertopic_detach","text":"Nothing","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_base_clusterer.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a base clusterer for skipping clustering step of bertopic pipeline — bt_base_clusterer","title":"Create a base clusterer for skipping clustering step of bertopic pipeline — bt_base_clusterer","text":"Create base clusterer skipping clustering step bertopic pipeline","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_base_clusterer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a base clusterer for skipping clustering step of bertopic pipeline — bt_base_clusterer","text":"","code":"bt_base_clusterer()"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_base_clusterer.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a base clusterer for skipping clustering step of bertopic pipeline — bt_base_clusterer","text":"empty clustering model (Python class)","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_base_clusterer.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a base clusterer for skipping clustering step of bertopic pipeline — bt_base_clusterer","text":"","code":"base_clusterer <- bt_base_clusterer()  clusterer <- bt_base_clusterer()"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_base_embedder.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a base embedder for skipping embedding step of bertopic pipeline — bt_base_embedder","title":"Create a base embedder for skipping embedding step of bertopic pipeline — bt_base_embedder","text":"Create base embedder skipping embedding step bertopic pipeline","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_base_embedder.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a base embedder for skipping embedding step of bertopic pipeline — bt_base_embedder","text":"","code":"bt_base_embedder()"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_base_embedder.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a base embedder for skipping embedding step of bertopic pipeline — bt_base_embedder","text":"empty embedding model (Python class)","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_base_embedder.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a base embedder for skipping embedding step of bertopic pipeline — bt_base_embedder","text":"","code":"base_emebdder <- bt_base_embedder()  embedder <- bt_base_embedder()"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_base_reducer.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a base reducer for skipping dimensionality reduction step of bertopic pipeline — bt_base_reducer","title":"Create a base reducer for skipping dimensionality reduction step of bertopic pipeline — bt_base_reducer","text":"Create base reducer skipping dimensionality reduction step bertopic pipeline","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_base_reducer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a base reducer for skipping dimensionality reduction step of bertopic pipeline — bt_base_reducer","text":"","code":"bt_base_reducer()"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_base_reducer.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a base reducer for skipping dimensionality reduction step of bertopic pipeline — bt_base_reducer","text":"empty dimensionality reduction model (Python class)","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_base_reducer.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a base reducer for skipping dimensionality reduction step of bertopic pipeline — bt_base_reducer","text":"","code":"base_reducer <- bt_base_reducer()  reducer <- bt_base_reducer()"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_compile_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Build a BERTopic model — bt_compile_model","title":"Build a BERTopic model — bt_compile_model","text":"Keep *_model = NULL proceed model made default parameters (see individual make_* function parameters). However, advisable accept default parameters model; tune model according dataset business question answering.","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_compile_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build a BERTopic model — bt_compile_model","text":"","code":"bt_compile_model(   ...,   embedding_model = NULL,   reduction_model = NULL,   clustering_model = NULL,   vectoriser_model = NULL,   ctfidf_model = NULL )"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_compile_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build a BERTopic model — bt_compile_model","text":"... Additional arguments sent bertopic.BERTopic() embedding_model Model creating embeddings (Python object) reduction_model Model reducing embeddings' dimensions (Python object) clustering_model Model clustering (Python object) vectoriser_model Model vectorising input topic representations (Python object) ctfidf_model Model performing class-based tf-idf (ctf-idf) (Python object)","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_compile_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build a BERTopic model — bt_compile_model","text":"BERTopic model","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_do_clustering.html","id":null,"dir":"Reference","previous_headings":"","what":"Cluster your data — bt_do_clustering","title":"Cluster your data — bt_do_clustering","text":"Cluster data","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_do_clustering.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cluster your data — bt_do_clustering","text":"","code":"bt_do_clustering(clustering_model, embeddings)"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_do_clustering.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cluster your data — bt_do_clustering","text":"clustering_model Python object, output bt_make_clusterer* embeddings Embeddings, output bt_do_embedding bt_do_reducing","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_do_clustering.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cluster your data — bt_do_clustering","text":"Cluster labels document","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_do_embedding.html","id":null,"dir":"Reference","previous_headings":"","what":"Embed your documents — bt_do_embedding","title":"Embed your documents — bt_do_embedding","text":"Takes document, list documents, returns numerical embedding can used features machine learning model semantic similarity search. pre-computed embeddings can skip step. bt_embed function designed used one step topic modelling pipeline.","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_do_embedding.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Embed your documents — bt_do_embedding","text":"","code":"bt_do_embedding(   embedder,   documents,   ...,   accelerator = \"mps\",   progress_bar = TRUE )"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_do_embedding.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Embed your documents — bt_do_embedding","text":"embedder embedding model (output bt_make_embedder) documents character vector documents embedded, e.g. text variable ... Optional additional parameters passed SentenceTransformer's encode function, e.g. batch_size accelerator string containing name hardware accelerator, e.g. \"mps\", \"cuda\". NULL acceleartor used. progress_bar logical value indicating whether progress bar shown console","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_do_embedding.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Embed your documents — bt_do_embedding","text":"array floating point numbers","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_do_embedding.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Embed your documents — bt_do_embedding","text":"Initially function built upon sentence_transformers Python library, may expanded accept frameworks. feed documents list. can use hardware accelerators e.g. GPUs, speed computation. function currently returns object two additional attributes: embedding_model, n_documents, appended embeddings extraction later steps pipeline, e.g. merging data frames later important check many documents entered.","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_do_reducing.html","id":null,"dir":"Reference","previous_headings":"","what":"Perform dimensionality reduction on your embeddings — bt_do_reducing","title":"Perform dimensionality reduction on your embeddings — bt_do_reducing","text":"Perform dimensionality reduction embeddings","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_do_reducing.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Perform dimensionality reduction on your embeddings — bt_do_reducing","text":"","code":"bt_do_reducing(reducer, embeddings)"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_do_reducing.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perform dimensionality reduction on your embeddings — bt_do_reducing","text":"reducer dimensionality reduction model embeddings embeddings","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_do_reducing.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Perform dimensionality reduction on your embeddings — bt_do_reducing","text":"Embeddingd reduced number dimensions","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_fit_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit a topic model on your documents & embeddings — bt_fit_model","title":"Fit a topic model on your documents & embeddings — bt_fit_model","text":"already performed dimensionality reduction embeddings, can feed reduced dimension embeddings embeddings argument, make sure supply bt_compile_model base reducer (output bt_base_reducer()) NOTE: bertopic model working pointer python object point memory. means input output model differentiated without explicitly saving model performing operation. model returned function changes input model.","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_fit_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit a topic model on your documents & embeddings — bt_fit_model","text":"","code":"bt_fit_model(model, documents, embeddings, topic_labels = NULL)"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_fit_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit a topic model on your documents & embeddings — bt_fit_model","text":"model Output bt_compile_model() another bertopic topic model documents documents topic model embeddings embeddings, can reduced dimensionality topic_labels Pre-existing labels, supervised topic modelling","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_fit_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit a topic model on your documents & embeddings — bt_fit_model","text":"fitted BERTopic model","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_fit_transform_model.html","id":null,"dir":"Reference","previous_headings":"","what":"fit a bertopic model to cleaned data — bt_fit_transform_model","title":"fit a bertopic model to cleaned data — bt_fit_transform_model","text":"fit bertopic model cleaned data","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_fit_transform_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"fit a bertopic model to cleaned data — bt_fit_transform_model","text":"","code":"bt_fit_transform_model(   cleaned_text,   calculated_embeddings = NULL,   reducer = NULL,   min_topic_size = 10,   nr_topics = NULL,   ngram_range = c(1, 1),   embedding_model = \"all-MiniLM-L6-v2\",   accelerator = \"mps\",   diversity = 0.1,   stopwords = TRUE,   random_state = NULL )"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_fit_transform_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"fit a bertopic model to cleaned data — bt_fit_transform_model","text":"cleaned_text cleaned text column model fit calculated_embeddings embeddings already calculated input embeddings matrix reducer dimensionality reduction model send BERTopic() min_topic_size minimum topic size nr_topics number topics find within dataset ngram_range ngram range topic representation embedding_model embedding model use used produce embeddings accelerator accelerator use - default mps, use NULL none diversity diversity topic representation (1 = diverse, 0 = diverse) stopwords whether remove stopwords topic representations random_state random state pass umap","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_fit_transform_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"fit a bertopic model to cleaned data — bt_fit_transform_model","text":"list containing fitted bertopic model embeddings used fit model","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_clusterer.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a clustering model — bt_make_clusterer","title":"Create a clustering model — bt_make_clusterer","text":"Instantiates clustering model can fed BertopicR pipeline.","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_clusterer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a clustering model — bt_make_clusterer","text":"","code":"bt_make_clusterer(   ...,   clustering_method = c(\"hdbscan\", \"kmeans\", \"agglomerative\", \"base\") )"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_clusterer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a clustering model — bt_make_clusterer","text":"... Arguments fed clustering function determined clustering_method = clustering_method string defining clustering method use","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_clusterer.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a clustering model — bt_make_clusterer","text":"clustering model (Python object)","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_clusterer.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a clustering model — bt_make_clusterer","text":"Available clustering models 'hdbscan', 'kmeans', 'agglomerative', 'base'. options, see either hdbscan, sklearn official documentation lists arguments can fed clustering model. type \"?bt_make_clusterer_hdbscan\", \"?bt_make_clusterer_kmeans\" etc. see named arguments supplied default clustering model. Executing \"bt_make_clusterer(clustering_method = 'base')\" equivalent calling `\"bt_base_clusterer()\" returns empty model used streamlining topic modelling process.","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_clusterer.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a clustering model — bt_make_clusterer","text":"","code":"empty_clustering_model <- bt_make_clusterer(clustering_method = \"base\")  hdbscan_model <- bt_make_clusterer(clustering_method = \"hdbscan\", min_cluster_size = 10L)  kmeans_model <- bt_make_clusterer(clustering_method = \"kmeans\", n_clusters = 10L)"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_clusterer_agglomerative.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an Agglomerative Clustering clustering model — bt_make_clusterer_agglomerative","title":"Create an Agglomerative Clustering clustering model — bt_make_clusterer_agglomerative","text":"Create Agglomerative Clustering clustering model","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_clusterer_agglomerative.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an Agglomerative Clustering clustering model — bt_make_clusterer_agglomerative","text":"","code":"bt_make_clusterer_agglomerative(n_clusters = 20L)"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_clusterer_agglomerative.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an Agglomerative Clustering clustering model — bt_make_clusterer_agglomerative","text":"n_clusters number clusters search (enter integer typing L number)","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_clusterer_agglomerative.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an Agglomerative Clustering clustering model — bt_make_clusterer_agglomerative","text":"Agglomerative Clustering clustering model (Python object)","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_clusterer_agglomerative.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create an Agglomerative Clustering clustering model — bt_make_clusterer_agglomerative","text":"","code":"clustering_model <- bt_make_clusterer_agglomerative(15L)  agglomerative_model <- bt_make_clusterer_agglomerative(n_clusters = 10L)"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_clusterer_hdbscan.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an HDBSCAN clustering model — bt_make_clusterer_hdbscan","title":"Create an HDBSCAN clustering model — bt_make_clusterer_hdbscan","text":"Instantiates HDBSCAN clustering model using hdbscan Python library.","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_clusterer_hdbscan.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an HDBSCAN clustering model — bt_make_clusterer_hdbscan","text":"","code":"bt_make_clusterer_hdbscan(   ...,   min_cluster_size = 10L,   min_samples = 10L,   metric = \"euclidean\",   cluster_selection_method = \"eom\",   prediction_data = FALSE )"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_clusterer_hdbscan.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an HDBSCAN clustering model — bt_make_clusterer_hdbscan","text":"... Additional arguments sent hdbscan.HDBSCAN() min_cluster_size Minimum number data points cluster, enter integer adding L number min_samples Controls number outliers generated, lower value = fewer outliers. Defaults min_cluster_size metric Distance metric calculate clusters cluster_selection_method method used select clusters. Default \"eom\". prediction_data Set TRUE intend using model functions hdbscan.prediction eg. using bt_outliers_probabilities","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_clusterer_hdbscan.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an HDBSCAN clustering model — bt_make_clusterer_hdbscan","text":"instance HDBSCAN clustering model (Python object.","code":""},{"path":[]},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_clusterer_hdbscan.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create an HDBSCAN clustering model — bt_make_clusterer_hdbscan","text":"","code":"clustering_model <- bt_make_clusterer_hdbscan(metric = \"minkowski\")"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_clusterer_kmeans.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a kmeans clustering model — bt_make_clusterer_kmeans","title":"Create a kmeans clustering model — bt_make_clusterer_kmeans","text":"Create kmeans clustering model","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_clusterer_kmeans.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a kmeans clustering model — bt_make_clusterer_kmeans","text":"","code":"bt_make_clusterer_kmeans(n_clusters = 10L)"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_clusterer_kmeans.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a kmeans clustering model — bt_make_clusterer_kmeans","text":"n_clusters number clusters search (enter integer typing L number)","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_clusterer_kmeans.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a kmeans clustering model — bt_make_clusterer_kmeans","text":"kMeans clustering model (Python object)","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_clusterer_kmeans.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a kmeans clustering model — bt_make_clusterer_kmeans","text":"","code":"clustering_model <- bt_make_clusterer_kmeans(15L)  kmeans_model <- bt_make_clusterer_kmeans(n_clusters = 10L)"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_ctfidf.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an instance of the ClassTfidfTransformer from the bertopic.vectorizers module — bt_make_ctfidf","title":"Create an instance of the ClassTfidfTransformer from the bertopic.vectorizers module — bt_make_ctfidf","text":"function creates instance ClassTfidfTransformer bertopic.vectorizers module, provided arguments. used generate representations topics selecting words frequent within topic less frequent entire corpus.","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_ctfidf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an instance of the ClassTfidfTransformer from the bertopic.vectorizers module — bt_make_ctfidf","text":"","code":"bt_make_ctfidf(reduce_frequent_words = TRUE, bm25_weighting = FALSE)"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_ctfidf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an instance of the ClassTfidfTransformer from the bertopic.vectorizers module — bt_make_ctfidf","text":"reduce_frequent_words frequent words reduced? Default TRUE. bm25_weighting BM25 weighting used? Default FALSE.","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_ctfidf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an instance of the ClassTfidfTransformer from the bertopic.vectorizers module — bt_make_ctfidf","text":"ctfidf model (Python object).","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_df.html","id":null,"dir":"Reference","previous_headings":"","what":"make a df combining bertopic output with columns in original data export — bt_make_df","title":"make a df combining bertopic output with columns in original data export — bt_make_df","text":"make df combining bertopic output columns original data export","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"make a df combining bertopic output with columns in original data export — bt_make_df","text":"","code":"bt_make_df( df, model, embeddings, text_var = message, date_var = created_time)"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_df.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"make a df combining bertopic output with columns in original data export — bt_make_df","text":"df original Sprinklr export topic modelling performed merge bertopic output model bertopic model embeddings embeddings used generate model text_var original, uncleaned text column, text used fit model date_var date column df corresponding text used fit model","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_df.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"make a df combining bertopic output with columns in original data export — bt_make_df","text":"df bertopic output merged input columns sprinkr export","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_embedder.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an embedding model — bt_make_embedder","title":"Create an embedding model — bt_make_embedder","text":"Initially function built upon sentence_transformers Python library, may expanded accept frameworks. feed documents list. can use hardware accelerators e.g. GPUs, speed computation.","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_embedder.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an embedding model — bt_make_embedder","text":"","code":"bt_make_embedder(model_name)"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_embedder.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an embedding model — bt_make_embedder","text":"model_name Name embedding model string (case sensitive)","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_embedder.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an embedding model — bt_make_embedder","text":"Python object","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_embedder.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create an embedding model — bt_make_embedder","text":"","code":"embedder <- bt_make_embedder(\"all-mpnet-base-v2\")  embedder <- bt_make_embedder(\"aLL-minilm-l6-v2\")"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_reducer_pca.html","id":null,"dir":"Reference","previous_headings":"","what":"Create pca dimensionality reduction model — bt_make_reducer_pca","title":"Create pca dimensionality reduction model — bt_make_reducer_pca","text":"function wraps PCA functionality Python's sklearn package use R via reticulate. allows perform dimension reduction high-dimensional data, intended use BertopicR pipeline. concerned processing time, likely want reduce dimensions dataset . case, compiling model bt_compile_model call reducer <- bt_base_reducer().","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_reducer_pca.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create pca dimensionality reduction model — bt_make_reducer_pca","text":"","code":"bt_make_reducer_pca(..., n_components, svd_solver = \"auto\")"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_reducer_pca.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create pca dimensionality reduction model — bt_make_reducer_pca","text":"... Sent sklearn.decomposition UMAP function adding additional arguments n_components Number components keep svd_solver method reducing components can auto, full, arpack, randomized","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_reducer_pca.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create pca dimensionality reduction model — bt_make_reducer_pca","text":"PCA Model can input bt_do_reducing reduce dimensions data","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_reducer_truncated_svd.html","id":null,"dir":"Reference","previous_headings":"","what":"Created Truncated SVD dimensionality reduction model — bt_make_reducer_truncated_svd","title":"Created Truncated SVD dimensionality reduction model — bt_make_reducer_truncated_svd","text":"function wraps Truncated SVD (Single Value Decomposition) functionality Python's sklearn package use R via reticulate. allows perform dimension reduction high-dimensional data. intended use BertopicR pipeline. concerned processing time, likely want reduce dimensions dataset . case, compiling model bt_compile_model call reducer <- bt_base_reducer().","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_reducer_truncated_svd.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Created Truncated SVD dimensionality reduction model — bt_make_reducer_truncated_svd","text":"","code":"bt_make_reducer_truncated_svd(   ...,   n_components,   n_iter = 5,   svd_solver = \"randomized\" )"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_reducer_truncated_svd.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Created Truncated SVD dimensionality reduction model — bt_make_reducer_truncated_svd","text":"... Sent sklearn.decomposition Truncated SVD function adding additional arguments n_components Number components keep n_iter Number iterations randomised svd solver. used svd solver \"arpack\". svd_solver method reducing components can arpack randomized","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_reducer_truncated_svd.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Created Truncated SVD dimensionality reduction model — bt_make_reducer_truncated_svd","text":"Truncated SVD Model can input bt_do_reducing reduce dimensions data","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_reducer_umap.html","id":null,"dir":"Reference","previous_headings":"","what":"Create umap dimensionality reduction model — bt_make_reducer_umap","title":"Create umap dimensionality reduction model — bt_make_reducer_umap","text":"function wraps UMAP functionality Python's umap-learn package use R via reticulate. allows perform dimension reduction high-dimensional data, intended use BertopicR pipeline/","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_reducer_umap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create umap dimensionality reduction model — bt_make_reducer_umap","text":"","code":"bt_make_reducer_umap(   ...,   n_neighbors = 15L,   n_components = 5L,   min_dist = 0,   metric = \"euclidean\",   random_state = 42L,   low_memory = FALSE,   verbose = TRUE )"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_reducer_umap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create umap dimensionality reduction model — bt_make_reducer_umap","text":"... Sent umap$UMAP adding additional arguments n_neighbors size local neighbourhood (terms number neighboring data points) used manifold approximation (default: 15). n_components number dimensions reduce (default: 5). min_dist minimum distance points low-dimensional representation (default: 0.0). metric metric use distance computation (default: \"euclidean\"). random_state seed used random number generator (default: 42). low_memory Loogical, use low memory version UMAP (default: FALSE) verbose Logical flag indicating whether report progress dimension reduction (default: TRUE).","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_reducer_umap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create umap dimensionality reduction model — bt_make_reducer_umap","text":"UMAP Model can input bt_do_reducing reduce dimensions data","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_reducer_umap.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create umap dimensionality reduction model — bt_make_reducer_umap","text":"concerned processing time, likely want reduce dimensions dataset . case, compiling model bt_compile_model call reducer <- bt_base_reducer(). low_memory = TRUE currently inadvisable trial error suggests results robust later clustering.","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_vectoriser.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a text vectoriser — bt_make_vectoriser","title":"Create a text vectoriser — bt_make_vectoriser","text":"function uses Python's sklearn feature extraction count vectorisation. creates CountVectorizer object specified parameters. CountVectorizer way convert text data vectors model input. Used inside BertopicR topc modelling pipeline.","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_vectoriser.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a text vectoriser — bt_make_vectoriser","text":"","code":"bt_make_vectoriser(   ...,   ngram_range = c(1, 2),   stop_words = \"english\",   min_frequency = 10L,   max_features = NULL )"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_vectoriser.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a text vectoriser — bt_make_vectoriser","text":"... Additional parameters passed sklearn's CountVectorizer ngram_range vector length 2 (default c(1, 2)) indicating lower upper boundary range n-values different word n-grams char n-grams extracted features. values n min_n <= n <= max_n used. example ngram_range c(1, 1) means unigrams, c(1, 2) means unigrams bigrams, c(2, 2) means bigrams. stop_words String (default 'english'). string, passed _check_stop_list appropriate stop list returned. 'english' currently default. min_frequency Integer (default 10L). building vocabulary ignore terms corpus frequency strictly lower given threshold. max_features Integer NULL (default NULL). NULL, build vocabulary considers top max_features ordered term frequency across corpus.","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_make_vectoriser.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a text vectoriser — bt_make_vectoriser","text":"sklearn CountVectorizer object configured provided parameters","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_merge_topics.html","id":null,"dir":"Reference","previous_headings":"","what":"Merges list(s) of topics together — bt_merge_topics","title":"Merges list(s) of topics together — bt_merge_topics","text":"Merge topics already-fitted BertopicR model. can feed list, merge topics together, list lists perform merges multiple groups topics. NOTE: bertopic model working pointer python object point memory. means input output model differentiated without explicitly saving model performing operation. model returned function changes input model.","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_merge_topics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merges list(s) of topics together — bt_merge_topics","text":"","code":"bt_merge_topics(fitted_model, documents, topics_to_merge)"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_merge_topics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merges list(s) of topics together — bt_merge_topics","text":"fitted_model Output bt_fit_model() another bertopic topic model. model must fitted data. documents documents model fitted topics_to_merge list (list lists/vectors) topics created bertopic model wish merge. Topics given numeric form.","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_merge_topics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Merges list(s) of topics together — bt_merge_topics","text":"bertopic model specified topics merged","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_merge_topics.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Merges list(s) of topics together — bt_merge_topics","text":"function updates model topics topics_to_merge list become 1 topic. grouped topics take topic representation (Name) first topic list. number topics can merged together, like merge two separate groups topics, must pass list lists/vectors topics_to_merge eg. list(c(1, 3), c(0, 6, 7)) list(list(1, 3), list(0, 6, 7))","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_merge_topics.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Merges list(s) of topics together — bt_merge_topics","text":"","code":"if (FALSE) bt_merge_topics( fitted_model = model, documents = documents, topics_to_merge = list(c(1, 3), c(0, 6, 7)))"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_outliers_ctfidf.html","id":null,"dir":"Reference","previous_headings":"","what":"Redistributes outliers using c-TF-IDF scores — bt_outliers_ctfidf","title":"Redistributes outliers using c-TF-IDF scores — bt_outliers_ctfidf","text":"Calculates cosine similarity c-TF-IDF documents topics redistributes outliers based topic highest similarity . Note purpose function obtain new list topics can used update model, make changes model , topic classification model outputs change running function. bt_update_topics function needs used make change model .","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_outliers_ctfidf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Redistributes outliers using c-TF-IDF scores — bt_outliers_ctfidf","text":"","code":"bt_outliers_ctfidf(fitted_model, documents, topics, threshold = 0.3)"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_outliers_ctfidf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Redistributes outliers using c-TF-IDF scores — bt_outliers_ctfidf","text":"fitted_model Output bt_fit_model() another bertopic topic model. model must fitted data. documents documents model fit topics current topics associated documents threshold minimum probability outlier reassigned","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_outliers_ctfidf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Redistributes outliers using c-TF-IDF scores — bt_outliers_ctfidf","text":"df document, old topic, new topic","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_outliers_embeddings.html","id":null,"dir":"Reference","previous_headings":"","what":"Redistributes outliers using embeddings — bt_outliers_embeddings","title":"Redistributes outliers using embeddings — bt_outliers_embeddings","text":"Uses cosine similarity document embeddings find topic closest outlier document reassigns documents accordingly. Note purpose function obtain new list topics can used update model, make changes model , topic classification model outputs change running function. bt_update_topics function needs used make change model .","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_outliers_embeddings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Redistributes outliers using embeddings — bt_outliers_embeddings","text":"","code":"bt_outliers_embeddings(   fitted_model,   documents,   topics,   embeddings,   embedding_model = NULL,   threshold = 0.3 )"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_outliers_embeddings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Redistributes outliers using embeddings — bt_outliers_embeddings","text":"fitted_model Output bt_fit_model() another bertopic topic model. model must fitted data. documents documents model fit topics current topics associated documents embeddings embeddings used create topics. embedding_model instantiate model embedding model need pass one threshold minimum probability outlier reassigned","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_outliers_embeddings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Redistributes outliers using embeddings — bt_outliers_embeddings","text":"df document, old topic, new topic","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_outliers_tokenset_similarity.html","id":null,"dir":"Reference","previous_headings":"","what":"Redistributes outliers using tokenset c-TF-IDF scores — bt_outliers_tokenset_similarity","title":"Redistributes outliers using tokenset c-TF-IDF scores — bt_outliers_tokenset_similarity","text":"Divides documents tokensets calculates c-TF-IDF similarity tokenset topic. outlier document, similarity scores tokenset topic summed together topic outlier redistributed topic highest similarity. Note purpose function obtain new list topics can used update model, make changes model , topic classification model outputs change running function. bt_update_topics function needs used make change model .","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_outliers_tokenset_similarity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Redistributes outliers using tokenset c-TF-IDF scores — bt_outliers_tokenset_similarity","text":"","code":"bt_outliers_tokenset_similarity(   ...,   fitted_model,   documents,   topics,   window = 4,   stride = 1,   threshold = 0.3 )"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_outliers_tokenset_similarity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Redistributes outliers using tokenset c-TF-IDF scores — bt_outliers_tokenset_similarity","text":"... Optional additional parameters passed approximate_distribution function, e.g. batch_size fitted_model Output bt_fit_model() another bertopic topic model. model must fitted data. documents documents model fit topics current topics associated documents window size moving window number tokens tokenset stride far window move step (number words skip moving next tokenset) threshold minimum probability outlier reassigned","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_outliers_tokenset_similarity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Redistributes outliers using tokenset c-TF-IDF scores — bt_outliers_tokenset_similarity","text":"df document, old topic, new topic","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_representation_hf.html","id":null,"dir":"Reference","previous_headings":"","what":"Use Huggingface models to create topic representation — bt_representation_hf","title":"Use Huggingface models to create topic representation — bt_representation_hf","text":"Use Huggingface models create topic representation","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_representation_hf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Use Huggingface models to create topic representation — bt_representation_hf","text":"","code":"bt_representation_hf(   ...,   fitted_model,   documents,   task,   hf_model,   default_prompt = \"keywords\",   custom_prompt = NULL,   nr_samples = 500,   nr_repr_docs = 20,   diversity = 10 )"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_representation_hf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Use Huggingface models to create topic representation — bt_representation_hf","text":"... arguments sent transformers.pipeline function fitted_model fitted bertopic model documents documents topic model fitted task Task defining pipeline returned. See https://huggingface.co/transformers/v3.0.2/main_classes/pipelines.html information. Use \"text-generation\" gpt-like models \"text2text-generation\" T5-like models hf_model model used pipeline make predictions default_prompt Whether use \"keywords\" \"documents\" default prompt. Passing custom_prompt render argument NULL. Default \"keywords\" prompt. custom_prompt custom prompt used pipeline. specified, \"keywords\" \"documents\" default_prompt used. Use \"[KEYWORDS]\" \"[DOCUMENTS]\" prompt decide keywords documents inserted. nr_samples Number sample documents representative docs chosen nr_repr_docs Number representative documents sent huggingface model diversity diversity documents sent huggingface model. 0 = diversity, 1 = max diversity.","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_representation_hf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Use Huggingface models to create topic representation — bt_representation_hf","text":"updated representation topic","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_representation_hf.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Use Huggingface models to create topic representation — bt_representation_hf","text":"Representative documents chosen topic sampling (nr_samples) number documents topic calculating documents representative topic c-tf-idf cosine similarity topic individual documents. representative documents (number defined nr_repr_docs parameter) extracted passed huggingface model topic description predicted.","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_representation_keybert.html","id":null,"dir":"Reference","previous_headings":"","what":"Create representation model using keybert — bt_representation_keybert","title":"Create representation model using keybert — bt_representation_keybert","text":"creates topic representations based KeyBERT algorithm.","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_representation_keybert.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create representation model using keybert — bt_representation_keybert","text":"","code":"bt_representation_keybert(   fitted_model,   documents,   document_embeddings,   embedding_model,   top_n_words = 10,   nr_repr_docs = 50,   nr_samples = 500,   nr_candidate_words = 100 )"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_representation_keybert.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create representation model using keybert — bt_representation_keybert","text":"fitted_model Output bt_fit_model() another bertopic topic model. model must fitted data. documents documents fitted_model fitted document_embeddings embeddings used fit model, dimensions specified embedder pass embedding model embedding_model model used create embeddings passed. used create word embeddings compared topic embeddings using cosine similarity. top_n_words number keywords/phrases extracted nr_repr_docs number documents used create topic embeddings nr_samples number samples select representative docs topic nr_candidate_words number words examine per topic","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_representation_keybert.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create representation model using keybert — bt_representation_keybert","text":"KeyBERTInspired representation model","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_representation_keybert.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create representation model using keybert — bt_representation_keybert","text":"KeyBERT python package used extraction key words documents. works : Selecting representative documents (nr_repr_docs) topic based c_tf_idf cosine similarity documents topics. achieved sampling nr_samples documents topic calculating c_tf_idf score choosing top nr_repr_docs . Candidate words (nr_candidate_words) selected topic based c_tf_idf scores topic Topic embeddings created averaging embeddings representative documents topic compared, using cosine similarity, candidate word embeddings give similarity score word topic top_n_words highest cosine similarity topic used represent topic","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_representation_mmr.html","id":null,"dir":"Reference","previous_headings":"","what":"Create representation model using Maximal Marginal Relevance — bt_representation_mmr","title":"Create representation model using Maximal Marginal Relevance — bt_representation_mmr","text":"Calculates maximal marginal relevance candidate words documents. Considers similarity keywords phrases already selected keywords phrases chooses representation based maximise diversity.","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_representation_mmr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create representation model using Maximal Marginal Relevance — bt_representation_mmr","text":"","code":"bt_representation_mmr(   fitted_model,   embedding_model,   diversity = 0.1,   top_n_words = 10 )"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_representation_mmr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create representation model using Maximal Marginal Relevance — bt_representation_mmr","text":"fitted_model Output bt_fit_model() another bertopic topic model. model must fitted data. embedding_model embedding model used embed keywords selected potential representative words. compatible sentence transformer models point. diversity diverse representation words/phrases . 0 = diverse, 1 = completely diverse top_n_words Number keywords/phrases extracted","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_representation_mmr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create representation model using Maximal Marginal Relevance — bt_representation_mmr","text":"MaximalMarginalRelevance representation model","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_representation_openai.html","id":null,"dir":"Reference","previous_headings":"","what":"Create representation model that uses OpenAI text generation models — bt_representation_openai","title":"Create representation model that uses OpenAI text generation models — bt_representation_openai","text":"Representative documents chosen topic sampling (nr_samples) number documents topic calculating documents representative topic c-tf-idf cosine similarity topic individual documents. representative documents (number defined nr_repr_docs parameter) extracted passed  OpenAI API generate topic labels based one Completion (chat = FALSE) ChatCompletion (chat = TRUE) models.","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_representation_openai.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create representation model that uses OpenAI text generation models — bt_representation_openai","text":"","code":"bt_representation_openai(   fitted_model,   documents,   openai_model = \"text-ada-001\",   nr_repr_docs = 10,   nr_samples = 500,   api_key = \"sk-\",   chat = FALSE,   delay_in_seconds = NULL,   prompt = NULL,   diversity = NULL )"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_representation_openai.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create representation model that uses OpenAI text generation models — bt_representation_openai","text":"fitted_model Output bt_fit_model() another bertopic topic model. model must fitted data. documents documents used fit fitted_model openai_model openai model use. using gpt-3.5 model, set chat = TRUE nr_repr_docs number representative documents per topic send openai model nr_samples Number sample documents representative docs chosen api_key openai ai api authentication key. can found openai account. chat set TRUE using gpt-3.5 model delay_in_seconds delay seconds consecutive prompts, avoid rate limit errors. prompt prompt used openai model. NULL, default prompt used. diversity diversity documents sent huggingface model. 0 = diversity, 1 = max diversity.","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_representation_openai.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create representation model that uses OpenAI text generation models — bt_representation_openai","text":"OpenAI representation model","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_update_topics.html","id":null,"dir":"Reference","previous_headings":"","what":"Update Topic Representations — bt_update_topics","title":"Update Topic Representations — bt_update_topics","text":"Updates topics representations based document-topic classification described list new_topics. initiating model bt_compile_model, want manipulate topic representations must use vectoriser/ctfidf model, can used bt_compile_model. NOTE: bertopic model working pointer python object point memory. means input output model differentiated without explicitly saving model performing operation. model returned function changes input model.","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_update_topics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update Topic Representations — bt_update_topics","text":"","code":"bt_update_topics(   fitted_model,   documents,   new_topics = NULL,   representation_model = NULL,   vectoriser_model = NULL,   ctfidf_model = NULL )"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_update_topics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update Topic Representations — bt_update_topics","text":"fitted_model Output bt_fit_model() another bertopic topic model. model must fitted data. documents documents model fit new_topics Topics update model representation_model model updating topic representations vectoriser_model Model vectorising input topic representations (Python object) ctfidf_model Model performing class-based tf-idf (ctf-idf) (Python object)","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_update_topics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Update Topic Representations — bt_update_topics","text":"updated model","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_update_topics.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Update Topic Representations — bt_update_topics","text":"NOTE: using function update outlier topics, may lead errors topic reduction topic merging techniques used afterwards. reason assign -1 document topic 1 another -1 document topic 2, unclear map -1 documents. matched topic 1 2.","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_viz_diff_terms.html","id":null,"dir":"Reference","previous_headings":"","what":"Function to find terms with the greatest difference between topics — bt_viz_diff_terms","title":"Function to find terms with the greatest difference between topics — bt_viz_diff_terms","text":"Function find terms greatest difference topics","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_viz_diff_terms.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function to find terms with the greatest difference between topics — bt_viz_diff_terms","text":"","code":"bt_viz_diff_terms(   merged_df,   text_var = text_clean,   topic_var = topic,   top_n = 15,   min_freq = 25,   include_outliers = FALSE,   type = c(\"lollipops\", \"bars\"),   plots = NULL )"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_viz_diff_terms.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function to find terms with the greatest difference between topics — bt_viz_diff_terms","text":"merged_df output makedf function.Can df includes topic column text_var text diff_terms extracted topic_var column containing topic variable top_n number terms extract min_freq minimum number times term appear considered include_outliers include outlier (-1) bertopic category? type lollipop bar chart plots specific plots output. input c(x, y) want 1 chart list(c(x, y), c(u, v)) x, y, u v topic numbers.","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_viz_diff_terms.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Function to find terms with the greatest difference between topics — bt_viz_diff_terms","text":"ggplot object top different terms pair topics","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_viz_top_terms.html","id":null,"dir":"Reference","previous_headings":"","what":"create top terms charts for topic modelling completed using bertopic — bt_viz_top_terms","title":"create top terms charts for topic modelling completed using bertopic — bt_viz_top_terms","text":"create top terms charts topic modelling completed using bertopic","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_viz_top_terms.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"create top terms charts for topic modelling completed using bertopic — bt_viz_top_terms","text":"","code":"bt_viz_top_terms(   merged_df,   text_var = text_clean,   topic_var = topic,   top_n = 15,   n_row = 2,   min_freq = 25,   include_outliers = FALSE,   type = c(\"lollipops\", \"bars\") )"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_viz_top_terms.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"create top terms charts for topic modelling completed using bertopic — bt_viz_top_terms","text":"merged_df df output makedf function.Can df includes topic column text_var text top terms extracted topic_var column containing topic variable top_n number terms extract n_row number rows plots take min_freq minimum number times term appear considered include_outliers include -1 (outlier) bertopic category type lollipop bar chart?","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/bt_viz_top_terms.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"create top terms charts for topic modelling completed using bertopic — bt_viz_top_terms","text":"list all_terms max_only top term bar charts","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/check_python_dependencies.html","id":null,"dir":"Reference","previous_headings":"","what":"Check that dependencies are loaded — check_python_dependencies","title":"Check that dependencies are loaded — check_python_dependencies","text":"Check dependencies loaded","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/check_python_dependencies.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check that dependencies are loaded — check_python_dependencies","text":"","code":"check_python_dependencies()"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/check_python_dependencies.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check that dependencies are loaded — check_python_dependencies","text":"message confirming whether dependencies loaded","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/install_python_dependencies.html","id":null,"dir":"Reference","previous_headings":"","what":"Install Python Dependencies — install_python_dependencies","title":"Install Python Dependencies — install_python_dependencies","text":"Install Python Dependencies","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/install_python_dependencies.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Install Python Dependencies — install_python_dependencies","text":"","code":"install_python_dependencies()"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/install_python_dependencies.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Install Python Dependencies — install_python_dependencies","text":"Nothing","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::%>% details.","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/pipe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pipe operator — %>%","text":"lhs value magrittr placeholder. rhs function call using magrittr semantics.","code":""},{"path":"https://aoiferyan-sc.github.io/BertopicR/reference/pipe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pipe operator — %>%","text":"result calling rhs(lhs).","code":""}]
