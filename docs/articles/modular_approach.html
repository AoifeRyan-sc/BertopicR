<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="BertopicR">
<title>Interacting with individaul modules • BertopicR</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.2.2/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.2.2/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Interacting with individaul modules">
<meta property="og:description" content="BertopicR">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">BertopicR</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.0.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="active nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/modular_approach.html">Interacting with individaul modules</a>
  </div>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav"></ul>
</div>

    
  </div>
</nav><div class="container template-article">



<script src="modular_approach_files/htmlwidgets-1.6.2/htmlwidgets.js"></script><link href="modular_approach_files/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet">
<script src="modular_approach_files/datatables-binding-0.28/datatables.js"></script><link href="modular_approach_files/dt-core-1.13.4/css/jquery.dataTables.min.css" rel="stylesheet">
<link href="modular_approach_files/dt-core-1.13.4/css/jquery.dataTables.extra.css" rel="stylesheet">
<script src="modular_approach_files/dt-core-1.13.4/js/jquery.dataTables.min.js"></script><link href="modular_approach_files/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet">
<script src="modular_approach_files/crosstalk-1.2.0/js/crosstalk.min.js"></script><div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Interacting with individaul modules</h1>
            
      
      
      <div class="d-none name"><code>modular_approach.Rmd</code></div>
    </div>

    
    
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://jpcompartir.github.io/BertopicR/" class="external-link">BertopicR</a></span><span class="op">)</span></span>
<span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://dplyr.tidyverse.org" class="external-link">dplyr</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://stringr.tidyverse.org" class="external-link">stringr</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyr.tidyverse.org" class="external-link">tidyr</a></span><span class="op">)</span></span></code></pre></div>
<div class="section level2">
<h2 id="modules">Modules<a class="anchor" aria-label="anchor" href="#modules"></a>
</h2>
<dl>
<dt>In the <a href="https://maartengr.github.io/BERTopic/getting_started/quickstart/quickstart.html" class="external-link">Bertopic
Python Library</a> by Maarten Grootendorst there are 6 sub-modules:</dt>
<dd>
<p>Embeddings - for transforming your documents into a numerical
representation</p>
<p>Dimensionality Reduction - for reducing the number of features of the
embeddings output</p>
<p>Clustering - finding groups of similar documents to represent as
topics</p>
<p>Vectorisers - find the n-grams to describe each topic</p>
<p>c-TF-IDF - create topic-level (rather than document) bag of words
matrices for representing topics</p>
<p>Fine-tuning topic representations - other tools for topic
representations (includes generative AI/LLMs)</p>
</dd>
</dl>
<p><img src="../reference/figures/schematic_of_modules.png" width="748"></p>
<p>This vignette will show you how to use {BertopicR} to tune each
module when creating your topic models.</p>
<div class="section level3">
<h3 id="data">Data<a class="anchor" aria-label="anchor" href="#data"></a>
</h3>
<p>For demonstrative purposes, we’ll use {stringr}‘s ’sentences’ data
set, which comes fairly clean. For help on cleaning text data visit
ParseR/LimpiaR documentation. Let’s take a look at the first five posts
for brevity.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sentences</span> <span class="op">&lt;-</span> <span class="fu">stringr</span><span class="fu">::</span><span class="va"><a href="https://stringr.tidyverse.org/reference/stringr-data.html" class="external-link">sentences</a></span></span>
<span><span class="va">sentences</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span><span class="op">]</span></span>
<span><span class="co">#&gt; [1] "The birch canoe slid on the smooth planks." </span></span>
<span><span class="co">#&gt; [2] "Glue the sheet to the dark blue background."</span></span>
<span><span class="co">#&gt; [3] "It's easy to tell the depth of a well."     </span></span>
<span><span class="co">#&gt; [4] "These days a chicken leg is a rare dish."   </span></span>
<span><span class="co">#&gt; [5] "Rice is often served in round bowls."</span></span></code></pre></div>
<p>Then we’ll turn the sentences into a data frame:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">df</span> <span class="op">&lt;-</span> <span class="fu">dplyr</span><span class="fu">::</span><span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html" class="external-link">tibble</a></span><span class="op">(</span>sentences <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/chartr.html" class="external-link">tolower</a></span><span class="op">(</span><span class="va">sentences</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="embeddings">Embeddings<a class="anchor" aria-label="anchor" href="#embeddings"></a>
</h3>
<p>In order to work efficiently with text data, we need to turn the
words into numbers. The current state-of-the-art approach to turning
text into numbers, is contextualised word embeddings. We’ll use an MpNet
model, ‘all-mpnet-base-v2’ to take our sentences and turn them into
numbers (embeddings). This will allow us to find similarities and
differences between our sentences, using standard mathematical
techniques (don’t worry if this isn’t making sense right now, often the
best way to learn is by doing).</p>
<p>In BertopicR you will usually either want to be making or doing with
modules, or, compiling or fitting with models. We make components, we do
actions on data with components. We compile our components into models,
and then we fit our models to data.</p>
<div class="section level4">
<h4 id="make-the-embedder">Make the embedder<a class="anchor" aria-label="anchor" href="#make-the-embedder"></a>
</h4>
<p>First we’ll make an embedder (or embedding_model) using the
<code>bt_make_embedder</code> function. Then we’ll embed our sentences
using the embedder.</p>
<p>TIP: It’s a good idea to save your embeddings, as when working with
many documents this process will be time consuming.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">embedder</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/bt_make_embedder.html">bt_make_embedder</a></span><span class="op">(</span></span>
<span>  model_name <span class="op">=</span> <span class="st">"all-mpnet-base-v2"</span></span>
<span>  <span class="op">)</span></span></code></pre></div>
</div>
<div class="section level4">
<h4 id="do-the-embedding">Do the embedding<a class="anchor" aria-label="anchor" href="#do-the-embedding"></a>
</h4>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">embeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/bt_do_embedding.html">bt_do_embedding</a></span><span class="op">(</span></span>
<span>  <span class="va">embedder</span>, </span>
<span>  <span class="va">df</span><span class="op">$</span><span class="va">sentences</span></span>
<span>  <span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Embedding proccess finished</span></span>
<span><span class="co">#&gt; all-mpnet-base-v2 added to embeddings attributes</span></span>
<span></span>
<span><span class="va">embeddings</span><span class="op">[</span><span class="fl">1</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">]</span></span>
<span><span class="co">#&gt;  [1]  0.0031732221 -0.0454455651 -0.0003524857  0.0046989759  0.0179691557</span></span>
<span><span class="co">#&gt;  [6] -0.0300240256 -0.0278541520 -0.0199615341 -0.0032402743  0.0257827863</span></span></code></pre></div>
<p>Each row of our embeddings output represents one of our original
sentences, and each column represents a different embedding dimension;
there are 768 dimensions outputted by the ‘all-mpnet-base-v2’ mode</p>
<p>We take a peek at the first 10 columns (dimensions), of the first row
of our embeddings and we see 10 floating point numbers.</p>
</div>
</div>
<div class="section level3">
<h3 id="reducing-dimensions">Reducing Dimensions<a class="anchor" aria-label="anchor" href="#reducing-dimensions"></a>
</h3>
<p>The next step in the pipeline is to reduce the dimensions of our
embeddings, we do this for two reasons:</p>
<ol style="list-style-type: decimal">
<li><p>to allow our clustering algorithm to run smoothly</p></li>
<li><p>to visualise our clusters on a plane (we will eventually reduce
to 2 dimensions)</p></li>
</ol>
<p>In machine learning more generally, dimensionality reduction is often
an important step to avoid overfitting and the curse of dimensionality.
In this example we will use the UMAP algorithm (however dimensionality
reduction using PCA and truncatedSVD are also currently available) for
more information on the UMAP algorithm - uniform manifold approximation
and projection for dimension reduction - <em>catchy</em>, see <a href="https://umap-learn.readthedocs.io/en/latest/" class="external-link">UMAP Docs</a>.</p>
<p>TIP: Like with embeddings, it’s a good idea to save your reduced
embeddings, as reducing dimensions can be a costly process.</p>
<div class="section level4">
<h4 id="make-the-reducer">Make the reducer<a class="anchor" aria-label="anchor" href="#make-the-reducer"></a>
</h4>
<p>We’ll use a low-ish value for n_neighbours (we have a small dataset)
and an output with 5 dimensions (n_components = 5L). We’ll set the
min_distance to 0, so that our dimensionality reduction model can place
very similar documents <em>very</em> close together. We’ll set the
metric to “Euclidean” see <a href="https://umap-learn.readthedocs.io/en/latest/embedding_space.html" class="external-link">Embedding
to non-Euclidean Spaces</a> for alternatives.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">reducer</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/bt_make_reducer_umap.html">bt_make_reducer_umap</a></span><span class="op">(</span></span>
<span>  n_neighbors <span class="op">=</span> <span class="fl">10L</span>, </span>
<span>  n_components <span class="op">=</span> <span class="fl">10L</span>,</span>
<span>  min_dist <span class="op">=</span> <span class="fl">0L</span>,</span>
<span>  metric <span class="op">=</span> <span class="st">"euclidean"</span></span>
<span>  <span class="op">)</span></span></code></pre></div>
</div>
<div class="section level4">
<h4 id="do-the-reducing">Do the reducing<a class="anchor" aria-label="anchor" href="#do-the-reducing"></a>
</h4>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">reduced_embeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/bt_do_reducing.html">bt_do_reducing</a></span><span class="op">(</span></span>
<span>  <span class="va">reducer</span>, embeddings <span class="op">=</span> <span class="va">embeddings</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; UMAP(low_memory=False, min_dist=0, n_components=10, n_neighbors=10, random_state=42, verbose=True)</span></span>
<span><span class="co">#&gt; Mon Sep  4 15:13:18 2023 Construct fuzzy simplicial set</span></span>
<span><span class="co">#&gt; Mon Sep  4 15:13:19 2023 Finding Nearest Neighbors</span></span>
<span><span class="co">#&gt; Mon Sep  4 15:13:19 2023 Finished Nearest Neighbor Search</span></span>
<span><span class="co">#&gt; Mon Sep  4 15:13:20 2023 Construct embedding</span></span>
<span><span class="co">#&gt; Mon Sep  4 15:13:21 2023 Finished embedding</span></span>
<span></span>
<span><span class="va">reduced_embeddings</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">2</span>, <span class="op">]</span></span>
<span><span class="co">#&gt;          [,1]     [,2]      [,3]     [,4]     [,5]     [,6]     [,7]     [,8]</span></span>
<span><span class="co">#&gt; [1,] 5.739700 6.123053 10.222593 5.357183 2.770175 6.529313 7.878425 2.544012</span></span>
<span><span class="co">#&gt; [2,] 4.772967 7.098315  8.398382 6.239881 3.431901 7.322098 9.159200 3.022838</span></span>
<span><span class="co">#&gt;          [,9]    [,10]</span></span>
<span><span class="co">#&gt; [1,] 6.631907 6.297930</span></span>
<span><span class="co">#&gt; [2,] 7.621526 6.564954</span></span></code></pre></div>
<p>We’ll take a peek at two of our rows which now represent the reduced
dimension embeddings for each document. Notice that our numbers are
floating points, but also that they are not bounded between -1 and
1.</p>
<p>The next step is to cluster our data. On a first pass, bertopic
considers each discovered cluster a topic. Choice of clustering model
and the selected parameters are therefore important. We’ll use an
hdbscan cluster, as that’s what bertopic was initially built with.</p>
</div>
</div>
<div class="section level3">
<h3 id="clustering">Clustering<a class="anchor" aria-label="anchor" href="#clustering"></a>
</h3>
<p>There is a lot to learn when it comes to clustering, and selecting
the correct parameters is notoriously difficult - especially when
clustering without pre-assigned labels, as <em>most</em> clustering
tends to be. For this run we’ll use the hdbscan clustering algorithm,
because we don’t know how many clusters we should look for in advance
(which we should if using kMeans clustering for exampel).<a href="https://hdbscan.readthedocs.io/en/latest/basic_hdbscan.html" class="external-link">hdbscan
documentation</a></p>
<p>It’s important to know that until you get down to the level of
updating topic representations, in the bertopic pipeline 1 topic = 1
cluster. It’s therefore crucial to gather what information you can about
your data to inform your clustering process.</p>
<div class="section level4">
<h4 id="make-the-clusterer">Make the clusterer<a class="anchor" aria-label="anchor" href="#make-the-clusterer"></a>
</h4>
<p>We’ll stick with a Euclidean distance metric, we’ll reduce the
min_cluster_size to 10, giving us a theoretical maximum of number of
clusters as: length(sentences) / 10 and min_samples equal to 5. The
relationship between min_cluster_size and min_samples is important, it
will default to min_samples = min_cluster_size if not specified, but
this is likely to have adverse effects on your clustering outputs when
dealing with larger datasets (as you’ll likely want to raise the
min_cluster_size parameter significantly). On the other hand, the
hdbscan documentation claims that min_samples, a parameter inherited
from dbscan, does not have such importance in the hdbscan algorithm -
though they also say it remains the algorithm’s biggest weakness.</p>
<p>We’ll also set cluster_selection_method = “leaf”, this means we’ll
tend to find many small clusters, rather than a few large clusters. This
is another parameter which is fraught with danger, to get this right the
first time is unlikely, and is likely to require trial and error, at
least in the beginning.</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">clusterer</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/bt_make_clusterer_hdbscan.html">bt_make_clusterer_hdbscan</a></span><span class="op">(</span>min_cluster_size <span class="op">=</span> <span class="fl">5L</span>,</span>
<span>                          metric <span class="op">=</span> <span class="st">"euclidean"</span>,</span>
<span>                          cluster_selection_method <span class="op">=</span> <span class="st">"leaf"</span>,</span>
<span>                          min_samples <span class="op">=</span> <span class="fl">3L</span>,</span>
<span>                          prediction_data <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level4">
<h4 id="do-the-clustering">Do the clustering<a class="anchor" aria-label="anchor" href="#do-the-clustering"></a>
</h4>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">clusters</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/bt_do_clustering.html">bt_do_clustering</a></span><span class="op">(</span>clustering_model <span class="op">=</span> <span class="va">clusterer</span>, embeddings <span class="op">=</span> <span class="va">reduced_embeddings</span><span class="op">)</span> </span></code></pre></div>
<p>clusters is now a list of cluster labels, we have 720 labels in total
- one for each sentence in {Stringr}’s sentences data set. The cluster
labels are output as integers, but it’s important <strong>not</strong>
to assume that they work like regular integers do. It’s <strong>not
necessarily</strong> the case that cluster 1 is closer to cluster 4 than
it is to cluster 15, the ordering of the labels can effectively be
considered random.</p>
<p>As you most likely have no labels or a training/test/validation data
set, you will have to rely on inspecting your clusters, remembering that
in bertopic 1 cluster = 1 topic. To check whether our clusters make
sense, we could draw upon our data analysis &amp; visualisation tool
kit, and inspect each cluster against every other. This would soon
become intractable. Instead, we’ll take a quick look at the distribution
and then we’ll use the bt_compile_model() and bt_fit_model() functions
to get our topic models out.</p>
</div>
<div class="section level4">
<h4 id="create-a-data-frame">Create a data frame<a class="anchor" aria-label="anchor" href="#create-a-data-frame"></a>
</h4>
<p>But first, we’re beginning to acquire a bunch of objects which may
become hard to maintain. We can store them in a data frame:</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu">dplyr</span><span class="fu">::</span><span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html" class="external-link">tibble</a></span><span class="op">(</span>sentence <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/chartr.html" class="external-link">tolower</a></span><span class="op">(</span><span class="va">sentences</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html" class="external-link">mutate</a></span><span class="op">(</span>embeddings <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">embeddings</span><span class="op">)</span>,</span>
<span>         reduced_embeddings <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">reduced_embeddings</span><span class="op">)</span>,</span>
<span>         cluster <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/integer.html" class="external-link">as.integer</a></span><span class="op">(</span><span class="va">clusters</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>If you want to save this data frame, you’ll need to save it as a
.Rdata/.rds object, not as a .csv or .xlsx as it contains list
columns.</p>
</div>
<div class="section level4">
<h4 id="count-the-clusters">Count the clusters<a class="anchor" aria-label="anchor" href="#count-the-clusters"></a>
</h4>
<p>We can see the distribution via a histogram:</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org" class="external-link">ggplot2</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">data</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html" class="external-link">filter</a></span><span class="op">(</span><span class="va">cluster</span> <span class="op">!=</span> <span class="op">-</span><span class="fl">1</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/count.html" class="external-link">count</a></span><span class="op">(</span><span class="va">cluster</span>, sort <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html" class="external-link">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html" class="external-link">aes</a></span><span class="op">(</span>x<span class="op">=</span> <span class="va">n</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_histogram.html" class="external-link">geom_histogram</a></span><span class="op">(</span>fill <span class="op">=</span> <span class="st">"midnightblue"</span>, bins <span class="op">=</span> <span class="fl">20</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html" class="external-link">theme_minimal</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html" class="external-link">xlab</a></span><span class="op">(</span><span class="st">"Cluster Size"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html" class="external-link">ylab</a></span><span class="op">(</span><span class="st">"Number of Clusters"</span><span class="op">)</span></span></code></pre></div>
<p><img src="modular_approach_files/figure-html/unnamed-chunk-12-1.png" width="700"></p>
<p>With the exception of the outlier group, the clusters are labelled in
order of size:</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">data</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/count.html" class="external-link">count</a></span><span class="op">(</span><span class="va">cluster</span>, sort <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># A tibble: 58 × 2</span></span></span>
<span><span class="co">#&gt;    cluster     n</span></span>
<span><span class="co">#&gt;      <span style="color: #949494; font-style: italic;">&lt;int&gt;</span> <span style="color: #949494; font-style: italic;">&lt;int&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 1</span>      -<span style="color: #BB0000;">1</span>   213</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 2</span>       0    34</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 3</span>       1    19</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 4</span>       2    18</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 5</span>       3    18</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 6</span>       4    17</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 7</span>       5    13</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 8</span>       6    13</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 9</span>       7    13</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">10</span>       8    13</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># ℹ 48 more rows</span></span></span></code></pre></div>
<p>213/720 (29.6%) of data points were labelled as noise (cluster ==
-1), and upon a first inspection they do appear to be quite
eclectic.</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">data</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html" class="external-link">filter</a></span><span class="op">(</span><span class="va">cluster</span> <span class="op">==</span> <span class="op">-</span><span class="fl">1</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/slice.html" class="external-link">slice</a></span><span class="op">(</span><span class="fl">30</span><span class="op">:</span><span class="fl">40</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html" class="external-link">pull</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt;  [1] "the heart beat strongly and with firm strokes."</span></span>
<span><span class="co">#&gt;  [2] "the hat brim was wide and too droopy."         </span></span>
<span><span class="co">#&gt;  [3] "cut the pie into large parts."                 </span></span>
<span><span class="co">#&gt;  [4] "he lay prone and hardly moved a limb."         </span></span>
<span><span class="co">#&gt;  [5] "the fin was sharp and cut the clear water."    </span></span>
<span><span class="co">#&gt;  [6] "oak is strong and also gives shade."           </span></span>
<span><span class="co">#&gt;  [7] "the pipe began to rust while new."             </span></span>
<span><span class="co">#&gt;  [8] "thieves who rob friends deserve jail."         </span></span>
<span><span class="co">#&gt;  [9] "the ripe taste of cheese improves with age."   </span></span>
<span><span class="co">#&gt; [10] "the hog crawled under the high fence."         </span></span>
<span><span class="co">#&gt; [11] "split the log with a quick, sharp blow."</span></span></code></pre></div>
<p>But how do our other clusters look?</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">data</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html" class="external-link">filter</a></span><span class="op">(</span><span class="va">cluster</span> <span class="op">==</span> <span class="fl">0</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/sample_n.html" class="external-link">sample_n</a></span><span class="op">(</span><span class="fl">5</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html" class="external-link">pull</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "a ridge on a smooth surface is a bump or flaw."  </span></span>
<span><span class="co">#&gt; [2] "crack the walnut with your sharp side teeth."    </span></span>
<span><span class="co">#&gt; [3] "a stiff cord will do to fasten your shoe."       </span></span>
<span><span class="co">#&gt; [4] "the stitch will serve but needs to be shortened."</span></span>
<span><span class="co">#&gt; [5] "screw the round cap on as tight as needed."</span></span></code></pre></div>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">data</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html" class="external-link">filter</a></span><span class="op">(</span><span class="va">cluster</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/sample_n.html" class="external-link">sample_n</a></span><span class="op">(</span><span class="fl">5</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html" class="external-link">pull</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "the rope will bind the seven books at once." </span></span>
<span><span class="co">#&gt; [2] "a list of names is carved around the base."  </span></span>
<span><span class="co">#&gt; [3] "he wrote down a long list of items."         </span></span>
<span><span class="co">#&gt; [4] "draw the chart with heavy black lines."      </span></span>
<span><span class="co">#&gt; [5] "flood the mails with requests for this book."</span></span></code></pre></div>
<p>The more clusters we look at, the more difficult it will become to
figure out what’s happening…</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">data</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html" class="external-link">filter</a></span><span class="op">(</span><span class="va">cluster</span> <span class="op">==</span> <span class="fl">2</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/sample_n.html" class="external-link">sample_n</a></span><span class="op">(</span><span class="fl">5</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html" class="external-link">pull</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "better hash is made of rare beef."       </span></span>
<span><span class="co">#&gt; [2] "a pot of tea helps to pass the evening." </span></span>
<span><span class="co">#&gt; [3] "tea in thin china has a sweet taste."    </span></span>
<span><span class="co">#&gt; [4] "these days a chicken leg is a rare dish."</span></span>
<span><span class="co">#&gt; [5] "a cup of sugar makes sweet fudge."</span></span></code></pre></div>
<p>Inspecting each of these clusters and trying to figure out what each
cluster means and how they inter-relate would soon become intractable
for humans. Thankfully, within BERTopic there are quantitative methods
already in place to aid this procedure.</p>
<p>Instead of looking at the individual posts in each cluster, we’ll
attempt to summarise their contents with keywords and phrases. In order
to this, we’ll make a vectoriser and a ctfidf model. After creating
these models, we can use everything we’ve looked at so far to compile a
model, fit the model on our data, and finally explore our topics and
their representations.</p>
</div>
<div class="section level4">
<h4 id="make-the-vectoriser">Make the vectoriser<a class="anchor" aria-label="anchor" href="#make-the-vectoriser"></a>
</h4>
<p>For the vectoriser we’ll set the ngram range as c(1, 2) this means
our topics can be represented as single words or bigrams. We’ll set
stop_words to ‘english’ so that English stop words are removed and we’ll
tell our vectoriser to only consider words that have a frequency of 3 or
higher, so that rare words and chance occurrences don’t clog our
representations too much. In practice, we will want to set a higher
value for min_frequency as we’ll be working with significantly more
data.</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">vectoriser</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/bt_make_vectoriser.html">bt_make_vectoriser</a></span><span class="op">(</span>ngram_range <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">)</span>, stop_words <span class="op">=</span> <span class="st">"english"</span>, min_frequency <span class="op">=</span> <span class="fl">3L</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level4">
<h4 id="make-the-ctfidf-model">Make the ctfidf model<a class="anchor" aria-label="anchor" href="#make-the-ctfidf-model"></a>
</h4>
<p>Then we’ll create a ctfidf model which will allow us to represent
each topic according to the words that are important to that topic (have
high frequency) and distinct to that topic (have relatively low
frequency in other topics):</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">ctfidf</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/bt_make_ctfidf.html">bt_make_ctfidf</a></span><span class="op">(</span>reduce_frequent_words <span class="op">=</span> <span class="cn">TRUE</span>, bm25_weighting <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div>
</div>
</div>
<div class="section level3">
<h3 id="compile-the-model">Compile the model<a class="anchor" aria-label="anchor" href="#compile-the-model"></a>
</h3>
<p>We’ve already made our individual components, or modules, and
selected their parameters. We’ve already performed the embeddings and
dimensionality reduction, so bertopic allows us to skip these steps
easily by feeding in empty models to the <code>bt_compile_model</code>
function for embedding and reducing. We can also skip clustering, but
won’t for this task as it adds extra complexity, the clustering we
performed above with bt_do_clustering was just to explore how our
clusterer would work in practice.</p>
<p>N.B. In practice you will need to pause and explore your parameters
in more depth.</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">topic_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/bt_compile_model.html">bt_compile_model</a></span><span class="op">(</span></span>
<span>  embedding_model <span class="op">=</span> <span class="fu"><a href="../reference/bt_base_embedder.html">bt_base_embedder</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>  reduction_model <span class="op">=</span> <span class="fu"><a href="../reference/bt_base_reducer.html">bt_base_reducer</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>  clustering_model <span class="op">=</span> <span class="va">clusterer</span>,</span>
<span>  vectoriser_model <span class="op">=</span> <span class="va">vectoriser</span>,</span>
<span>  ctfidf_model <span class="op">=</span> <span class="va">ctfidf</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model built</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="fit-the-model">Fit the model<a class="anchor" aria-label="anchor" href="#fit-the-model"></a>
</h3>
<p>We feed in our reduced embeddings rather than the original
embeddings, this allows us to skip steps in the workflow; this can save
us a lot of time, particularly when we have many documents.</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/bt_fit_model.html">bt_fit_model</a></span><span class="op">(</span><span class="va">topic_model</span>, <span class="va">data</span><span class="op">$</span><span class="va">sentence</span>, embeddings <span class="op">=</span> <span class="va">reduced_embeddings</span><span class="op">)</span></span></code></pre></div>
<p>We can create a look up table to join our sentences to their topic
labels and their topic descriptions and join this information with our
original dataframe:</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">topic_representations</span> <span class="op">&lt;-</span> <span class="va">topic_model</span><span class="op">$</span><span class="fu">get_topic_info</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">topic_rep_lookup</span> <span class="op">&lt;-</span> <span class="va">topic_representations</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html" class="external-link">select</a></span><span class="op">(</span>topic <span class="op">=</span> <span class="va">Topic</span>, description <span class="op">=</span> <span class="va">Name</span>, topic_size <span class="op">=</span> <span class="va">Count</span><span class="op">)</span></span>
<span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="va">data</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html" class="external-link">mutate</a></span><span class="op">(</span>topic <span class="op">=</span> <span class="va">topic_model</span><span class="op">$</span><span class="va">topics_</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate-joins.html" class="external-link">left_join</a></span><span class="op">(</span><span class="va">topic_rep_lookup</span><span class="op">)</span></span>
<span><span class="co">#&gt; Joining with `by = join_by(topic)`</span></span>
<span></span>
<span><span class="op">(</span><span class="va">data</span> <span class="op">&lt;-</span> <span class="va">data</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/relocate.html" class="external-link">relocate</a></span><span class="op">(</span><span class="va">sentence</span>, <span class="va">topic</span>, <span class="va">topic_size</span>, <span class="va">description</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># A tibble: 720 × 7</span></span></span>
<span><span class="co">#&gt;    sentence   topic topic_size description embeddings reduced_embeddings cluster</span></span>
<span><span class="co">#&gt;    <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>      <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span>      <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>       <span style="color: #949494; font-style: italic;">&lt;list&gt;</span>     <span style="color: #949494; font-style: italic;">&lt;list&gt;</span>               <span style="color: #949494; font-style: italic;">&lt;int&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 1</span> the birch…    -<span style="color: #BB0000;">1</span>        213 -1_night_l… <span style="color: #949494;">&lt;dbl[…]&gt;</span>   <span style="color: #949494;">&lt;dbl [720 × 10]&gt;</span>        -<span style="color: #BB0000;">1</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 2</span> glue the …    -<span style="color: #BB0000;">1</span>        213 -1_night_l… <span style="color: #949494;">&lt;dbl[…]&gt;</span>   <span style="color: #949494;">&lt;dbl [720 × 10]&gt;</span>        -<span style="color: #BB0000;">1</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 3</span> it's easy…    43          6 43_pipe_te… <span style="color: #949494;">&lt;dbl[…]&gt;</span>   <span style="color: #949494;">&lt;dbl [720 × 10]&gt;</span>        39</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 4</span> these day…     2         18 2_sweet_ra… <span style="color: #949494;">&lt;dbl[…]&gt;</span>   <span style="color: #949494;">&lt;dbl [720 × 10]&gt;</span>         2</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 5</span> rice is o…     2         18 2_sweet_ra… <span style="color: #949494;">&lt;dbl[…]&gt;</span>   <span style="color: #949494;">&lt;dbl [720 × 10]&gt;</span>         2</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 6</span> the juice…    14         10 14_tree_se… <span style="color: #949494;">&lt;dbl[…]&gt;</span>   <span style="color: #949494;">&lt;dbl [720 × 10]&gt;</span>        14</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 7</span> the box w…    45          5 45_box_squ… <span style="color: #949494;">&lt;dbl[…]&gt;</span>   <span style="color: #949494;">&lt;dbl [720 × 10]&gt;</span>        50</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 8</span> the hogs …    -<span style="color: #BB0000;">1</span>        213 -1_night_l… <span style="color: #949494;">&lt;dbl[…]&gt;</span>   <span style="color: #949494;">&lt;dbl [720 × 10]&gt;</span>        -<span style="color: #BB0000;">1</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 9</span> four hour…    -<span style="color: #BB0000;">1</span>        213 -1_night_l… <span style="color: #949494;">&lt;dbl[…]&gt;</span>   <span style="color: #949494;">&lt;dbl [720 × 10]&gt;</span>        -<span style="color: #BB0000;">1</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">10</span> a large s…    -<span style="color: #BB0000;">1</span>        213 -1_night_l… <span style="color: #949494;">&lt;dbl[…]&gt;</span>   <span style="color: #949494;">&lt;dbl [720 × 10]&gt;</span>        -<span style="color: #BB0000;">1</span></span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># ℹ 710 more rows</span></span></span></code></pre></div>
<p>The df compiled is a common place for everything we have generated so
far, in practice we don’t really need the cluster column now that we
have the topic column, they provide the same information. Topics and
clusters should have largely the same labels, the only discrepancy would
be where multiple clusters/topics are the same size.</p>
</div>
</div>
<div class="section level2">
<h2 id="changing-the-model-representation">Changing the Model Representation<a class="anchor" aria-label="anchor" href="#changing-the-model-representation"></a>
</h2>
<p>Once you are happy with the topics/clusters that have been formed,
there are a few methods we can use to improve the topic representations
and get a better understanding of what each topic is about.</p>
<p>The representation methods currently available are:</p>
<ol style="list-style-type: decimal">
<li><p><strong>KeyBERT</strong> is a keyword extraction technique that
uses BERT embeddings to represent our topics with appropriate keywords
and phrases.</p></li>
<li><p><strong>MaximalMarginalRelevance</strong> is a concept used to
select the most relevant keywords or phrases while promoting diversity
in keywords. It balances relevance to the topic with distinctiveness
from previously chosen keywords or phrases using a trade-off parameter
called lambda.</p></li>
<li><p><strong>OpenAI</strong> allows us to use their available models
to generate topic summaries. An OpenAI API key is required to access
their api and models.</p></li>
<li><p><strong>HuggingFace</strong> allows us to use their available
models to generate topic summaries. Unlike with OpenAI, you will not
need an API key and this is completely free. However, the models are not
as sophisticated as some of OpenAI’s.</p></li>
</ol>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">representation_keybert</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/bt_representation_keybert.html">bt_representation_keybert</a></span><span class="op">(</span>fitted_model <span class="op">=</span> <span class="va">topic_model</span>,</span>
<span>                                                    documents <span class="op">=</span> <span class="va">sentences</span>,</span>
<span>                                                    document_embeddings <span class="op">=</span> <span class="va">embeddings</span>,</span>
<span>                                                    embedding_model <span class="op">=</span> <span class="va">embedder</span>,</span>
<span>                                                    top_n_words <span class="op">=</span> <span class="fl">10</span>,</span>
<span>                                                    nr_repr_docs <span class="op">=</span> <span class="fl">50</span>,</span>
<span>                                                    nr_samples <span class="op">=</span> <span class="fl">500</span>,</span>
<span>                                                    nr_candidate_words <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span>
<span></span>
<span><span class="va">representation_mmr</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/bt_representation_mmr.html">bt_representation_mmr</a></span><span class="op">(</span>fitted_model <span class="op">=</span> <span class="va">topic_model</span>,</span>
<span>                                            embedding_model <span class="op">=</span> <span class="va">embedder</span>,</span>
<span>                                            diversity <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="va">representation_openai</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/bt_representation_openai.html">bt_representation_openai</a></span><span class="op">(</span>fitted_model <span class="op">=</span> <span class="va">topic_model</span>,</span>
<span>                                                  documents <span class="op">=</span> <span class="va">sentences</span>,</span>
<span>                                                  openai_model <span class="op">=</span> <span class="st">"gpt-3.5-turbo"</span>,</span>
<span>                                                  nr_repr_docs <span class="op">=</span> <span class="fl">10</span>,</span>
<span>                                                  chat <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                                                  api_key <span class="op">=</span> <span class="st">"sk-"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">representation_hf</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/bt_representation_hf.html">bt_representation_hf</a></span><span class="op">(</span>fitted_model <span class="op">=</span> <span class="va">topic_model</span>,</span>
<span>                                          documents <span class="op">=</span> <span class="va">sentences</span>,</span>
<span>                                          task <span class="op">=</span> <span class="st">"text2text-generation"</span>,</span>
<span>                                          hf_model <span class="op">=</span> <span class="st">"google/flan-t5-base"</span>,</span>
<span>                                          default_prompt <span class="op">=</span> <span class="st">"keywords"</span><span class="op">)</span></span></code></pre></div>
<p>Now that we have trialed a few representation methods, we can look at
how they compare to default representations and we should be able to get
a good idea of what each topic is about. You will notice that the
gpt-3.5 model gives the most coherent topic representation and it would
be easy to just take that as gospel and chose a topic title based on
that. It is important to remember, like with the other representation
methods, only the number you input for nr_repr_docs in
bt_representation_openai has been sent to the model and for a large
topic, these documents may not represent the topic as a whole.</p>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">topic_representations</span> <span class="op">&lt;-</span> <span class="va">topic_model</span><span class="op">$</span><span class="fu">get_topic_info</span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html" class="external-link">mutate</a></span><span class="op">(</span>keybert <span class="op">=</span> <span class="va">representation_keybert</span>,</span>
<span>         mmr <span class="op">=</span> <span class="va">representation_mmr</span>,</span>
<span>         <span class="co"># openai = representation_openai,</span></span>
<span>         flanT5 <span class="op">=</span> <span class="va">representation_hf</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html" class="external-link">select</a></span><span class="op">(</span><span class="op">-</span><span class="va">Representative_Docs</span><span class="op">)</span></span>
<span></span>
<span><span class="va">topic_representations</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span> <span class="fu">DT</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/DT/man/datatable.html" class="external-link">datatable</a></span><span class="op">(</span>options <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>scrollX <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<div class="datatables html-widget html-fill-item-overflow-hidden html-fill-item" id="htmlwidget-0b7a42acc1d94e940f06" style="width:100%;height:auto;"></div>
<script type="application/json" data-for="htmlwidget-0b7a42acc1d94e940f06">{"x":{"filter":"none","vertical":false,"data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58"],[-1,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56],[213,34,19,18,18,17,13,13,13,13,12,11,11,11,10,10,10,10,9,9,9,9,8,8,8,8,8,8,8,8,8,7,7,7,7,7,7,6,6,6,6,6,6,6,6,6,5,5,5,5,5,5,5,5,5,5,5,5],["-1_night_large_girl_broke","0_left_straight_wooden_space","1_write_black_long_lines","2_sweet_rare_covered_far","3_friends_need_turn_tales","4_saw_dog_quick_tell","5_boy_child_sent_poor","6_tender_odor_need_food","7_sound_leaves_grass_wet","8_gold_carved_torn_quite","9_win_lead_heavy_crack","10_desk_sat_office_paint","11_week_tried_quite_needed","12_wind_tree_pink_near","13_screen_straw_let_corner","14_tree_sent_knife_ice","15_right_tried_drop_cause","16_north_loud_waste_bring","17_space_office_neat_bad","18_carved_dried_torn_heavy","19_home_week_quite_long","20_burned_went_waste_tree","21_seen_pink_drifts_stood","22_words_tried_times_office","23_ice_steady_bowl_water","24_stain_spot_wet_dried","25_add_drop_comes_fast","26_pink_north_small_used","27_tender_odor_dirt_cloth","28_screen_sat_room_went","29_tight_straw_covered_tell","30_times_lines_drifts_dirt","31_win_poor_china_bank","32_ship_lost_port_torn","33_sheet_hole_square_spot","34_tales_neat_men_tell","35_brought_road_moved_day","36_port_taste_hold_water","37_low_end_beat_just","38_bowl_hot_came_light","39_loud_china_street_bank","40_act_fast_time_early","41_dust_near_let_lawn","42_door_sent_food_needed","43_pipe_tell_ran_cut","44_sat_fast_rose_like","45_box_square_seen_wooden","46_words_sweet_better_lost","47_times_tales_dog_boy","48_paint_lines_covered_edge","49_come_time_new_","50_tin_little_rare_needs","51_steady_went_lawn_dark","52_room_start_lost_store","53_wood_saw_dry_used","54_cup_contents_horse_tall","55_comes_base_great_act","56_stain_odor_hands_burned"],[["night","large","girl","broke","days","rose","coat","deep","clear","form"],["left","straight","wooden","space","tin","quick","port","near","knife","crack"],["write","black","long","lines","lead","base","heavy","carved","cloth","sheet"],["sweet","rare","covered","far","cup","comes","china","bowl","food","drink"],["friends","need","turn","tales","cause","like","write","make","great","bad"],["saw","dog","quick","tell","street","right","house","hole","black","small"],["boy","child","sent","poor","hands","far","children","little","young","came"],["tender","odor","need","food","drink","needed","young","came","horse","road"],["sound","leaves","grass","wet","turn","dirt","brown","dry","wind","clear"],["gold","carved","torn","quite","knife","hold","worn","coat","white","hung"],["win","lead","heavy","crack","cause","strong","stood","sound","time","needs"],["desk","sat","office","paint","blue","light","hung","floor","wide","man"],["week","tried","quite","needed","great","come","bank","worn","man","dull"],["wind","tree","pink","near","brought","street","edge","way","hung","blue"],["screen","straw","let","corner","lack","hot","high","old","",""],["tree","sent","knife","ice","lack","cut","kept","makes","used","fine"],["right","tried","drop","cause","struck","bad","lost","just","don","hard"],["north","loud","waste","bring","read","low","little","time","lack","dog"],["space","office","neat","bad","add","takes","leaves","store","paper","large"],["carved","dried","torn","heavy","stood","wood","moved","hold","house","floor"],["home","week","quite","long","start","friends","ran","words","broke","took"],["burned","went","waste","tree","pipe","crack","contents","dark","house","big"],["seen","pink","drifts","stood","low","edge","rose","house","dry","took"],["words","tried","times","office","end","needs","takes","strong","",""],["ice","steady","bowl","water","tight","days","came","early","make","green"],["stain","spot","wet","dried","read","dry","cut","paper","sharp","blue"],["add","drop","comes","fast","set","old","","","",""],["pink","north","small","used","blue","round","","","",""],["tender","odor","dirt","cloth","water","dry","deep","brass","sharp","fine"],["screen","sat","room","went","children","sound","home","straight","took","store"],["tight","straw","covered","tell","door","kept","","","",""],["times","lines","drifts","dirt","road","street","dust","deep","men","sharp"],["win","poor","china","bank","early","don","bright","way","good",""],["ship","lost","port","torn","big","sharp","hard","","",""],["sheet","hole","square","spot","paint","edge","dull","round","tight","set"],["tales","neat","men","tell","serve","little","set","work","good",""],["brought","road","moved","day","big","wide","night","","",""],["port","taste","hold","water","strong","","","","",""],["low","end","beat","just","kept","best","good","","",""],["bowl","hot","came","light","red","wide","road","","",""],["loud","china","street","bank","black","big","set","floor","red",""],["act","fast","time","early","men","","","","",""],["dust","near","let","lawn","drop","beat","worn","wall","old",""],["door","sent","food","needed","bad","used","red","old","",""],["pipe","tell","ran","cut","clear","small","","","",""],["sat","fast","rose","like","","","","","",""],["box","square","seen","wooden","paper","bright","red","high","",""],["words","sweet","better","lost","work","make","clear","strong","",""],["times","tales","dog","boy","young","","","","",""],["paint","lines","covered","edge","corner","ran","coat","black","left","fine"],["come","time","new","","","","","","",""],["tin","little","rare","needs","work","make","box","gold","used",""],["steady","went","lawn","dark","bright","like","brass","small","new","green"],["room","start","lost","store","","","","","",""],["wood","saw","dry","used","make","brass","kept","best","",""],["cup","contents","horse","tall","broke","","","","",""],["comes","base","great","act","day","new","","","",""],["stain","odor","hands","burned","cloth","bring","better","takes","green",""]],[["outliers"],["wood_knife_stood_wooden_desk_round_sharp_tin_gold_cloth"],["carved_write_fast_paper_lines_form_good_make_black_sheet"],["brown_food_bowl_taste_round_drink_covered_cup_hot_old"],["dull_lack_cause_don_make_tall_small_write_old_like"],["hole_house_saw_street_takes_quick_black_dog_hard_tell"],["poor_little_children_child_ran_small_way_hands_came_young"],["odor_grass_drink_horse_way_tender_food_road_man_makes"],["dirt_grass_wet_turn_dry_leaves_wind_brown_clear_green"],["carved_knife_hung_gold_brass_hold_quite_bright_torn_way"],["stood_win_strong_sound_set_crack_time_lead_heavy_just"],["desk_floor_dull_wall_paint_hung_light_strong_blue_red"],["worn_dull_week_come_good_quite_old_bank_tried_man"],["round_hung_wind_tree_street_blue_near_pink_edge_brought"],["straw_corner_lack_let_high_hot_screen_old"],["ice_knife_kept_tree_used_cut_makes_lack_fine_sent"],["tried_drop_lost_right_cause_struck_just_bad_hard_new"],["lack_road_time_waste_little_night_loud_bring_best_dog"],["neat_space_large_bad_leaves_wall_store_takes_paper_office"],["wood_round_wall_dried_carved_floor_torn_house_hold_stood"],["kept_words_quite_ran_dull_broke_took_friends_home_long"],["waste_pipe_leaves_tree_contents_burned_crack_house_dark_went"],["drifts_stood_edge_old_dry_water_house_rose_took_seen"],["takes_strong_office_end_times_needs_words_tried"],["green_water_tight_bowl_steady_ice_make_early_came_days"],["dry_dried_wet_paper_sharp_stain_cut_used_blue_read"],["old_comes_drop_fast_set_add"],["round_pink_north_small_blue_used"],["sharp_brass_water_tender_cloth_dry_odor_deep_dirt_fine"],["home_room_children_store_straight_high_sound_screen_left_took"],["door_straw_kept_tell_tight_covered"],["dirt_road_drifts_sharp_lines_dust_street_deep_men_times"],["bank_bright_don_way_win_early_poor_china_good"],["sharp_lost_torn_port_hard_big_ship"],["hole_wall_paint_sheet_square_tight_round_green_edge_dull"],["little_tales_neat_work_tell_good_serve_men_set"],["road_night_moved_wide_day_brought_big"],["water_strong_hold_taste_port"],["end_kept_beat_good_low_best_just"],["came_road_bowl_red_wide_hot_light"],["loud_black_bank_floor_street_red_china_set_big"],["fast_time_early_act_men"],["wall_dust_near_let_drop_beat_lawn_old_worn"],["food_door_red_old_needed_bad_used_sent"],["pipe_cut_clear_small_tell_ran"],["rose_like_fast_sat"],["wooden_bright_paper_seen_red_box_square_high"],["clear_work_make_strong_lost_better_sweet_words"],["dog_young_tales_times_boy"],["covered_lines_fine_paint_coat_ran_black_corner_edge_left"],["come_time_new"],["tin_work_gold_make_rare_used_little_needs_box"],["bright_dark_new_steady_like_small_red_went_green_brass"],["lost_room_store_start"],["saw_dry_brass_make_used_kept_wood_best"],["horse_contents_broke_tall_cup"],["comes_act_base_day_new_great"],["stain_hands_burned_bring_green_cloth_takes_odor_better"]],[["outliers"],["knife_tin_port_wooden_crack_space_near_straight_quick_left"],["lines_cloth_sheet_heavy_lead_black_carved_base_long_write"],["cup_china_bowl_drink_food_covered_rare_far_comes_sweet"],["tales_write_turn_make_bad_need_like_great_cause_friends"],["dog_black_house_street_hole_small_quick_right_tell_saw"],["child_hands_children_poor_came_young_far_sent_little_boy"],["horse_odor_food_road_need_drink_needed_came_young_tender"],["grass_dry_dirt_wet_wind_leaves_clear_brown_turn_sound"],["knife_coat_carved_worn_torn_quite_hung_hold_white_gold"],["lead_crack_sound_stood_strong_heavy_needs_time_cause_win"],["desk_blue_paint_man_light_hung_wide_sat_floor_office"],["worn_dull_tried_needed_bank_quite_come_man_great_week"],["pink_blue_tree_street_brought_edge_near_hung_way_wind"],["straw_hot_corner_old_high_let___lack_screen"],["knife_ice_cut_sent_used_kept_makes_fine_lack_tree"],["drop_struck_lost_tried_don_hard_bad_just_cause_right"],["north_dog_loud_waste_read_lack_low_time_little_bring"],["paper_neat_office_leaves_large_add_bad_takes_store_space"],["wood_house_floor_moved_heavy_hold_stood_torn_dried_carved"],["broke_week_words_ran_friends_long_took_start_quite_home"],["burned_house_dark_pipe_crack_tree_contents_big_waste_went"],["rose_drifts_house_pink_stood_dry_edge_low_took_seen"],["office_tried_needs_end_strong_takes_times___words"],["bowl_green_make_early_days_tight_steady_came_water_ice"],["stain_paper_read_dry_dried_blue_sharp_cut_wet_spot"],["drop_fast_old_set_____comes_add"],["pink_round_north_used_small_____blue"],["odor_dirt_brass_cloth_water_dry_deep_sharp_fine_tender"],["screen_children_home_went_store_room_sat_took_sound_straight"],["door_straw_covered_kept_____tell_tight"],["lines_road_street_drifts_deep_dirt_sharp_dust_men_times"],["win_bright_china_bank_early_poor__way_don_good"],["port_sharp_torn_hard_lost_big____ship"],["paint_dull_hole_round_tight_spot_square_edge_set_sheet"],["tales_serve_set_little_work_neat_men_good__tell"],["road_day_wide_night_big____moved_brought"],["port_taste_water_strong_hold_____"],["beat_best_end_kept_good____just_low"],["bowl_road_hot_red_came_light____wide"],["loud_bank_red_set_floor_china_black_street__big"],["act_fast_early_men_time_____"],["dust_lawn_drop_wall_let_worn_beat_old_near_"],["door_used_food_sent_red_bad_needed_old__"],["pipe_cut_small_ran_clear_tell____"],["rose_like_fast_______sat"],["box_square_wooden_paper_red_seen_bright_high__"],["words_lost_work_clear_strong_better_sweet___make"],["tales_dog_young_boy______times"],["paint_left_corner_lines_covered_edge_ran_black_fine_coat"],["new_time________come"],["tin_rare_box_used_gold_needs_work_make_little_"],["lawn_bright_green_dark_new_brass_went_like_small_steady"],["room_store_lost_start______"],["brass_dry_saw_kept_make_used_best___wood"],["horse_broke_tall_contents______cup"],["base_new_day_act_great_____comes"],["stain_odor_hands_green_burned_better_takes_cloth_bring_"]],[["outliers"],["a knife with a tin port"],["carved base of a sheet of cloth"],["china tea bowl with a cup of tea"],["writing a tale of friendship"],["black dog tells a house in the street a hole"],["a young boy came to the aid of a poor child and sent him to the far"],["horse came on the road"],["a swan turns green and clears its eyes as the wind turns the grass and"],["carved wooden coat hanged on a white wall"],["i need to win the game"],["a man sitting on a desk in an office with blue lights and blue walls"],["i want to come back to the bank and try again."],["a tree hung by a wind near the edge of a street in pink and"],["old fashioned tv with a low screen and a low corner"],["ice knife"],["don't drop a ball just because you tried hard"],["little dog bring little noise in the north"],["add a neat and tidy feel to your office with this large paper bag"],["carved wooden floor in a house"],["i ran into my friends and broke up with them on the first day of school"],["a house burned down by a tree"],["a house with pink and dry roses standing on the edge of a dry edge"],["i need strong words to end my office"],["bowl of greens"],["a sandpaper spot with a sanding paper"],["add drop to your set of old tv"],["small round pink round used in the north"],["a tad more than a tad more than a "],["a child takes a look at the screen at home as he sits on the"],["straw covered door"],["road with sharp lines and drifts of dirt and dust"],["chinese bank win good way to win"],["a ship torn off the coast of a port"],["a square of white paint with a white spot on the edge"],["little men tell tales"],["a moving road"],["port"],["beats"],["red bowl coming out of the road"],["a set of red carpeted floors in a bank in china"],["a man acted fast in the early morning"],["old wall near old lawn"],["red door used for food"],["pipe running through a small clear pipe"],["i like roses sat fast ."],["red and white squares on a wooden box"],["i lost my job"],["young boy with a dog"],["black and white striped coat of paint on the edge of a corner"],["new year 's resolution"],["box of gold"],["small green brass wrought iron fence with small brass lights on the lawn"],["lost and found in a room at a store"],["woodworking"],["broken cup with contents"],["a new day comes and goes"],["bringing green to your hands with a cloth"]]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>Topic<\/th>\n      <th>Count<\/th>\n      <th>Name<\/th>\n      <th>Representation<\/th>\n      <th>keybert<\/th>\n      <th>mmr<\/th>\n      <th>flanT5<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"scrollX":true,"columnDefs":[{"className":"dt-right","targets":[1,2]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
</div>
<div class="section level2">
<h2 id="modifying-topics">Modifying Topics<a class="anchor" aria-label="anchor" href="#modifying-topics"></a>
</h2>
<div class="section level3">
<h3 id="merging-topics">Merging Topics<a class="anchor" aria-label="anchor" href="#merging-topics"></a>
</h3>
<p>Particularly when using hdbscan we can end up with a large number of
topics and it can be useful to merge some of these topics which we think
are suitably similar. We can get a certain idea about this from the
topic descriptions that we have already generated, but it can also be
useful to look at the data more closely before merging.</p>
<div class="section level4">
<h4 id="hierarchical-clustering">Hierarchical Clustering<a class="anchor" aria-label="anchor" href="#hierarchical-clustering"></a>
</h4>
<p>Hdbscan clustering forms clusters through a hierarchical processes
which you can visualise with a dendrogram. This can be useful when
merging topics as you can see how clusters split to become the topics
that emerged from our topic modelling process. The x-axis here is a
measure of the distance between topic embeddings, so when clusters split
at a higher x-value there is a larger distance between their embeddings.
We can see that for this particular dataset, the clusters split into
their final topics quite early on in the hierarchy and so it might not
be appropriate to merge topics based on how they have emerged in the
hierarchy.</p>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">hierarchical_topics</span> <span class="op">&lt;-</span> <span class="va">topic_model</span><span class="op">$</span><span class="fu">hierarchical_topics</span><span class="op">(</span><span class="va">sentences</span><span class="op">)</span></span>
<span><span class="va">topic_model</span><span class="op">$</span><span class="fu">visualize_hierarchy</span><span class="op">(</span>hierarchical_topics <span class="op">=</span> <span class="va">hierarchical_topics</span><span class="op">)</span><span class="op">$</span><span class="fu">show</span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<pre><code>#&gt; 
  0%|          | 0/56 [00:00&lt;?, ?it/s]
100%|##########| 56/56 [00:00&lt;00:00, 554.60it/s]
100%|##########| 56/56 [00:00&lt;00:00, 554.05it/s]</code></pre>
<div>                        <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script charset="utf-8" src="https://cdn.plot.ly/plotly-2.24.1.min.js"></script><div id="cdd87b82-753b-4105-9a62-299a2fb2f24e" class="plotly-graph-div" style="height:1055px; width:1000px;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("cdd87b82-753b-4105-9a62-299a2fb2f24e")) {                    Plotly.newPlot(                        "cdd87b82-753b-4105-9a62-299a2fb2f24e",                        [{"hoverinfo":"text","marker":{"color":"rgb(61,153,112)"},"mode":"lines","text":["comes_base_great_act_day","","","act_fast_time_early_men"],"x":[0.0,0.7367104242928515,0.7367104242928515,0.0],"xaxis":"x","y":[-15.0,-15.0,-25.0,-25.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(61,153,112)"},"mode":"lines","text":["write_black_long_lines_lead","","","act_comes_base_great_fast"],"x":[0.0,0.8379933285857839,0.8379933285857839,0.7367104242928515],"xaxis":"x","y":[-5.0,-5.0,-20.0,-20.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(255,65,54)"},"mode":"lines","text":["come_time_new__","","","win_lead_heavy_crack_cause"],"x":[0.0,0.8741029258598041,0.8741029258598041,0.0],"xaxis":"x","y":[-35.0,-35.0,-45.0,-45.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(0,116,217)"},"mode":"lines","text":["act_base_write_fast_great","","","come_time_win_lead_heavy"],"x":[0.8379933285857839,1.020197890415084,1.020197890415084,0.8741029258598041],"xaxis":"x","y":[-12.5,-12.5,-40.0,-40.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(35,205,205)"},"mode":"lines","text":["home_week_quite_long_start","","","week_tried_quite_needed_great"],"x":[0.0,0.7322455875696929,0.7322455875696929,0.0],"xaxis":"x","y":[-55.0,-55.0,-65.0,-65.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(133,20,75)"},"mode":"lines","text":["words_tried_times_office_end","","","words_sweet_better_lost_work"],"x":[0.0,0.7357174663588586,0.7357174663588586,0.0],"xaxis":"x","y":[-75.0,-75.0,-85.0,-85.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(0,116,217)"},"mode":"lines","text":["week_quite_home_tried_long","","","words_tried_times_office_sweet"],"x":[0.7322455875696929,1.054464093313805,1.054464093313805,0.7357174663588586],"xaxis":"x","y":[-60.0,-60.0,-80.0,-80.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(255,220,0)"},"mode":"lines","text":["friends_need_turn_tales_cause","","","right_tried_drop_cause_struck"],"x":[0.0,0.7798933378889688,0.7798933378889688,0.0],"xaxis":"x","y":[-105.0,-105.0,-115.0,-115.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(255,220,0)"},"mode":"lines","text":["space_office_neat_bad_add","","","cause_bad_friends_right_turn"],"x":[0.0,0.9214618901630149,0.9214618901630149,0.7798933378889688],"xaxis":"x","y":[-95.0,-95.0,-110.0,-110.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(255,220,0)"},"mode":"lines","text":["add_drop_comes_fast_set","","","dust_near_let_lawn_drop"],"x":[0.0,0.8058335824605488,0.8058335824605488,0.0],"xaxis":"x","y":[-125.0,-125.0,-135.0,-135.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(255,220,0)"},"mode":"lines","text":["bad_cause_friends_right_tried","","","drop_add_dust_near_let"],"x":[0.9214618901630149,0.994120400108275,0.994120400108275,0.8058335824605488],"xaxis":"x","y":[-102.5,-102.5,-130.0,-130.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(0,116,217)"},"mode":"lines","text":["words_week_tried_quite_home","","","drop_add_bad_cause_dust"],"x":[1.054464093313805,1.1752908666369162,1.1752908666369162,0.994120400108275],"xaxis":"x","y":[-70.0,-70.0,-116.25,-116.25],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(40,35,35)"},"mode":"lines","text":["desk_sat_office_paint_blue","","","sat_fast_rose_like_"],"x":[0.0,0.801817988379282,0.801817988379282,0.0],"xaxis":"x","y":[-145.0,-145.0,-155.0,-155.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(61,153,112)"},"mode":"lines","text":["screen_sat_room_went_children","","","room_start_lost_store_"],"x":[0.0,0.7120096433377865,0.7120096433377865,0.0],"xaxis":"x","y":[-165.0,-165.0,-175.0,-175.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(0,116,217)"},"mode":"lines","text":["sat_desk_office_paint_fast","","","room_screen_sat_went_children"],"x":[0.801817988379282,1.0553883002013125,1.0553883002013125,0.7120096433377865],"xaxis":"x","y":[-150.0,-150.0,-170.0,-170.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(0,116,217)"},"mode":"lines","text":["words_tried_drop_add_friends","","","sat_room_desk_office_went"],"x":[1.1752908666369162,1.2031446877144025,1.2031446877144025,1.0553883002013125],"xaxis":"x","y":[-93.125,-93.125,-160.0,-160.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(0,116,217)"},"mode":"lines","text":["act_lead_heavy_base_time","","","words_tried_office_drop_sat"],"x":[1.020197890415084,1.260716979452376,1.260716979452376,1.2031446877144025],"xaxis":"x","y":[-26.25,-26.25,-126.5625,-126.5625],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(255,65,54)"},"mode":"lines","text":["gold_carved_torn_quite_knife","","","carved_dried_torn_heavy_stood"],"x":[0.0,0.7278241279915325,0.7278241279915325,0.0],"xaxis":"x","y":[-185.0,-185.0,-195.0,-195.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(255,65,54)"},"mode":"lines","text":["ship_lost_port_torn_big","","","port_taste_hold_water_strong"],"x":[0.0,0.776282375681635,0.776282375681635,0.0],"xaxis":"x","y":[-205.0,-205.0,-215.0,-215.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(255,65,54)"},"mode":"lines","text":["torn_carved_hold_gold_dried","","","port_ship_lost_torn_taste"],"x":[0.7278241279915325,0.9569327161694986,0.9569327161694986,0.776282375681635],"xaxis":"x","y":[-190.0,-190.0,-210.0,-210.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(35,205,205)"},"mode":"lines","text":["tin_little_rare_needs_work","","","left_straight_wooden_space_tin"],"x":[0.0,0.7469940112792506,0.7469940112792506,0.0],"xaxis":"x","y":[-225.0,-225.0,-235.0,-235.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(35,205,205)"},"mode":"lines","text":["tin_left_box_straight_needs","","","box_square_seen_wooden_paper"],"x":[0.7469940112792506,0.8704823278752244,0.8704823278752244,0.0],"xaxis":"x","y":[-230.0,-230.0,-245.0,-245.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(0,116,217)"},"mode":"lines","text":["torn_hold_carved_port_ship","","","box_tin_wooden_left_needs"],"x":[0.9569327161694986,1.159552387672495,1.159552387672495,0.8704823278752244],"xaxis":"x","y":[-200.0,-200.0,-237.5,-237.5],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(61,153,112)"},"mode":"lines","text":["times_tales_dog_boy_young","","","tales_neat_men_tell_serve"],"x":[0.0,0.7859723217957028,0.7859723217957028,0.0],"xaxis":"x","y":[-265.0,-265.0,-275.0,-275.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(61,153,112)"},"mode":"lines","text":["boy_child_sent_poor_hands","","","tales_times_neat_men_tell"],"x":[0.0,0.8762983457837246,0.8762983457837246,0.7859723217957028],"xaxis":"x","y":[-255.0,-255.0,-270.0,-270.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(61,153,112)"},"mode":"lines","text":["pipe_tell_ran_cut_clear","","","saw_dog_quick_tell_street"],"x":[0.0,0.818389541864389,0.818389541864389,0.0],"xaxis":"x","y":[-285.0,-285.0,-295.0,-295.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(61,153,112)"},"mode":"lines","text":["boy_tales_child_little_sent","","","tell_saw_dog_pipe_quick"],"x":[0.8762983457837246,0.9730805874876851,0.9730805874876851,0.818389541864389],"xaxis":"x","y":[-262.5,-262.5,-290.0,-290.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(255,65,54)"},"mode":"lines","text":["burned_went_waste_tree_pipe","","","steady_went_lawn_dark_bright"],"x":[0.0,0.7950795602336649,0.7950795602336649,0.0],"xaxis":"x","y":[-315.0,-315.0,-325.0,-325.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(255,65,54)"},"mode":"lines","text":["cup_contents_horse_tall_broke","","","went_dark_burned_pipe_steady"],"x":[0.0,0.9755085110980379,0.9755085110980379,0.7950795602336649],"xaxis":"x","y":[-305.0,-305.0,-320.0,-320.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(35,205,205)"},"mode":"lines","text":["win_poor_china_bank_early","","","loud_china_street_bank_black"],"x":[0.0,0.6983989311532843,0.6983989311532843,0.0],"xaxis":"x","y":[-335.0,-335.0,-345.0,-345.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(0,116,217)"},"mode":"lines","text":["went_contents_dark_lawn_cup","","","china_bank_win_poor_loud"],"x":[0.9755085110980379,1.1318646248581075,1.1318646248581075,0.6983989311532843],"xaxis":"x","y":[-312.5,-312.5,-340.0,-340.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(133,20,75)"},"mode":"lines","text":["north_loud_waste_bring_read","","","low_end_beat_just_kept"],"x":[0.0,0.7881785914876069,0.7881785914876069,0.0],"xaxis":"x","y":[-365.0,-365.0,-375.0,-375.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(133,20,75)"},"mode":"lines","text":["brought_road_moved_day_big","","","low_north_loud_waste_read"],"x":[0.0,0.9707290492566807,0.9707290492566807,0.7881785914876069],"xaxis":"x","y":[-355.0,-355.0,-370.0,-370.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(255,220,0)"},"mode":"lines","text":["wood_saw_dry_used_make","","","tree_sent_knife_ice_lack"],"x":[0.0,0.8662313273769238,0.8662313273769238,0.0],"xaxis":"x","y":[-385.0,-385.0,-395.0,-395.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(255,220,0)"},"mode":"lines","text":["door_sent_food_needed_bad","","","tender_odor_need_food_drink"],"x":[0.0,0.7693089317382125,0.7693089317382125,0.0],"xaxis":"x","y":[-405.0,-405.0,-415.0,-415.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(255,220,0)"},"mode":"lines","text":["wood_tree_sent_knife_used","","","food_door_needed_need_odor"],"x":[0.8662313273769238,0.9926244737746202,0.9926244737746202,0.7693089317382125],"xaxis":"x","y":[-390.0,-390.0,-410.0,-410.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(0,116,217)"},"mode":"lines","text":["low_road_north_waste_loud","","","sent_food_door_wood_needed"],"x":[0.9707290492566807,1.1091809113424917,1.1091809113424917,0.9926244737746202],"xaxis":"x","y":[-362.5,-362.5,-400.0,-400.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(40,35,35)"},"mode":"lines","text":["bowl_hot_came_light_red","","","ice_steady_bowl_water_tight"],"x":[0.0,0.706981982746535,0.706981982746535,0.0],"xaxis":"x","y":[-435.0,-435.0,-445.0,-445.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(40,35,35)"},"mode":"lines","text":["sweet_rare_covered_far_cup","","","bowl_ice_came_steady_hot"],"x":[0.0,0.854107440072346,0.854107440072346,0.706981982746535],"xaxis":"x","y":[-425.0,-425.0,-440.0,-440.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(0,116,217)"},"mode":"lines","text":["low_sent_food_door_road","","","bowl_sweet_ice_hot_rare"],"x":[1.1091809113424917,1.1653796071175673,1.1653796071175673,0.854107440072346],"xaxis":"x","y":[-381.25,-381.25,-432.5,-432.5],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(0,116,217)"},"mode":"lines","text":["went_china_contents_bank_dark","","","food_bowl_ice_low_road"],"x":[1.1318646248581075,1.1972910451145684,1.1972910451145684,1.1653796071175673],"xaxis":"x","y":[-326.25,-326.25,-406.875,-406.875],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(61,153,112)"},"mode":"lines","text":["screen_straw_let_corner_lack","","","tight_straw_covered_tell_door"],"x":[0.0,0.8001366405319948,0.8001366405319948,0.0],"xaxis":"x","y":[-455.0,-455.0,-465.0,-465.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(61,153,112)"},"mode":"lines","text":["paint_lines_covered_edge_corner","","","sheet_hole_square_spot_paint"],"x":[0.0,0.7844850052403061,0.7844850052403061,0.0],"xaxis":"x","y":[-475.0,-475.0,-485.0,-485.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(61,153,112)"},"mode":"lines","text":["straw_tight_screen_let_covered","","","paint_edge_sheet_hole_lines"],"x":[0.8001366405319948,0.9948964135300224,0.9948964135300224,0.7844850052403061],"xaxis":"x","y":[-460.0,-460.0,-480.0,-480.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(255,65,54)"},"mode":"lines","text":["pink_north_small_used_blue","","","wind_tree_pink_near_brought"],"x":[0.0,0.7212442344049732,0.7212442344049732,0.0],"xaxis":"x","y":[-495.0,-495.0,-505.0,-505.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(255,65,54)"},"mode":"lines","text":["pink_wind_brought_tree_north","","","seen_pink_drifts_stood_low"],"x":[0.7212442344049732,0.8360890279174114,0.8360890279174114,0.0],"xaxis":"x","y":[-500.0,-500.0,-515.0,-515.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(0,116,217)"},"mode":"lines","text":["paint_covered_straw_tight_sheet","","","pink_wind_edge_north_seen"],"x":[0.9948964135300224,1.21503768998778,1.21503768998778,0.8360890279174114],"xaxis":"x","y":[-470.0,-470.0,-507.5,-507.5],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(0,116,217)"},"mode":"lines","text":["food_bowl_china_low_ice","","","edge_pink_paint_straw_covered"],"x":[1.1972910451145684,1.2662192951193478,1.2662192951193478,1.21503768998778],"xaxis":"x","y":[-366.5625,-366.5625,-488.75,-488.75],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(35,205,205)"},"mode":"lines","text":["times_lines_drifts_dirt_road","","","tender_odor_dirt_cloth_water"],"x":[0.0,0.7260935178041159,0.7260935178041159,0.0],"xaxis":"x","y":[-525.0,-525.0,-535.0,-535.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(35,205,205)"},"mode":"lines","text":["dirt_deep_times_tender_odor","","","stain_odor_hands_burned_cloth"],"x":[0.7260935178041159,0.9263852548213412,0.9263852548213412,0.0],"xaxis":"x","y":[-530.0,-530.0,-545.0,-545.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(35,205,205)"},"mode":"lines","text":["stain_spot_wet_dried_read","","","sound_leaves_grass_wet_turn"],"x":[0.0,0.7928671657672419,0.7928671657672419,0.0],"xaxis":"x","y":[-555.0,-555.0,-565.0,-565.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(35,205,205)"},"mode":"lines","text":["odor_dirt_cloth_deep_drifts","","","wet_sound_leaves_grass_dry"],"x":[0.9263852548213412,0.9800775834365979,0.9800775834365979,0.7928671657672419],"xaxis":"x","y":[-537.5,-537.5,-560.0,-560.0],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(0,116,217)"},"mode":"lines","text":["low_edge_tree_bowl_food","","","dirt_odor_wet_stain_dry"],"x":[1.2662192951193478,1.2797792472175473,1.2797792472175473,0.9800775834365979],"xaxis":"x","y":[-427.65625,-427.65625,-548.75,-548.75],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(0,116,217)"},"mode":"lines","text":["child_boy_tell_tales_dog","","","edge_low_dry_dirt_pink"],"x":[0.9730805874876851,1.294866823112748,1.294866823112748,1.2797792472175473],"xaxis":"x","y":[-276.25,-276.25,-488.203125,-488.203125],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(0,116,217)"},"mode":"lines","text":["hold_port_torn_box_gold","","","street_tell_low_edge_dry"],"x":[1.159552387672495,1.3679343244161237,1.3679343244161237,1.294866823112748],"xaxis":"x","y":[-218.75,-218.75,-382.2265625,-382.2265625],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","marker":{"color":"rgb(0,116,217)"},"mode":"lines","text":["fast_great_words_sat_office","","","low_street_edge_hold_little"],"x":[1.260716979452376,1.4898554382727995,1.4898554382727995,1.3679343244161237],"xaxis":"x","y":[-76.40625,-76.40625,-300.48828125,-300.48828125],"yaxis":"y","type":"scatter"},{"hoverinfo":"text","hovertext":["act_base_write_fast_great","week_quite_home_tried_long","bad_cause_friends_right_tried","words_week_tried_quite_home","sat_desk_office_paint_fast","words_tried_drop_add_friends","act_lead_heavy_base_time","torn_carved_hold_gold_dried","tin_left_box_straight_needs","torn_hold_carved_port_ship","boy_tales_child_little_sent","went_contents_dark_lawn_cup","wood_tree_sent_knife_used","low_road_north_waste_loud","low_sent_food_door_road","went_china_contents_bank_dark","straw_tight_screen_let_covered","pink_wind_brought_tree_north","paint_covered_straw_tight_sheet","food_bowl_china_low_ice","dirt_deep_times_tender_odor","odor_dirt_cloth_deep_drifts","low_edge_tree_bowl_food","child_boy_tell_tales_dog","hold_port_torn_box_gold","fast_great_words_sat_office"],"marker":{"color":"black"},"mode":"markers","showlegend":false,"x":[0.8379933285857839,0.7322455875696929,0.9214618901630149,1.054464093313805,0.801817988379282,1.1752908666369162,1.020197890415084,0.7278241279915325,0.7469940112792506,0.9569327161694986,0.8762983457837246,0.9755085110980379,0.8662313273769238,0.9707290492566807,1.1091809113424917,1.1318646248581075,0.8001366405319948,0.7212442344049732,0.9948964135300224,1.1972910451145684,0.7260935178041159,0.9263852548213412,1.2662192951193478,0.9730805874876851,1.159552387672495,1.260716979452376],"y":[-12.5,-60.0,-102.5,-70.0,-150.0,-93.125,-26.25,-190.0,-230.0,-200.0,-262.5,-312.5,-390.0,-362.5,-381.25,-326.25,-460.0,-500.0,-470.0,-366.5625,-530.0,-537.5,-427.65625,-276.25,-218.75,-76.40625],"type":"scatter"},{"hoverinfo":"text","hovertext":["act_comes_base_great_fast","come_time_win_lead_heavy","words_tried_times_office_sweet","cause_bad_friends_right_turn","drop_add_dust_near_let","drop_add_bad_cause_dust","room_screen_sat_went_children","sat_room_desk_office_went","words_tried_office_drop_sat","port_ship_lost_torn_taste","box_tin_wooden_left_needs","tales_times_neat_men_tell","tell_saw_dog_pipe_quick","went_dark_burned_pipe_steady","china_bank_win_poor_loud","low_north_loud_waste_read","food_door_needed_need_odor","sent_food_door_wood_needed","bowl_ice_came_steady_hot","bowl_sweet_ice_hot_rare","food_bowl_ice_low_road","paint_edge_sheet_hole_lines","pink_wind_edge_north_seen","edge_pink_paint_straw_covered","wet_sound_leaves_grass_dry","dirt_odor_wet_stain_dry","edge_low_dry_dirt_pink","street_tell_low_edge_dry","low_street_edge_hold_little"],"marker":{"color":"black"},"mode":"markers","showlegend":false,"x":[0.7367104242928515,0.8741029258598041,0.7357174663588586,0.7798933378889688,0.8058335824605488,0.994120400108275,0.7120096433377865,1.0553883002013125,1.2031446877144025,0.776282375681635,0.8704823278752244,0.7859723217957028,0.818389541864389,0.7950795602336649,0.6983989311532843,0.7881785914876069,0.7693089317382125,0.9926244737746202,0.706981982746535,0.854107440072346,1.1653796071175673,0.7844850052403061,0.8360890279174114,1.21503768998778,0.7928671657672419,0.9800775834365979,1.2797792472175473,1.294866823112748,1.3679343244161237],"y":[-20.0,-40.0,-80.0,-110.0,-130.0,-116.25,-170.0,-160.0,-126.5625,-210.0,-237.5,-270.0,-290.0,-320.0,-340.0,-370.0,-410.0,-400.0,-440.0,-432.5,-406.875,-480.0,-507.5,-488.75,-560.0,-548.75,-488.203125,-382.2265625,-300.48828125],"type":"scatter"}],                        {"autosize":false,"height":1055,"hovermode":"closest","showlegend":false,"width":1000,"xaxis":{"mirror":"allticks","rangemode":"tozero","showgrid":false,"showline":true,"showticklabels":true,"ticks":"outside","type":"linear","zeroline":false},"yaxis":{"mirror":"allticks","rangemode":"tozero","showgrid":false,"showline":true,"showticklabels":true,"tickmode":"array","ticks":"outside","ticktext":["1_write_black_long","55_comes_base_great","40_act_fast_time","49_come_time_new","9_win_lead_heavy","19_home_week_quite","11_week_tried_quite","22_words_tried_times","46_words_sweet_better","17_space_office_neat","3_friends_need_turn","15_right_tried_drop","25_add_drop_comes","41_dust_near_let","10_desk_sat_office","44_sat_fast_rose","28_screen_sat_room","52_room_start_lost","8_gold_carved_torn","18_carved_dried_torn","32_ship_lost_port","36_port_taste_hold","50_tin_little_rare","0_left_straight_wooden","45_box_square_seen","5_boy_child_sent","47_times_tales_dog","34_tales_neat_men","43_pipe_tell_ran","4_saw_dog_quick","54_cup_contents_horse","20_burned_went_waste","51_steady_went_lawn","31_win_poor_china","39_loud_china_street","35_brought_road_moved","16_north_loud_waste","37_low_end_beat","53_wood_saw_dry","14_tree_sent_knife","42_door_sent_food","6_tender_odor_need","2_sweet_rare_covered","38_bowl_hot_came","23_ice_steady_bowl","13_screen_straw_let","29_tight_straw_covered","48_paint_lines_covered","33_sheet_hole_square","26_pink_north_small","12_wind_tree_pink","21_seen_pink_drifts","30_times_lines_drifts","27_tender_odor_dirt","56_stain_odor_hands","24_stain_spot_wet","7_sound_leaves_grass"],"tickvals":[-5.0,-15.0,-25.0,-35.0,-45.0,-55.0,-65.0,-75.0,-85.0,-95.0,-105.0,-115.0,-125.0,-135.0,-145.0,-155.0,-165.0,-175.0,-185.0,-195.0,-205.0,-215.0,-225.0,-235.0,-245.0,-255.0,-265.0,-275.0,-285.0,-295.0,-305.0,-315.0,-325.0,-335.0,-345.0,-355.0,-365.0,-375.0,-385.0,-395.0,-405.0,-415.0,-425.0,-435.0,-445.0,-455.0,-465.0,-475.0,-485.0,-495.0,-505.0,-515.0,-525.0,-535.0,-545.0,-555.0,-565.0],"type":"linear","zeroline":false,"range":[-570.0,0.0]},"template":{"data":{"barpolar":[{"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"white","showlakes":true,"showland":true,"subunitcolor":"#C8D4E3"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"white","polar":{"angularaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""},"bgcolor":"white","radialaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"yaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"zaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"baxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"bgcolor":"white","caxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2}}},"title":{"font":{"size":22,"color":"Black"},"text":"\u003cb\u003eHierarchical Clustering\u003c\u002fb\u003e","x":0.5,"xanchor":"center","yanchor":"top"},"hoverlabel":{"font":{"size":16,"family":"Rockwell"},"bgcolor":"white"},"plot_bgcolor":"#ECEFF1"},                        {"responsive": true}                    )                };                            </script>
</div>
<p>The hierarchical structure is based on how topics emerge based on the
similarity of their embeddings, however, we can often find topics that
we think should be merged based on our own knowledge. For example,
despite their embeddings having a relatively large distance between
them, topic 2 and 14 both appear to be about food.</p>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">topic_representations</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html" class="external-link">filter</a></span><span class="op">(</span><span class="va">Topic</span> <span class="op"><a href="https://rdrr.io/r/base/match.html" class="external-link">%in%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">2</span>,<span class="fl">14</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt;   Topic Count                     Name</span></span>
<span><span class="co">#&gt; 1     2    18 2_sweet_rare_covered_far</span></span>
<span><span class="co">#&gt; 2    14    10   14_tree_sent_knife_ice</span></span>
<span><span class="co">#&gt;                                                    Representation</span></span>
<span><span class="co">#&gt; 1 sweet, rare, covered, far, cup, comes, china, bowl, food, drink</span></span>
<span><span class="co">#&gt; 2      tree, sent, knife, ice, lack, cut, kept, makes, used, fine</span></span>
<span><span class="co">#&gt;                                                 keybert</span></span>
<span><span class="co">#&gt; 1 brown_food_bowl_taste_round_drink_covered_cup_hot_old</span></span>
<span><span class="co">#&gt; 2     ice_knife_kept_tree_used_cut_makes_lack_fine_sent</span></span>
<span><span class="co">#&gt;                                                      mmr</span></span>
<span><span class="co">#&gt; 1 cup_china_bowl_drink_food_covered_rare_far_comes_sweet</span></span>
<span><span class="co">#&gt; 2      knife_ice_cut_sent_used_kept_makes_fine_lack_tree</span></span>
<span><span class="co">#&gt;                             flanT5</span></span>
<span><span class="co">#&gt; 1 china tea bowl with a cup of tea</span></span>
<span><span class="co">#&gt; 2                        ice knife</span></span></code></pre></div>
<p>For larger topics we could use ParseR to analyse topic bigrams here,
but since these topics are relatively small, we can just examine
exemplars.</p>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">data</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html" class="external-link">filter</a></span><span class="op">(</span><span class="va">topic</span> <span class="op"><a href="https://rdrr.io/r/base/match.html" class="external-link">%in%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">2</span>,<span class="fl">14</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html" class="external-link">select</a></span><span class="op">(</span><span class="va">sentence</span>, <span class="va">topic</span><span class="op">)</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># A tibble: 28 × 2</span></span></span>
<span><span class="co">#&gt;    sentence                                 topic</span></span>
<span><span class="co">#&gt;    <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>                                    <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 1</span> these days a chicken leg is a rare dish.     2</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 2</span> rice is often served in round bowls.         2</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 3</span> the juice of lemons makes fine punch.       14</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 4</span> a pot of tea helps to pass the evening.      2</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 5</span> a cup of sugar makes sweet fudge.            2</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 6</span> the fruit peel was cut in thick slices.     14</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 7</span> a pound of sugar costs more than eggs.       2</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 8</span> he ordered peach pie with ice cream.        14</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 9</span> the fruit of a fig tree is apple shaped.    14</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">10</span> fruit flavors are used in fizz drinks.      14</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># ℹ 18 more rows</span></span></span></code></pre></div>
<p>I am pretty happy that these two topics could be merged into a larger
“food” topic, to do this we use the bt_merge_topics function:</p>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/bt_merge_topics.html">bt_merge_topics</a></span><span class="op">(</span>fitted_model <span class="op">=</span> <span class="va">topic_model</span>,</span>
<span>                documents <span class="op">=</span> <span class="va">sentences</span>,</span>
<span>                topics_to_merge <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">14</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>We have been maintaining a dataframe all along that is tracking each
step we’ve completed, it would be good to now update that dataframe with
our new topics.</p>
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">data</span> <span class="op">&lt;-</span> <span class="va">data</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html" class="external-link">mutate</a></span><span class="op">(</span>merged_topics <span class="op">=</span> <span class="va">topic_model</span><span class="op">$</span><span class="va">topics_</span><span class="op">)</span></span></code></pre></div>
</div>
</div>
<div class="section level3">
<h3 id="reducing-outliers">Reducing Outliers<a class="anchor" aria-label="anchor" href="#reducing-outliers"></a>
</h3>
<p>One feature of hdbscan is the outlier category, which can be quite
large. Sometimes we might want to redistribute these outlier documents
so that they fall within one of the existing topics. There are a number
of methods to achieve this and it is good practice to look at different
parameters and different methods when reducing outliers as it can be
quite difficult to redistribute outlier documents while maintaining
clarity within your topics. To this end, you should consider project
goal is before implementing any of these methods, it is more important
to have concise and coherent topics or to force most/all of your
documents into topics, is it a balance of the two?</p>
<p>The methods currently available to us are:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Tokenset Similarity:</strong> Divides each documents into
tokensets and calculates the c-TF-IDF cosine similarity between each
tokenset and each topic. The summation of each cosine similarity score
for each topic across each outlier document gives the most similar topic
for each outlier document.</p></li>
<li><p><strong>Embeddings:</strong> Measures the cosine similarty
between embeddings for each outlier document and each topic. If we have
passed an empty embedding model to bt_compile_model (which we did), we
must specify an embedding model to be used with this function.</p></li>
<li><p><strong>c-TF-IDF:</strong> Calculates the c-TF-IDF cosine
similarity for each outlier document and topic and redistributes
outliers based on the topic with which it has the highest
similarity.</p></li>
</ol>
<p>We can play with all outlier strategies as, unlike when we merge
topics or fit the model, the bt_outlier_* functions do not update the
model, they only output a df with each document, their current topic
classification and the potential new topics. We must update the model
using bt_update_topics to actually change the topics within the
model.</p>
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">outliers_ts_sim</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/bt_outliers_tokenset_similarity.html">bt_outliers_tokenset_similarity</a></span><span class="op">(</span>fitted_model <span class="op">=</span> <span class="va">topic_model</span>,</span>
<span>                                                   documents <span class="op">=</span> <span class="va">sentences</span>,</span>
<span>                                                   topics <span class="op">=</span> <span class="va">topic_model</span><span class="op">$</span><span class="va">topics_</span>,</span>
<span>                                                   threshold <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span></span>
<span></span>
<span><span class="va">outliers_embed</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/bt_outliers_embeddings.html">bt_outliers_embeddings</a></span><span class="op">(</span>fitted_model <span class="op">=</span> <span class="va">topic_model</span>,</span>
<span>                                         documents <span class="op">=</span> <span class="va">sentences</span>,</span>
<span>                                         topics <span class="op">=</span> <span class="va">topic_model</span><span class="op">$</span><span class="va">topics_</span>,</span>
<span>                                         embeddings <span class="op">=</span> <span class="va">reduced_embeddings</span>,</span>
<span>                                         embedding_model <span class="op">=</span> <span class="va">embedder</span>,</span>
<span>                                         threshold <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span></span>
<span></span>
<span><span class="va">outliers_ctfidf</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/bt_outliers_ctfidf.html">bt_outliers_ctfidf</a></span><span class="op">(</span>fitted_model <span class="op">=</span> <span class="va">topic_model</span>,</span>
<span>                                      documents <span class="op">=</span> <span class="va">sentences</span>,</span>
<span>                                      topics <span class="op">=</span> <span class="va">topic_model</span><span class="op">$</span><span class="va">topics_</span>,</span>
<span>                                      threshold <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span></span></code></pre></div>
<p>It would be useful now to look at how each method has redistributed
the outlier topics. The graph below shows how outliers have been
redistributed to topics below topic 12. You can see how each strategy
does not redistribute topics in the same way, the embedding strategy for
example, has found that 6 outlier documents are best represented by
topic 1, while no other strategy has found any outlier documents that
are best represented by topic 1. The embedding method has also
redistributed all outlier documents, while the c-TF-IDF and tokenset
similarity methods have left certain documents as outliers. This is
where playing around with the threshold parameter, to find a good fit
for your data and chosen strategy, is important.</p>
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">data</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html" class="external-link">mutate</a></span><span class="op">(</span>outliers_ts_sim <span class="op">=</span> <span class="va">outliers_ts_sim</span><span class="op">$</span><span class="va">new_topics</span>,</span>
<span>         outliers_embed <span class="op">=</span> <span class="va">outliers_embed</span><span class="op">$</span><span class="va">new_topics</span>,</span>
<span>         outliers_ctfidf <span class="op">=</span> <span class="va">outliers_ctfidf</span><span class="op">$</span><span class="va">new_topics</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html" class="external-link">filter</a></span><span class="op">(</span><span class="va">merged_topics</span> <span class="op">==</span> <span class="op">-</span><span class="fl">1</span>,</span>
<span>         <span class="va">outliers_ctfidf</span> <span class="op">&lt;</span> <span class="fl">12</span>,</span>
<span>         <span class="va">outliers_embed</span> <span class="op">&lt;</span> <span class="fl">12</span>,</span>
<span>         <span class="va">outliers_ts_sim</span> <span class="op">&lt;</span> <span class="fl">12</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html" class="external-link">select</a></span><span class="op">(</span><span class="va">outliers_ts_sim</span>, <span class="va">outliers_embed</span>, <span class="va">outliers_ctfidf</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://tidyr.tidyverse.org/reference/pivot_longer.html" class="external-link">pivot_longer</a></span><span class="op">(</span><span class="fu"><a href="https://tidyselect.r-lib.org/reference/everything.html" class="external-link">everything</a></span><span class="op">(</span><span class="op">)</span>, names_to <span class="op">=</span> <span class="st">"outlier_distribution_strategy"</span>, values_to <span class="op">=</span> <span class="st">"topic"</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html" class="external-link">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html" class="external-link">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">as.factor</a></span><span class="op">(</span><span class="va">topic</span><span class="op">)</span>, fill <span class="op">=</span> <span class="va">outlier_distribution_strategy</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_bar.html" class="external-link">geom_bar</a></span><span class="op">(</span>position <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/position_dodge.html" class="external-link">position_dodge2</a></span><span class="op">(</span>preserve <span class="op">=</span> <span class="st">"single"</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html" class="external-link">theme_minimal</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html" class="external-link">labs</a></span><span class="op">(</span>x <span class="op">=</span> <span class="st">"Numbers"</span>, </span>
<span>       y <span class="op">=</span> <span class="st">"Count"</span>, </span>
<span>       title <span class="op">=</span> <span class="st">"Number of outliers in each topic after redistribution"</span>,</span>
<span>       fill <span class="op">=</span> <span class="st">"Outlier redistribution strategy"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_colour_discrete.html" class="external-link">scale_fill_discrete</a></span><span class="op">(</span>labels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span>outliers_ctfidf <span class="op">=</span> <span class="st">"c-TF-IDF"</span>,</span>
<span>                               outliers_embed <span class="op">=</span> <span class="st">"Embeddings"</span>,</span>
<span>                               outliers_ts_sim <span class="op">=</span> <span class="st">"Tokenset similarity"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p><img src="modular_approach_files/figure-html/unnamed-chunk-30-1.png" width="700"></p>
<p>You should take a look at some of the documents which have been
redistributed and the topic which they have been redistributed to before
deciding on the best strategy for your data. This can be quite laborious
for large amounts of data with many topics. As this dataset is
relatively small it may not be appropriate, but you can use things like
bigrams and top terms on outlier documents that have been redistributed
to investigate if they line up with the topics they have been moved
to.</p>
<p>Once you have settled on a new list of topics that you are happy
with, we can update the dataframe we have been keeping. For example, if
after looking at the data we decided that the Tokenset Similarity method
was the most appropriate:</p>
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">data</span> <span class="op">&lt;-</span> <span class="va">data</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html" class="external-link">mutate</a></span><span class="op">(</span>new_topics <span class="op">=</span> <span class="va">outliers_ts_sim</span><span class="op">$</span><span class="va">new_topics</span><span class="op">)</span></span>
<span></span>
<span><span class="va">data</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html" class="external-link">filter</a></span><span class="op">(</span><span class="va">merged_topics</span> <span class="op">==</span> <span class="op">-</span><span class="fl">1</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html" class="external-link">select</a></span><span class="op">(</span><span class="va">merged_topics</span>, <span class="va">new_topics</span><span class="op">)</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># A tibble: 213 × 2</span></span></span>
<span><span class="co">#&gt;    merged_topics new_topics</span></span>
<span><span class="co">#&gt;            <span style="color: #949494; font-style: italic;">&lt;int&gt;</span>      <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 1</span>            -<span style="color: #BB0000;">1</span>         -<span style="color: #BB0000;">1</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 2</span>            -<span style="color: #BB0000;">1</span>         32</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 3</span>            -<span style="color: #BB0000;">1</span>         -<span style="color: #BB0000;">1</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 4</span>            -<span style="color: #BB0000;">1</span>         50</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 5</span>            -<span style="color: #BB0000;">1</span>         31</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 6</span>            -<span style="color: #BB0000;">1</span>          8</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 7</span>            -<span style="color: #BB0000;">1</span>         27</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 8</span>            -<span style="color: #BB0000;">1</span>         -<span style="color: #BB0000;">1</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 9</span>            -<span style="color: #BB0000;">1</span>         53</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">10</span>            -<span style="color: #BB0000;">1</span>         37</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># ℹ 203 more rows</span></span></span></code></pre></div>
<p>If you will be using your model in the future, you can also update
your model with those new topics.</p>
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/bt_update_topics.html">bt_update_topics</a></span><span class="op">(</span>fitted_model <span class="op">=</span> <span class="va">topic_model</span>,</span>
<span>                 documents <span class="op">=</span> <span class="va">sentences</span>,</span>
<span>                 new_topics <span class="op">=</span> <span class="va">outliers_ts_sim</span><span class="op">$</span><span class="va">new_topics</span><span class="op">)</span></span></code></pre></div>
<p>And that’s it! You’ve completed a basic topic modelling pipeline
using bertopic! If you would like to have a deeper look at what else we
can do using bertopic, refer to the BertopicR function documentation and
the BERTopic python library, <a href="https://maartengr.github.io/BERTopic/index.html" class="external-link uri">https://maartengr.github.io/BERTopic/index.html</a>.</p>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Jack Penzer, Aoife Ryan.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
