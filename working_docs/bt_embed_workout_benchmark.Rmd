---
title: "Modularising the fit_transform_model function"
author: "Jack Penzer"
date: "2023-06-14"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

For working out + testing of individual features. Trying to squeeze everything into one function initially appears to make it easier for the user but quickly becomes too inflexible, and too much gets hidden from them (too many implict arguments). The package should be easy to get off the ground with, but also pleasant to use as the user continues/becomes more experienced, i.e. the right abstractions should be made which provide the user with good mental models.

# Load libraries
```{r}
library(BertopicR)
library(dplyr)
library(tidyr)
library(stringr)
```

## bt_embed

```{r}
# BertopicR::import_bertopic()
data <- bert_example_data %>%
  janitor::clean_names() %>%
  filter(!is.na(message), !duplicated(message))

docs <- data %>% head(10) %>% pull(message)

embeddings <- docs %>%
  bt_embed()

docs %>%
  bt_embed(accelerator = "mps")

docs %>%
  bt_embed()

docs <- data %>% pull(message)
```

### Benchmarking CPU vs GPU

```{r}
docs_1k <- bert_example_data %>%
  janitor::clean_names() %>%
  head(1000) %>%
  pull(message)
```

```{r}
cpu_start <- Sys.time()
cpu_embeddings <- docs_1k %>%
  bt_embed(accelerator = "cpu")
(cpu_end <- Sys.time() - cpu_start)
```

```{r}
mps_start <- Sys.time()
mps_embeddings <- docs_1k %>%
  bt_embed(accelerator = "mps") 
(mps_end <- Sys.time() - mps_start)
```

### All docs?

### MPS all - bench

```{r}
library(bench)
embed <- function(documents, batch_size){
  bt_embed(documents = documents, batch_size = batch_size)
}

results <- bench::press(
  batch_size = c(16L, 32L, 64L, 128L, 256L),
  {
    bench::mark(embed(documents = docs, batch_size = batch_size))
  }
)

results %>%
  readr::write_csv("~/Google Drive/My Drive/data_science_project_work/internal_projects/bertopic/experiment_results/mps_batchsize.csv")

ggplot2::autoplot(results)
```

CPU vs MPS
```{r}
accel_embed <- function(documents, accelerator){
  bt_embed(documents = documents, accelerator = accelerator)
}
results_accel <- bench::press(
  bench::mark(accel_embed(documents = docs, accelerator = accelerator), iterations = 5),
  accelerator = c("mps", "cpu", NULL)
)


results_accel %>%
  readr::write_csv("~/Google Drive/My Drive/data_science_project_work/internal_projects/bertopic/experiment_results/accelerator_bt_embed.csv")
ggplot2::autoplot(results_accel)
```


```{r}
mps_all_start <- Sys.time()
mps_all_embeddings <- docs %>%
  bt_embed(accelerator = "mps")
(mps_all_end <- Sys.time() - mps_all_start) #19.6 seconds, 182 batches, looks like it's batch_size = 32L as defualt?
```

```{r}
mps_all_batch_start <- Sys.time()
mps_all_batch_embeddings <- docs %>%
  bt_embed(accelerator = "mps", batch_size = 128L)
(mps_all_batch_end <- Sys.time() - mps_all_batch_start) #25.91 seconds
#128L slower than default
```

```{r}
mps_all_batch64_start <- Sys.time()
mps_all_batch64_embeddings <- docs %>%
  bt_embed(accelerator = "mps", batch_size = 64L)
(mps_all_batch64_end <- Sys.time() - mps_all_batch64_start) #31.2 seconds
```


```{r}
mps_all_batch268_start <- Sys.time()
mps_all_batch268_embeddings <- docs %>%
  bt_embed(accelerator = "mps", batch_size = 256L)
(mps_all_batch268_end <- Sys.time() - mps_all_batch268_start) #25.03366 secs
```


### CPU All
```{r}
cpu_all_start <- Sys.time()
cpu_all_embeddings <- docs %>%
  bt_embed(accelerator = "cpu")
(cpu_all_end <- Sys.time() - cpu_all_start) # 1:59 in fact*
```

### CPU ALL BATCH
```{r}
cpu_all_batch_16_start <- Sys.time()
cpu_all_batch_16_embeddings <- docs %>%
  bt_embed(accelerator = "cpu", batch_size = 16L)
(cpu_all_batch_16_end <- Sys.time() - cpu_all_batch_16_start)

cpu_all_batch_16_end - cpu_all_end #2:06, no faster
```

Use bench::mark and bench::press to benchmark the results

## bt_reducer
```{r}
docs <- data %>% head(10) %>% pull(message)
embeddings <- docs %>%
  bt_embed()

reduced_embeddings <- embeddings %>%
  bt_reducer(return_value = "reduced_embeddings")
reduced_embeddings$reduced_embeddings
reduced_embeddings$base_reducer
```

```{r}
embeddings_df <- embeddings %>% as.data.frame()
reduced_embeddings <- bt_reducer(embeddings, verbose = FALSE, return_value = "reduced_embeddings")

#Parameters
n_neighbors = 15L;n_components = 5L; min_dist = 0.0; metric = "euclidean"; random_state = 42L; verbose = FALSE

umap <- reticulate::import("umap")
#Test outside of function
reducer <- umap$UMAP(n_neighbors = n_neighbors,
                     n_components = n_components,
                     min_dist = min_dist,
                     metric = metric,
                     random_state = random_state,
                     verbose = verbose)

#Fit a model
fitted_model <- reducer$fit(embeddings)

#Get embeddings from fitted model
fitted_model$embedding_

not_transformed <- reducer$fit(embeddings)
transformed <- reducer$fit_transform(embeddings)

not_transformed$transform(embeddings)
```


# RStudio example for py classes
```{r}
library(reticulate)
Hi <- PyClass("Hi", list(
  name = NULL,
  `__init__` = function(self, name) {
    self$name <- name
    NULL
  },
  say_hi = function(self) {
    paste0("Hi ", self$name)
  }
))

a <- Hi("World")
a$name <- "Jack"
a$say_hi()
```


## Creating the BaseDimensionalityReduction Class
```{r}
empty_reduction_model <- py$bertopic$dimensionality$BaseDimensionalityReduction()

base_dim <- reticulate::PyClass("BaseDimensionalityReduction", 
                    defs = list(
                      fit = function(self, X){
                        return(self)
                      },
                      transform = function(self, X){
                        return(X)
                        }
                        )
                    )
base_dim$fit("x")
base_dim$transform(X = "Hello")
```

# Testing bt_embed, bt_reducer and fit_transform on some real data
```{r}
library(BertopicR)
library(dplyr)
data <- bert_example_data %>%
  janitor::clean_names() %>%
  filter(!duplicated(message))

embeddings <- bt_embed(data$message)

reducer <- bt_reducer(
  embeddings = embeddings, return_value = "reduced_embeddings", 
  n_components = 10L,
  n_neighbors = 10L,
  min_dist = 0.0001,
  metric = "euclidean", 
  random_state = 18L
)

base_model <- reducer$base_reducer
reduced_embeddings <- reducer$reduced_embeddings
base_model
base_model$fit(1)

library(reticulate)
BertopicR::import_bertopic()

# Fit BERTopic without actually performing any dimensionality reduction
empty_dimensionality_model = BaseDimensionalityReduction()
topic_model = BERTopic(umap_model=empty_dimensionality_model)

topics <- bt_fit_transform_model(cleaned_text = data$message, calculated_embeddings = reduced_embeddings, reducer = base_model, stopwords =  FALSE)

topics_min_size <-  bt_fit_transform_model(cleaned_text = data$message[1:100], calculated_embeddings = reduced_embeddings[1:100, ], reducer = base_model, stopwords =  FALSE, min_topic_size = 5L)
```

It works with the BaseDimensionalityReduction object from Bertopic
```{r}
py_run_string("from bertopic.dimensionality import BaseDimensionalityReduction")
bert_base_dim <- py$BaseDimensionalityReduction
model <- bt_fit_transform_model(cleaned_text = data$message[1:10], calculated_embeddings = reduced_embeddings[1:10,], reducer = bert_base_dim, stopwords =  FALSE)

model$approximate_distribution(data$message[1:10])
```

But not with my implementation
```{r}
reducer <- bt_reducer(
  embeddings = embeddings[1:10, ], return_value = "reduced_embeddings", 
  n_components = 5L,
  n_neighbors = 5L,
  min_dist = 0.0001,
  metric = "euclidean", 
  random_state = 18L
)

base_model <- reducer$base_reducer
reduced_embeddings <- reducer$reduced_embeddings

topics <- bt_fit_transform_model(cleaned_text = data$message[1:10], calculated_embeddings = reduced_embeddings[1:10,], reducer = base_model, stopwords =  FALSE)


bert_base_dim$fit("hello")
bert_base_dim$transform(self = bert_base_dim, X = "why?")

base_model$transform(self = base_model, X = "why?")
base_model$fit(self =base_model, X = "hello")


bert_base_dim$transform(X = "why?")
base_model$transform(X = 1)

?bert_base_dim$fit
```

```{r}
base_model_two <- base_dimensionality_reduction()
base_dimensionality_reduction()

bt_fit_transform_model(cleaned_text = data$message[1:10], calculated_embeddings = reduced_embeddings[1:10,], reducer = base_model_two, stopwords =  FALSE)
```

Quick benchmark: 
Obvs totally unfair as you should include the calculating of the embeddings for the first modular approach, but that makes it pretty complicated (more complicated than it needs to be), the point is to test what happens when we make a change
```{r}
monolithic <- bench::mark(
  bt_fit_transform_model(cleaned_text = data$message)
)

embeddings <- bt_embed(data$message)
reduced <- bt_reducer(embeddings)
reducer <- reduced$base_reducer
reduced_embeddings <- reduced$reduced_embeddings

modular <- bench::mark(
  bt_fit_transform_model(cleaned_text = data$message, calculated_embeddings = reduced_embeddings, 
                         reducer = reducer)
)

modular_change <- bench::mark(
  bt_fit_transform_model(cleaned_text = data$message, calculated_embeddings = reduced_embeddings, 
                         reducer = reducer, min_topic_size = 5L)
)
modular_change

monolithic_five <- bench::mark(
  bt_fit_transform_model(cleaned_text = data$message, min_topic_size = 5L)
)
monolithic_five

results_df <- rbind(monolithic, modular, modular_change, monolithic_five) %>%
  mutate(desc = c("monolith", "modular", "modular_5L", "monolith_5L"), .before = 1)

results_df %>%
  readr::write_csv("~/Google Drive/My Drive/data_science_project_work/internal_projects/bertopic/experiment_results/monolithic_vs_modular.csv")
```



