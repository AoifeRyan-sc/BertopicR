---
title: "bertopic"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{bertopic}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

BertopicR is a package that allows us to use the Python BERTopic package in r. To do this we need to leverage the reticulate library which allows us to embed a python session within our r session. Firstly, we load our libraries:
```{r load libraries, message = FALSE}
library(BertopicR)
library(reticulate)
```

```{r, echo = FALSE}
library(dplyr)
library(stringr)
library(LimpiaR)
library(plotly)
```

Then check that our environment has been configured correctly
```{r check installation}
check_python_dependencies()
```

Now we can import the python BERTopic library through BertopicR via reticulate 
```{r import bertopic, message = FALSE}
BertopicR:::import_bertopic()
```

Now we can look at what's been loaded in. The "py" prefix tells r that we want to look within the python environment. We should see a vector containing: backend; BERTopic; cluster; plotting; representation; vectorizers
```{r check modules}
names(py$bertopic)
```


```{r, echo = FALSE}
# all(names(py$bertopic) == c("backend", "BERTopic", "cluster", "plotting", "representation", "vectorizers"))
    
```

Other than spam and duplicate post removal, there is no data preprocessing strictly required for topic analysis using BERT. That said, I have found better results by removing hashtags and it can be useful to remove mentions, urls and emojis at this stage as it will not affect the results and will be helpful when looking at the data later. We can use the ParseR clean_text function to do this (*note that this function removes urls*).

Load and preprocess the data:
```{r load data, eval = FALSE, echo = FALSE}
data <- bert_example_data %>%
  janitor::clean_names() %>% # clean column titles
  mutate(text_clean = message, .before = message) %>% # add column for clean text
  mutate(text_clean = tolower(text_clean)) %>% # all posts lower case
  limpiar_tags(text_var = text_clean, hashtag = F) %>%  # remove mentions
  mutate(text_clean = str_remove_all(text_clean, "@user"), # remove mentions
         text_clean = str_remove_all(text_clean, "#\\S+")) %>% # remove hashtags
  limpiar_url(text_var = text_clean) %>% # remove urls
  limpiar_emojis(text_var = text_clean) %>% # remove emojis
  limpiar_spaces(text_var = text_clean) %>% # remove unnecessary spaces
  distinct(text_clean, .keep_all = TRUE) # remove duplicates
```

```{r clean data}
data <- bert_example_data %>%
  janitor::clean_names() %>%
  mutate(text_clean = message, .before = message) %>%
  ParseR::clean_text(text_var = text_clean,
                     tolower = TRUE,
                     hashtags = FALSE,
                     mentions = FALSE,
                     emojis = FALSE,
                     punctuation = TRUE,
                     digits = TRUE,
                     in_parallel = TRUE) %>%
  distinct(text_clean, .keep_all = TRUE)
```

Next we use the fit_transform_model() function to fit the model to our data. 

```{r embedings and model}

model <- fit_transform_model(cleaned_text = data$text_clean,
                             min_topic_size = NULL,
                             ngram_range = c(1, 2),
                             embedding_model = "all-MiniLM-L6-v2",
                             accelerator = "mps",
                             diversity = 0.3,
                             stopwords = TRUE,
                             random_state = 42)

model <- fit_transform_model(cleaned_text = data$text_clean,
                                     min_topic_size = 20,
                                     ngram_range = c(1, 2),
                                     embedding_model = "all-MiniLM-L6-v2",
                                     accelerator = NULL,
                                     diversity = NULL,
                                     stopwords = TRUE,
                                     random_state = 42)
```

Now that we have fit the model we can look at the topics it has predicted for our data. The -1 category here represents outlier posts that do not fit as well into the identified topics.
```{r get topics 1}
model$get_topic_info()
```

Bert has identified x number of topics, this is probably too many for us to process and perform meaningful analysis on, we can adjust the model parameters so that a more manageable number of topics are identified. It can be helpful to glance through all of these topics in order to get a more granular understanding of the data and to identify topics which might be irrelevant to the analysis and could be removed.

# Adjusting Model Parameters

When creating the model, we can alter the min_topic_size arguments to control the size number of posts required per topic. It is also possible to use the nr_topics argument to specify the exact number of topics we would like the model to output however this should not be done without careful examination of the data. One thing to note is that we are using R to code in python and sometimes r objects do not translate well to python. For example, the min_topic_size argument for the BERTopic() function requires an integer as an input. In Python, 50 is defined as an integer type, in R, it is a double type and so we will need to specify it as an integer here.

what else can we do here..

Note: More information on hyperparameter tuning can be found on the BERTopic website

```{r regenerate model}
model <- py$bertopic$BERTopic(min_topic_size = as.integer(50))


model_output <- model$fit_transform(documents = data$text_clean, 
                                    embeddings = embeddings)
```

Lets look at the output topics this time:

```{r get topics 2}
model$get_topic_info()
```

Maybe now we would like to manually merge some topics that we think might be similar and reduce the size of the large outlier category (-1). **Merging topics should be performed before outlier reduction**. Before merging topics we should take a closer look at the data to make sure we understand what each topic is and its semantic similarity to another topic.

# Looking at the Data

While the get_topic_info() function is very useful for giving us an overview of the topics present in our data it is important to also know which documents are in each topic.

```{r get docs}
# model$get_document_info(data$text_clean) %>% head()
```

We can also look at a umap that shows us the topic distribution. This can be helpful in showing us how closely different topics relate to one another and we should continue to check this diagram as we merge topics or reduce outliers.

```{r umap 1}
model$visualize_documents(docs = data$text_clean, # data
                          embeddings = embeddings) # embeddings
```

A key difference between Bert and LDA is the use of hierarchical clustering, this means we can visualise how each cluster was formed and what other clusters it might be similar to.

```{r tree diagram 1}
hierarchical_topics <- model$hierarchical_topics(data$text_clean)

model$visualize_hierarchical_documents(data$text_clean, hierarchical_topics, embeddings = embeddings)

```

We can also look at a topic similarity heatmap to see how similar topics are.

```{r heatmap 1}
viewer()
model$visualize_heatmap()
```


```{python}
r.model.visualize_heatmap()
```

Now that we've had a closer look at the documents and the division of topics we can start to merge them. We can see from the tree diagram where we visualise the topic hierarchy that x and y and a and b are branches from the same cluster (is this the best way to say this?), having looked at the individual documents it is also apparent that they discuss similar topics (what are the topics), it might make sense to merge these. Make sure to save your model before merging topics in case you are unhappy with the result and want to revert back.

```{r save and merge}
# model$save(path = "BertopicR_example.bt", # where to save the model
#            save_embeddings = TRUE) # save embeddings - this isn't that necessary?
# 
# # look at this syntax
# topics_to_merge <- [[x, y],
#                     [a, b]]
# 
# model$merge_topics(docs = data$text_clean,
#                    topics_to_merge = topics_to_merge)
# 
# model$get_topic_info()
```

If we are unhappy with the new topics we can always go back to the original model.

```{r reload old model}
# model_og <- bertopic$BERTopic$load(path = "BertopicR_example.bt",
#                                    embedding_model = embedding_model)
```

Now that we are happy with the merged topics we can look at reducing some of the "outliers". It is up to you how important this is for your project, perhaps it is more important to get really high quality topics at the expense of some of the data or maybe it is important to include as much of the data as possible at the expense of resolution. In this example we are going to use the "embeddings" strategy, but there are lots of different strategies which can be used for doing this that you can find here: https://maartengr.github.io/BERTopic/getting_started/outlier_reduction/outlier_reduction.html#exploration

In order to correctly reassign topics from outliers we need to tell the function which document belongs to which topic, to do this we will use the get_document_info(). You can play around with the threshold parameter to adjust the number of reassigned documents, it refers to the minimum similarity between a document and a topic for it to be reassigned to that topic and the correct value is entirely dependent on the dataset and your specifications for the project.

```{r get docs 2}
# document_info <- model$get_document_info(data$text_clean) #create dataframe with info on each document
# document_info$head() # look at the first few rows of the dataframe
```

```{r outlier reduction}
# new_topics <- model$reduce_outliers(docs = data$text_clean, # data
#                                     topics = document_info$Topic, # Topic column from document_info df 
#                                     strategy="embeddings", # strategy
#                                     threshold = 0.3) # threshold

```
 
Once we have our new topics, we can update our model to reflect this, again, make sure to save your model before updating it in case we want to return to this.

```{r save and update topics}
# model$save(path = "BERTopic_example.bt",
#            save_embeddings = TRUE) # save embeddings - this isn't that necessary?
# 
# model$update_topics(docs = data$text_clean, # data
#                     topics = new_topics) # new topics 

```

Let's look at the document umap again. It looks like a lot of outliers have been put in groups that do not seem to be very similar to them. If you hover your mouse over some of these posts that look like outliers it is clear that they do not all belong to the topic in which they have been placed. In this case, it might be a good idea to reload the model from before updating the topics to reduce outliers and trying again with a higher threshold.

```{r umap 2}
# model$visualize_documents(docs = data$text_clean, # data
#                           embeddings = embeddings) # embeddings
```



