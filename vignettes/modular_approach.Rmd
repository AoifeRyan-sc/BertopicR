---
title: "Interacting with individaul modules"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    highlight: tango
    code_folding: show
vignette: >
  %\VignetteIndexEntry{modular_approach}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, message = FALSE}
library(BertopicR)

library(dplyr)
library(stringr)
library(tidyr)
```

# Modules

In the [Bertopic Python Library](https://maartengr.github.io/BERTopic/getting_started/quickstart/quickstart.html) by Maarten Grootendorst there are 6 sub-modules:

:   Embeddings - for transforming your documents into a numerical representation

    Dimensionality Reduction - for reducing the number of features of the embeddings output

    Clustering - finding groups of similar documents to represent as topics

    Vectorisers - find the n-grams to describe each topic

    c-TF-IDF - create topic-level (rather than document) bag of words matrices for representing topics

    Fine-tuning topic representations - other tools for topic representations (includes generative AI/LLMs)

This vignette will show you how to use {BertopicR} to tune each module when creating your topic models.

## Data

For demonstrative purposes, we'll use {stringr}'s 'sentences' data set, which comes fairly clean. For help on cleaning text data visit ParseR/LimpiaR documentation. Let's take a look at the first five posts for brevity.

```{r}
sentences <- stringr::sentences
sentences[1:5]
```

Then we'll turn the sentences into a data frame:

```{r}
df <- dplyr::tibble(sentences = tolower(sentences))
```

## Embeddings

In order to work efficiently with text data, we need to turn the words into numbers. The current state-of-the-art approach to turning text into numbers, is contextualised word embeddings. We'll use an MpNet model, 'all-mpnet-base-v2' to take our sentences and turn them into numbers (embeddings). This will allow us to find similarities and differences between our sentences, using standard mathematical techniques (don't worry if this isn't making sense right now, often the best way to learn is by doing).

In BertopicR you will usually either want to be making or doing with modules, or, compiling or fitting with models. We make components, we do actions on data with components. We compile our components into models, and then we fit our models to data.

### Make the embedder

First we'll make an embedder (or embedding_model) using the `bt_make_embedder` function. Then we'll embed our sentences using the embedder.

TIP: It's a good idea to save your embeddings, as when working with many documents this process will be time consuming.

```{r}
embedder <- bt_make_embedder(
  model_name = "all-mpnet-base-v2"
  )
```

### Do the embedding

```{r}
embeddings <- bt_do_embedding(
  embedder, 
  df$sentences
  )

embeddings[1, 1:10]
```

Each row of our embeddings output represents one of our original sentences, and each column represents a different embedding dimension; there are 768 dimensions outputted by the 'all-mpnet-base-v2' mode

We take a peek at the first 10 columns (dimensions), of the first row of our embeddings and we see 10 floating point numbers. 

## Reducing Dimensions

The next step in the pipeline is to reduce the dimensions of our embeddings, we do this for two reasons:

In the bertopic pipeline we perform dimensionality reduction for two reasons

1.  to allow our clustering algorithm to run smoothly

2.  to visualise our clusters on a plane (we will eventually reduce to 2 dimensions)

In machine learning more generally, dimensionality reduction is often an important step to avoid overfitting and the curse of dimensionality. For more information on the UMAP algorithm - uniform manifold approximation and projection for dimension reduction - *catchy*, see [UMAP Docs](https://umap-learn.readthedocs.io/en/latest/).

TIP: Like with embeddings, it's a good idea to save your reduced embeddings, as reducing dimensions can be a costly process.

### Make the reducer

We'll use a low-ish value for n_neighbours (we have a small dataset) and an output with 5 dimensions (n_components = 5L). We'll set the min_distance to 0, so that our dimensionality reduction model can place very similar documents *very* close together. We'll set the metric to "Euclidean" see [Embedding to non-Euclidean Spaces](https://umap-learn.readthedocs.io/en/latest/embedding_space.html) for alternatives.

```{r}
reducer <- bt_make_reducer_umap(
  n_neighbors = 10L, 
  n_components = 10L,
  min_dist = 0L,
  metric = "euclidean"
  )
```

### Do the reducing

```{r}
reduced_embeddings <- bt_do_reducing(
  reducer, embeddings = embeddings
)

reduced_embeddings[1:2, ]
```

We'll take a peek at two of our rows which now represent the reduced dimension embeddings for each document. Notice that our numbers are floating points, but also that they are not bounded between -1 and 1.

The next step is to cluster our data. On a first pass, bertopic considers each discovered cluster a topic. Choice of clustering model and the selected parameters are therefore important. We'll use an hdbscan cluster, as that's what bertopic was initially built with.

## Clustering

There is a lot to learn when it comes to clustering, and selecting the correct parameters is notoriously difficult - especially when clustering without pre-assigned labels, as *most* clustering tends to be. For this run we'll use the hdbscan clustering algorithm, because we don't know how many clusters we should look for in advance (which we should if using kMeans clustering for exampel).[hdbscan documentation](https://hdbscan.readthedocs.io/en/latest/basic_hdbscan.html)

It's important to know that until you get down to the level of updating topic representations, in the bertopic pipeline 1 topic = 1 cluster. It's therefore crucial to gather what information you can about your data to inform your clustering process.

### Make the clusterer

We'll stick with a Euclidean distance metric, we'll reduce the min_cluster_size to 10, giving us a theoretical maximum of number of clusters as: length(sentences) / 10 and min_samples equal to 5. The relationship between min_cluster_size and min_samples is important, it will default to min_samples = min_cluster_size if not specified, but this is likely to have adverse effects on your clustering outputs when dealing with larger datasets (as you'll likely want to raise the min_cluster_size parameter significantly). On the other hand, the hdbscan documentation claims that min_samples, a parameter inherited from dbscan, does not have such importance in the hdbscan algorithm - though they also say it remains the algorithm's biggest weakness.

We'll also set cluster_selection_method = "leaf", this means we'll tend to find many small clusters, rather than a few large clusters. This is another parameter which is fraught with danger, to get this right the first time is unlikely, and is likely to require trial and error, at least in the beginning.

```{r}
clusterer <- bt_make_clusterer_hdbscan(min_cluster_size = 5L,
                          metric = "euclidean",
                          cluster_selection_method = "leaf",
                          min_samples = 3L,
                          prediction_data = TRUE)
```

### Do the clustering

```{r}
clusters <- bt_do_clustering(clustering_model = clusterer, embeddings = reduced_embeddings) 
```

clusters is now a list of cluster labels, we have 720 labels in total - one for each sentence in {Stringr}'s sentences data set. The cluster labels are output as integers, but it's important **not** to assume that they work like regular integers do. It's **not necessarily** the case that cluster 1 is closer to cluster 4 than it is to cluster 15, the ordering of the labels can effectively be considered random.

As you most likely have no labels or a training/test/validation data set, you will have to rely on inspecting your clusters, remembering that in bertopic 1 cluster = 1 topic. To check whether our clusters make sense, we could draw upon our data analysis & visualisation tool kit, and inspect each cluster against every other. This would soon become intractable. Instead, we'll take a quick look at the distribution and then we'll use the bt_compile_model() and bt_fit_model() functions to get our topic models out.

### Create a data frame

But first, we're beginning to acquire a bunch of objects which may become hard to maintain. We can store them in a data frame:
```{r}
data <- dplyr::tibble(sentence = tolower(sentences)) %>%
  mutate(embeddings = list(embeddings),
         reduced_embeddings = list(reduced_embeddings),
         cluster = as.integer(clusters))
```

If you want to save this data frame, you'll need to save it as a .Rdata/.rds object, not as a .csv or .xlsx as it contains list columns.

### Count the clusters

We can see the distribution via a histogram:
```{r}
library(ggplot2)

data %>%
  filter(cluster != -1) %>%
  count(cluster, sort = TRUE) %>%
  ggplot(aes(x= n)) +
  geom_histogram(fill = "midnightblue", bins = 20) +
  theme_minimal() +
  xlab("Cluster Size") +
  ylab("Number of Clusters")
```

With the exception of the outlier group, the clusters are labelled in order of size:

```{r}
data %>%
  count(cluster, sort = TRUE)
```

213/720 (29.6%) of data points were labelled as noise (cluster == -1), and upon a first inspection they do appear to be quite eclectic. 

```{r}
data %>%
  filter(cluster == -1) %>% 
  slice(30:40) %>%
  pull(sentence)
```

But how do our other clusters look?

```{r}
data %>%
  filter(cluster == 0) %>%
  sample_n(10) %>% 
  pull(sentence)
```

```{r}
data %>%
  filter(cluster == 1) %>%
  sample_n(10) %>%
  pull(sentence)
```

The more clusters we look at, the more difficult it will become to figure out what's happening...
```{r}
data %>%
  filter(cluster == 2) %>%
  sample_n(10) %>%
  pull(sentence)
```

Inspecting each of these clusters and trying to figure out what each cluster means and how they inter-relate would soon become intractable for humans. Thankfully, within BERTopic there are quantitative methods already in place to aid this procedure.

Instead of looking at the individual posts in each cluster, we'll attempt to summarise their contents with keywords and phrases. In order to this, we'll make a vectoriser and a ctfidf model. After creating these models, we can use everything we've looked at so far to compile a model, fit the model on our data, and finally explore our topics and their representations.

### Make the vectoriser 
For the vectoriser we'll set the ngram range as c(1, 2) this means our topics can be represented as single words or bigrams. We'll set stop_words to 'english' so that English stop words are removed and we'll tell our vectoriser to only consider words that have a frequency of 3 or higher, so that rare words and chance occurrences don't clog our representations too much. In practice, we will want to set a higher value for min_frequency as we'll be working with significantly more data.

```{r}
vectoriser <- bt_make_vectoriser(ngram_range = c(1, 2), stop_words = "english", min_frequency = 3L)
```

### Make the ctfidf model

Then we'll create a ctfidf model which will allow us to represent each topic according to the words that are important to that topic (have high frequency) and distinct to that topic (have relatively low frequency in other topics):
```{r}
ctfidf <- bt_make_ctfidf(reduce_frequent_words = TRUE, bm25_weighting = FALSE)
```

## Compile the model

We've already made our individual components, or modules, and selected their parameters. We've already performed the embeddings and dimensionality reduction, so bertopic allows us to skip these steps easily by feeding in empty models to the `bt_compile_model` function for embedding and reducing. We can also skip clustering, but won't for this task as it adds extra complexity, the clustering we performed above with bt_do_clustering was just to explore how our clusterer would work in practice.

N.B. In practice you will need to pause and explore your parameters in more depth.

```{r}
topic_model <- bt_compile_model(
  embedding_model = bt_base_embedder(),
  reduction_model = bt_base_reducer(),
  clustering_model = clusterer,
  vectoriser_model = vectoriser,
  ctfidf_model = ctfidf
)
```

## Fit the model

We feed in our reduced embeddings rather than the original embeddings, this allows us to skip steps in the workflow; this can save us a lot of time, particularly when we have many documents.

```{r}
fitted_model <- bt_fit_model(topic_model, data$sentence, embeddings = reduced_embeddings)
```

We can create a look up table to join our sentences to their topic labels and their topic descriptions and join this information with our original dataframe:

```{r}
topic_representations <- fitted_model$get_topic_info()
topic_rep_lookup <- topic_representations %>%
  select(topic = Topic, description = Name, topic_size = Count)

data <- data %>%
  mutate(topic = fitted_model$topics_) %>%
  left_join(topic_rep_lookup)

(data <- data %>% 
  relocate(sentence, topic, topic_size, description)
)
```

The df compiled is a common place for everything we have generated so far, in practice we don't really need the cluster column now that we have the topic column, they provide the same information. Topics and clusters should have largely the same labels, the only discrepancy would be where multiple clusters/topics are the same size.

# Changing the Model Representation

# Creating Representation Models

Once you are happy with the topics/clusters that have been formed, there are a few methods we can use to improve the topic representations. To do this, we first create the representation method, and then use this method to update the topic model. 

The representation methods currently available are:

1. **KeyBERT** is a keyword extraction technique that uses BERT embeddings to represent our topics with appropriate keywords and phrases. To achieve this we first create a KeyBERTInspired representation model.

2. **MaximalMarginalRelevance** is a concept used to select the most relevant keywords or phrases while promoting diversity in keywords. It balances relevance to the topic with distinctiveness from previously chosen keywords or phrases using a trade-off parameter called lambda.

3. **OpenAI** allows us to use their available models to generate topic summaries. An OpenAI API key is required to access their api and models.

After creating each of these representation models below we can compile them into a single representation dictionary, this allows us to pass each representation model to the topic model. If we only wish to pass a single representation model, this step is not necessary.

```{r keybert, eval = FALSE}
representation_keybert <- bt_representation_keybert(top_n_words = 10,
                                                    nr_docs = 50,
                                                    nr_samples = 500,
                                                    nr_candidate_words = 100)

representation_mmr <- bt_representation_mmr(diversity = 0.5)


representation_openai <- bt_representation_openai(openai_model = "gpt-3.5-turbo",
                                                  nr_docs = 5,
                                                  chat = TRUE,
                                                  api_key = "sk-")

# compile models
representation_dict <- c(
  "KeyBERT" = representation_keybert,
  "MMR" = representation_mmr,
  "OpenAI" = representation_openai
)
```

```{r keybert_echo_false, echo = FALSE}
representation_keybert <- bt_representation_keybert(top_n_words = 10,
                                                    nr_docs = 50,
                                                    nr_samples = 500,
                                                    nr_candidate_words = 100)

representation_mmr <- bt_representation_mmr(diversity = 0.5)


representation_openai <- bt_representation_openai(openai_model = "gpt-3.5-turbo",
                                                  nr_docs = 5,
                                                  chat = TRUE,
                                                  api_key = Sys.getenv("OPENAI_API_KEY"))

# compile models
representation_dict <- c(
  "KeyBERT" = representation_keybert,
  "MMR" = representation_mmr,
  "OpenAI" = representation_openai
)
```

After creating our representation models we can pass them to the bt_update_topics function to update our representations.

++Note:++ When we initiated the current model using bt_compile_model, in order to skip the embedding step within model fitting (as we had already performed this), we passed an empty embedding_model. When using KeyBERT or MMR, an embedding model is required to calculate word and topic embeddings and so when updating representations using either of these methods, we must pass an embedding model. If you passed a valid embedding model to bt_compile_model, this is not required. 

```{r update_keybert}
fitted_model <- bt_update_topics(fitted_model = fitted_model,
                                 documents = sentences,
                                 representation_model = representation_dict,
                                 embedding_model = embedder)

fitted_model$get_topic_info() %>% select(-Representative_Docs, -Representation) %>% head(10)
```

## Utilising the Hugging Face Model Hub
We can also utilise Hugging Face models to create topic representations. Some of these models can be very useful for creating coherent topic summaries without the cost associated with OpenAI models.

This method for generating topic representations is slightly different to the above mentioned methods in that bt_representation_hf returns the actual topic representations rather than a representation model. 

```{r}
hf_representations <- bt_representation_hf(task = "text2text-generation",
                                           hf_model = "google/flan-t5-base",
                                           topic_model = fitted_model,
                                           documents = sentences,
                                           default_prompt = "keywords")

hf_representations
```

We then use the set_topic_labels function to create a CustomName for each topic with the generated representations instead of the bt_update_topics function.

```{r}
fitted_model$set_topic_labels(hf_representations)

fitted_model$get_topic_info() %>% select(-Representative_Docs, -Representation) %>% head(10)
```

# Modifying Topics

## Merging Topics

Particularly when using hdbscan we can end up with a large number of topics and it can be useful to merge some of these topics which we think are suitably similar. We can get a certain idea about this from the topic descriptions that we have already generated, but it can also be useful to look at the data more closely before merging.

### Hierarchical Clustering

Hdbscan clustering forms clusters through a hierarchical processes which you can visualise with a dendrogram. This can be useful when merging topics as you can see how clusters split to become the topics that emerged from our topic modelling process. The x-axis here is a measure of the distance between topic embeddings, so when clusters split at a higher x-value there is a larger distance between their embeddings. We can see that for this particular dataset, the clusters split into their final topics quite early on in the hierarchy and so it might not be appropriate to merge topics based on how they have emerged in the hierarchy. 

```{r}
hierarchical_topics <- fitted_model$hierarchical_topics(sentences)
fitted_model$visualize_hierarchy(hierarchical_topics = hierarchical_topics)$show()
```

The hierarchical structure is based on how topics emerge based on the similarity of their embeddings, however, we can often find topics that we think should be merged based on our knowledge. For example, despite their embeddings having a relatively large distance between them, topic 2 and 14 both appear to be about food. 

```{r}
fitted_model$get_topic_info() %>% 
  filter(Topic %in% c(2,14)) %>% 
  select(-Representative_Docs, - Representation)
```

For larger topics we could use ParseR to analyse topic bigrams here, but since these topics are relatively small, we can just examine exemplars.

```{r}
data %>%
  filter(Topic %in% c(2,14)) %>% 
  arrange(Topic) %>% 
  select(Document, Topic)
```

I am pretty happy that these two topics could be merged into a larger "food" topic, to do this we use the bt_merge_topics function:

```{r}
bt_merge_topics(fitted_model = fitted_model,
                documents = sentences,
                topics_to_merge = list(2, 14))

fitted_model$get_topic_info()
```

We have been maintaining a dataframe all along that is tracking each step we've completed, it would be good to now update that dataframe with our new topics.

```{r}
data <- data %>%
  mutate(merged_topics = fitted_model$topics_)
```
## Reducing Outliers

One feature of hdbscan is the outlier category, which can be quite large. Sometimes we might want to redistribute these outlier documents so that they fall within one of the existing topics. There are a number of methods to achieve this and it is good practice to look at different parameters and different methods when reducing outliers as it can be quite difficult to redistribute outlier documents while maintaining clarity within your topics. To this end, you should consider what your goal is for your project before perusing any of these methods, it is more important to have concise and coherent topics or to force most/all of your documents into topics, is it a balance of the two? 

The methods currently available to us are:

1. **Tokenset Similarity:** Divides each documents into tokensets and calculates the c-TF-IDF cosine similarity between each tokenset and each topic. The summation of each cosine similarity score for each topic across each outlier document gives the most similar topic for each outlier document.

2. **Embeddings:** Measures the cosine similarty between embeddings for each outlier document and each topic. If we have passed an empty embedding model to bt_compile_model (which we did), we must specify an embedding model to be used with this function.

3. **c-TF-IDF:** Calculates the c-TF-IDF cosine similarity for each outlier document and topic and redistributes outliers based on the topic with which it has the highest similarity.

We can play with all outlier strategies as, unlike when we merge topics or fit the model, the bt_outlier_* functions do not update the model, they only output a df with each document, their current topic classification and the potential new topics. We must update the model using bt_update_topics to actually change the topics within the model.

```{r}
outliers_ts_sim <- bt_outliers_tokenset_similarity(fitted_model = fitted_model,
                                                   documents = sentences,
                                                   topics = fitted_model$topics_,
                                                   threshold = 0.1)

outliers_embed <- bt_outliers_embeddings(fitted_model = fitted_model,
                                         documents = sentences,
                                         topics = fitted_model$topics_,
                                         embeddings = reduced_embeddings,
                                         embedding_model = embedder,
                                         threshold = 0.1)

outliers_ctfidf <- bt_outliers_ctfidf(fitted_model = fitted_model,
                                      documents = sentences,
                                      topics = fitted_model$topics_,
                                      threshold = 0.1)
```

It would be useful now to look at how each method has redistributed the outlier topics. The graph below shows how outliers have been redistributed to topics below topic 12. You can see how each strategy does not redistribute topics in the same way, the embedding strategy for example, has found that 6 outlier documents are best represented by topic 1, while no other strategy has found any outlier documents that are best represented by topic 1. The embedding method has also redistributed all outlier documents, while the c-TF-IDF and tokenset similarity methods have left certain documents as outliers. This is where playing around with the threshold parameter, to find a good fit for your data and chosen strategy, is important.

```{r}
data %>%
  mutate(outliers_ts_sim = outliers_ts_sim$new_topics,
         outliers_embed = outliers_embed$new_topics,
         outliers_ctfidf = outliers_ctfidf$new_topics) %>%
  filter(merged_topics == -1,
         outliers_ctfidf < 12,
         outliers_embed < 12,
         outliers_ts_sim < 12) %>%
  select(outliers_ts_sim, outliers_embed, outliers_ctfidf) %>%
  pivot_longer(everything(), names_to = "outlier_distribution_strategy", values_to = "topic") %>%
  ggplot(aes(x = as.factor(topic), fill = outlier_distribution_strategy)) +
  geom_bar(position = position_dodge2()) +
  theme_minimal() + 
  labs(x = "Numbers", 
       y = "Count", 
       title = "Number of outliers in each topic after redistribution",
       fill = "Outlier redistribution strategy") +
  scale_fill_discrete(labels = c(outliers_ctfidf = "c-TF-IDF",
                               outliers_embed = "Embeddings",
                               outliers_ts_sim = "Tokenset similarity"))

```

